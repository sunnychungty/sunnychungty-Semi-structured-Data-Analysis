{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5925fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "import torch.nn as nn\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, Dataset\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e17ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sunny\\\\Desktop\\\\Master\\\\Sem3\\\\5212\\\\A1\\\\Dataset_Assignment1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcabf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random seed\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56df5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up field & loading datasets\n",
    "\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text not in STOP_WORDS]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", TEXT), \n",
    "                   (\"abstract\", None),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "from torchtext.legacy.data import Dataset\n",
    "\n",
    "def split_dataset(dataset, split_index):\n",
    "    fields = dataset.fields\n",
    "    examples = dataset.examples\n",
    "    top_examples = examples[:split_index]\n",
    "    remaining_examples = examples[split_index:]\n",
    "    \n",
    "    top_dataset = Dataset(top_examples, fields)\n",
    "    remaining_dataset = Dataset(remaining_examples, fields)\n",
    "    \n",
    "    return top_dataset, remaining_dataset\n",
    "\n",
    "train_data_1000, valid_data = split_dataset(train_data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597bcde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cfd46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocab\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data_1000, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0144b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Sunny\\\\anaconda3\\\\Lib\\\\site-packages\\\\~=rch\\\\lib\\\\asmjit.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sunny\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting torch==1.9.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp39-cp39-win_amd64.whl (3128.0 MB)\n",
      "     ---------------------------------------- 3.1/3.1 GB 350.4 kB/s eta 0:00:00\n",
      "Collecting torchvision==0.10.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting torchaudio==0.9.1\n",
      "  Downloading torchaudio-0.9.1-cp39-cp39-win_amd64.whl (216 kB)\n",
      "     -------------------------------------- 216.0/216.0 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sunny\\anaconda3\\lib\\site-packages (from torch==1.9.1+cu111) (4.3.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\sunny\\anaconda3\\lib\\site-packages (from torchvision==0.10.1+cu111) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sunny\\anaconda3\\lib\\site-packages (from torchvision==0.10.1+cu111) (1.23.5)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855a8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of torch.overrides failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\overrides.py\", line 1262, in <module>\n",
      "    has_torch_function = _add_docstr(\n",
      "RuntimeError: function '_has_torch_function' already has a docstring\n",
      "]\n",
      "[autoreload of torch._tensor failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\", line 50, in <module>\n",
      "    class Tensor(torch._C._TensorBase):\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\", line 326, in Tensor\n",
      "    detach = _C._add_docstr(_C._TensorBase.detach, r\"\"\"\n",
      "RuntimeError: method 'detach' already has a docstring\n",
      "]\n",
      "[autoreload of torch._torch_docs failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\_torch_docs.py\", line 127, in <module>\n",
      "    add_docstr(torch.abs, r\"\"\"\n",
      "RuntimeError: function 'abs' already has a docstring\n",
      "]\n",
      "C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:151: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchtext==0.10.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e655d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterator #1 for small train_data\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "def preprocess_target_label(target_field):\n",
    "    for example in train_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "        \n",
    "    for example in test_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "\n",
    "def generate_label_iterator(dataset, target, validation = True):\n",
    "    preprocess_target_label(target)\n",
    "    \n",
    "    label_attr = f\"label_{target}\"\n",
    "    if validation:\n",
    "        iterators = data.BucketIterator.splits(\n",
    "            (dataset, valid_data, test_data),\n",
    "            batch_size = BATCH_SIZE,\n",
    "            device = device,\n",
    "            sort_key = lambda x: len(getattr(x, label_attr)),\n",
    "            sort_within_batch = False)\n",
    "        return iterators[0], iterators[1], iterators[2]\n",
    "    else:\n",
    "        iterators = data.BucketIterator.splits(\n",
    "            (dataset, test_data),\n",
    "            batch_size = BATCH_SIZE,\n",
    "            device = device,\n",
    "            sort_key = lambda x: len(getattr(x, label_attr)),\n",
    "\n",
    "            sort_within_batch = False)\n",
    "        return iterators[0], iterators[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2c9527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd6901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c88eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization & optimizer\n",
    "def generate_model_and_optimizer(embedding_dim=100, hidden_dim=256, output_dim=1, lr=1e-3):\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "    model = RNN(INPUT_DIM, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853e7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, label_field, target_field):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "        \n",
    "        # Use the specific label field\n",
    "        loss = criterion(predictions, getattr(batch, label_field))\n",
    "        \n",
    "        acc = binary_accuracy(predictions, getattr(batch, label_field))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, label_field, target_field):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "            \n",
    "            # Use the specific label field\n",
    "            loss = criterion(predictions, getattr(batch, label_field))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, getattr(batch, label_field))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5af9ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "# models = [model_IT, model_CL, model_CV]\n",
    "# optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "# iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "# for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "#     print(f\"Training model for {label_name}...\")\n",
    "#     best_valid_loss_IT = float(\"inf\")\n",
    "#     best_valid_loss_CL = float(\"inf\")\n",
    "#     best_valid_loss_CV = float(\"inf\")\n",
    "    \n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        \n",
    "#         if label_name == label_names[0]:\n",
    "#             train_loss_IT, train_acc_IT = train(model_IT, train_iterator_IT, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_IT, test_acc_IT = evaluate(model_IT, test_iterator_IT, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_IT < best_valid_loss_IT:\n",
    "#                 best_valid_loss_IT = test_loss_IT\n",
    "#                 torch.save(models[0].state_dict(), \"RNN_model_IT.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'InformationTheory Test Loss: {test_loss_IT:.3f} | Test Acc: {test_acc_IT*100:.2f}%')\n",
    "#             model_IT.eval()\n",
    "#             y_predict = []\n",
    "#             y_test = []\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in test_iterator_IT:\n",
    "#                     predictions = model_IT(batch.title).squeeze(1)\n",
    "#                     rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "#                     y_predict += rounded_preds.tolist()\n",
    "#                     y_test += batch.InformationTheory.tolist()\n",
    "                       \n",
    "#         elif label_name == label_names[1]:\n",
    "#             train_loss_CL, train_acc_CL = train(model_CL, train_iterator_CL, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_CL, test_acc_CL = evaluate(model_CL, test_iterator_CL, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CL < best_valid_loss_CL:\n",
    "#                 best_valid_loss_CL = test_loss_CL\n",
    "#                 torch.save(models[1].state_dict(), \"RNN_model_CL.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputationalLinguistics Test Loss: {test_loss_CL:.3f} | Test Acc: {test_acc_CL*100:.2f}%')\n",
    "\n",
    "           \n",
    "#         elif label_name == label_names[2]:\n",
    "#             train_loss_CV, train_acc_CV = train(model_CV, train_iterator_CV, optimizers[2], criterion, label_name, \"title\")\n",
    "#             test_loss_CV, test_acc_CV = evaluate(model_CV, test_iterator_CV, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CV < best_valid_loss_CV:\n",
    "#                 best_valid_loss_CV = test_loss_CV\n",
    "#                 torch.save(models[2].state_dict(), \"RNN_model_CV.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputerVision Test Loss: {test_loss_CV:.3f} | Test Acc: {test_acc_CV*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea088014",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "# models = [model_IT, model_CL, model_CV]\n",
    "# optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "# iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "# best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "# model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "# target_field = \"title\"\n",
    "\n",
    "# for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "#     print(f\"Training model for {label_name}...\")\n",
    "    \n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         train_loss, train_acc = train(model, train_iterator, optimizer, criterion, label_name, target_field)\n",
    "#         test_loss, test_acc = evaluate(model, test_iterator, criterion, label_name, target_field)\n",
    "        \n",
    "#         end_time = time.time()\n",
    "#         epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "#         if test_loss < best_valid_losses[idx]:\n",
    "#             best_valid_losses[idx] = test_loss\n",
    "#             torch.save(model.state_dict(), model_file_names[idx])\n",
    "\n",
    "#         print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#         print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        \n",
    "#         # For predictions and ground truth collection\n",
    "#         model.eval()\n",
    "#         y_predict = []\n",
    "#         y_test = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_iterator:\n",
    "#                 predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "#                 rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "#                 y_predict += rounded_preds.tolist()\n",
    "#                 y_test += getattr(batch, label_name).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9071437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.694 | Test Acc: 53.08%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.710 | Test Acc: 53.10%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.728 | Test Acc: 53.11%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.744 | Test Acc: 53.12%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.756 | Test Acc: 53.15%\n",
      "Training model for P1_ComputationalLinguistics, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.577 | Test Acc: 80.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.546 | Test Acc: 80.43%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.528 | Test Acc: 80.50%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.517 | Test Acc: 80.57%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.510 | Test Acc: 80.67%\n",
      "Training model for P1_ComputerVision, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.655 | Test Acc: 65.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.663 | Test Acc: 65.37%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.671 | Test Acc: 65.37%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.677 | Test Acc: 65.30%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.682 | Test Acc: 65.05%\n"
     ]
    }
   ],
   "source": [
    "def train_loop(ModelsList, OptimizersList, IteratorsList, N_EPOCHS, label_names, targe_field, modelFileNames, bestLossesList, preprocess, train_size):\n",
    "\n",
    "    for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, ModelsList, OptimizersList, IteratorsList)):\n",
    "        print(f\"Training model for {preprocess}_{label_name}, using {train_size} data of {target_field}...\")\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion, label_name, target_field)\n",
    "            test_loss, test_acc = evaluate(model, test_iterator, criterion, label_name, target_field)\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "            if test_loss < best_valid_losses[idx]:\n",
    "                bestLossesList[idx] = test_loss\n",
    "                torch.save(model.state_dict(), modelFileNames[idx])\n",
    "\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "            # For predictions and ground truth collection\n",
    "            model.eval()\n",
    "            y_predict = []\n",
    "            y_test = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_iterator:\n",
    "                    predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "                    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                    y_predict += rounded_preds.tolist()\n",
    "                    y_test += getattr(batch, label_name).tolist()\n",
    "#     return y_predict, y_test\n",
    "\n",
    "N_EPOCHS = 5\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "target_field = \"title\"\n",
    "\n",
    "# train_loop for \"title\", using top 1000 records of train data and preprocessing method 1\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"1000\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f12f1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InformationTheory:\n",
      "[[9517   70]\n",
      " [8393   86]]\n",
      "Accuracy: 0.5315509797409499\n",
      "Macro Precision: 0.5413305845466649\n",
      "Macro Recall: 0.5014205756598811\n",
      "Macro F1 score: 0.3560699520644659\n",
      "MCC: 0.015324910755452786\n",
      "\n",
      "\n",
      "ComputationalLinguistics:\n",
      "[[14545   153]\n",
      " [ 3340    28]]\n",
      "Accuracy: 0.8066533820436178\n",
      "Macro Precision: 0.4839737302625584\n",
      "Macro Recall: 0.4989519798288843\n",
      "Macro F1 score: 0.45428797497951073\n",
      "MCC: -0.008196549018356652\n",
      "\n",
      "\n",
      "ComputerVision:\n",
      "[[11639   208]\n",
      " [ 6101   118]]\n",
      "Accuracy: 0.6507804716041182\n",
      "Macro Precision: 0.5090255635249445\n",
      "Macro Recall: 0.5007084620599409\n",
      "Macro F1 score: 0.4114112584752512\n",
      "MCC: 0.005057378501559664\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, iterator, label_field):\n",
    "    y_predict = []\n",
    "    y_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.title).squeeze(1)\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "            y_predict += rounded_preds.tolist()\n",
    "            y_test += getattr(batch, label_field).tolist()\n",
    "\n",
    "    y_predict = np.asarray(y_predict)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    recall = recall_score(y_test, y_predict, average='macro')\n",
    "    precision = precision_score(y_test, y_predict, average='macro')\n",
    "    f1score = f1_score(y_test, y_predict, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    matthews = matthews_corrcoef(y_test, y_predict)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"{label_field}:\")\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Macro Precision:', precision)\n",
    "    print('Macro Recall:', recall)\n",
    "    print('Macro F1 score:', f1score)\n",
    "    print('MCC:', matthews)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluate models\n",
    "evaluate_model(model_IT, test_iterator_IT, \"InformationTheory\")\n",
    "evaluate_model(model_CL, test_iterator_CL, \"ComputationalLinguistics\")\n",
    "evaluate_model(model_CV, test_iterator_CV, \"ComputerVision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26953537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P2_InformationTheory, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.701 | Test Acc: 53.02%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.715 | Test Acc: 53.02%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.729 | Test Acc: 53.02%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "InformationTheory Test Loss: 0.740 | Test Acc: 53.02%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.751 | Test Acc: 53.02%\n",
      "Training model for P2_ComputationalLinguistics, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.711 | Test Acc: 19.22%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.631 | Test Acc: 79.90%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.583 | Test Acc: 80.36%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "ComputationalLinguistics Test Loss: 0.554 | Test Acc: 80.49%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.536 | Test Acc: 80.53%\n",
      "Training model for P2_ComputerVision, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.752 | Test Acc: 34.86%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.739 | Test Acc: 34.85%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.728 | Test Acc: 34.86%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.720 | Test Acc: 34.86%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.715 | Test Acc: 34.88%\n"
     ]
    }
   ],
   "source": [
    "TEXT_P2 = Field(sequential=True, tokenize=\"spacy\", lower=True)\n",
    "LABEL_P2 = LabelField(dtype=torch.float)\n",
    "\n",
    "# Field - P2\n",
    "train_datafield_P2 = [(\"title\", TEXT_P2),\n",
    "                      (\"abstract\", None),\n",
    "                      (\"InformationTheory\", LABEL_P2),\n",
    "                      (\"ComputationalLinguistics\", LABEL_P2),\n",
    "                      (\"ComputerVision\", LABEL_P2)\n",
    "                      ]\n",
    "\n",
    "train_data_P2, test_data_P2 = TabularDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.csv\", test=\"test.csv\", format=\"csv\",\n",
    "    skip_header=True, fields=train_datafield_P2)\n",
    "\n",
    "train_data_1000_P2, valid_data_P2 = split_dataset(train_data_P2, 1000)\n",
    "\n",
    "# Building vocab - P2\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT_P2.build_vocab(train_data_1000_P2, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL_P2.build_vocab(train_data_1000_P2)\n",
    "\n",
    "# Create iterator #2 for P2 train_data\n",
    "\n",
    "train_iterator_IT_P2, validation_IT_P2, test_iterator_IT_P2 = generate_label_iterator(train_data_1000_P2, \"InformationTheory\", validation=True)\n",
    "train_iterator_CL_P2, validation_CL_P2, test_iterator_CL_P2 = generate_label_iterator(train_data_1000_P2, \"ComputationalLinguistics\", validation=True)\n",
    "train_iterator_CV_P2, validation_CV_P2, test_iterator_CV_P2 = generate_label_iterator(train_data_1000_P2, \"ComputerVision\", validation=True)\n",
    "\n",
    "model_IT_P2, optimizer_IT_P2 = generate_model_and_optimizer()\n",
    "model_CL_P2, optimizer_CL_P2 = generate_model_and_optimizer()\n",
    "model_CV_P2, optimizer_CV_P2 = generate_model_and_optimizer()\n",
    "\n",
    "# Training Loop - P2\n",
    "\n",
    "models_P2 = [model_IT_P2, model_CL_P2, model_CV_P2]\n",
    "optimizers_P2 = [optimizer_IT_P2, optimizer_CL_P2, optimizer_CV_P2]\n",
    "iterators_P2 = [(train_iterator_IT_P2, test_iterator_IT_P2), (train_iterator_CL_P2, test_iterator_CL_P2), (train_iterator_CV_P2, test_iterator_CV_P2)]\n",
    "\n",
    "best_valid_losses_P2 = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names_P2 = [\"RNN_model_IT_P2.pt\", \"RNN_model_CL_P2.pt\", \"RNN_model_CV_P2.pt\"]\n",
    "\n",
    "train_loop(models_P2, optimizers_P2, iterators_P2, N_EPOCHS, label_names, target_field, model_file_names_P2, best_valid_losses_P2, \"P2\", \"1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8de4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InformationTheory:\n",
      "[[9456  131]\n",
      " [8357  122]]\n",
      "Accuracy: 0.530167164840031\n",
      "Macro Precision: 0.5065308478131063\n",
      "Macro Recall: 0.5003620760427228\n",
      "Macro F1 score: 0.3590810877684564\n",
      "MCC: 0.0030754925015638412\n",
      "\n",
      "\n",
      "ComputationalLinguistics:\n",
      "[[14511   187]\n",
      " [ 3334    34]]\n",
      "Accuracy: 0.8051035093545887\n",
      "Macro Precision: 0.4835075543677393\n",
      "Macro Recall: 0.4986860962226347\n",
      "Macro F1 score: 0.45537573550692434\n",
      "MCC: -0.009310099164717622\n",
      "\n",
      "\n",
      "ComputerVision:\n",
      "[[  160 11687]\n",
      " [   82  6137]]\n",
      "Accuracy: 0.3485552972434407\n",
      "Macro Precision: 0.5027340330430138\n",
      "Macro Recall: 0.5001600646219684\n",
      "Macro F1 score: 0.26848618108041583\n",
      "MCC: 0.0013230600371548133\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate models\n",
    "evaluate_model(model_IT_P2, test_iterator_IT_P2, \"InformationTheory\")\n",
    "evaluate_model(model_CL_P2, test_iterator_CL_P2, \"ComputationalLinguistics\")\n",
    "evaluate_model(model_CV_P2, test_iterator_CV_P2, \"ComputerVision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "184805ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "180304b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", TEXT), \n",
    "                   (\"abstract\", None),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, test_iterator_IT = generate_label_iterator(train_data, \"InformationTheory\", validation = False)\n",
    "train_iterator_CL, test_iterator_CL = generate_label_iterator(train_data, \"ComputationalLinguistics\", validation = False)\n",
    "train_iterator_CV, test_iterator_CV = generate_label_iterator(train_data, \"ComputerVision\", validation = False)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2659279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 34s\n",
      "InformationTheory Test Loss: 0.804 | Test Acc: 53.06%\n",
      "Epoch: 02 | Epoch Time: 0m 32s\n",
      "InformationTheory Test Loss: 0.819 | Test Acc: 53.10%\n",
      "Epoch: 03 | Epoch Time: 0m 32s\n",
      "InformationTheory Test Loss: 0.808 | Test Acc: 53.10%\n",
      "Epoch: 04 | Epoch Time: 0m 32s\n",
      "InformationTheory Test Loss: 0.813 | Test Acc: 53.11%\n",
      "Epoch: 05 | Epoch Time: 0m 32s\n",
      "InformationTheory Test Loss: 0.805 | Test Acc: 53.11%\n",
      "Training model for P1_ComputationalLinguistics, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 31s\n",
      "ComputationalLinguistics Test Loss: 0.494 | Test Acc: 81.02%\n",
      "Epoch: 02 | Epoch Time: 0m 32s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.09%\n",
      "Epoch: 03 | Epoch Time: 0m 31s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.19%\n",
      "Epoch: 04 | Epoch Time: 0m 31s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.23%\n",
      "Epoch: 05 | Epoch Time: 0m 32s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.27%\n",
      "Training model for P1_ComputerVision, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 32s\n",
      "ComputerVision Test Loss: 0.701 | Test Acc: 34.73%\n",
      "Epoch: 02 | Epoch Time: 0m 31s\n",
      "ComputerVision Test Loss: 0.712 | Test Acc: 34.63%\n",
      "Epoch: 03 | Epoch Time: 0m 32s\n",
      "ComputerVision Test Loss: 0.705 | Test Acc: 34.63%\n",
      "Epoch: 04 | Epoch Time: 0m 32s\n",
      "ComputerVision Test Loss: 0.707 | Test Acc: 34.65%\n",
      "Epoch: 05 | Epoch Time: 0m 32s\n",
      "ComputerVision Test Loss: 0.702 | Test Acc: 34.68%\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba932eef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "# models = [model_IT, model_CL, model_CV]\n",
    "# optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "# iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "# for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "#     print(f\"Training model for {label_name}...\")\n",
    "#     best_valid_loss_IT = float(\"inf\")\n",
    "#     best_valid_loss_CL = float(\"inf\")\n",
    "#     best_valid_loss_CV = float(\"inf\")\n",
    "    \n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        \n",
    "#         if label_name == label_names[0]:\n",
    "#             train_loss_IT, train_acc_IT = train(model_IT, train_iterator_IT, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_IT, test_acc_IT = evaluate(model_IT, test_iterator_IT, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_IT < best_valid_loss_IT:\n",
    "#                 best_valid_loss_IT = test_loss_IT\n",
    "#                 torch.save(models[0].state_dict(), \"RNN_model_IT.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'InformationTheory Test Loss: {test_loss_IT:.3f} | Test Acc: {test_acc_IT*100:.2f}%')\n",
    "#             model_IT.eval()\n",
    "#             y_predict = []\n",
    "#             y_test = []\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in test_iterator_IT:\n",
    "#                     predictions = model_IT(batch.title).squeeze(1)\n",
    "#                     rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "#                     y_predict += rounded_preds.tolist()\n",
    "#                     y_test += batch.InformationTheory.tolist()\n",
    "                       \n",
    "#         elif label_name == label_names[1]:\n",
    "#             train_loss_CL, train_acc_CL = train(model_CL, train_iterator_CL, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_CL, test_acc_CL = evaluate(model_CL, test_iterator_CL, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CL < best_valid_loss_CL:\n",
    "#                 best_valid_loss_CL = test_loss_CL\n",
    "#                 torch.save(models[1].state_dict(), \"RNN_model_CL.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputationalLinguistics Test Loss: {test_loss_CL:.3f} | Test Acc: {test_acc_CL*100:.2f}%')\n",
    "\n",
    "           \n",
    "#         elif label_name == label_names[2]:\n",
    "#             train_loss_CV, train_acc_CV = train(model_CV, train_iterator_CV, optimizers[2], criterion, label_name, \"title\")\n",
    "#             test_loss_CV, test_acc_CV = evaluate(model_CV, test_iterator_CV, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CV < best_valid_loss_CV:\n",
    "#                 best_valid_loss_CV = test_loss_CV\n",
    "#                 torch.save(models[2].state_dict(), \"RNN_model_CV.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputerVision Test Loss: {test_loss_CV:.3f} | Test Acc: {test_acc_CV*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c628dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P2_InformationTheory, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.693 | Test Acc: 53.02%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.705 | Test Acc: 53.02%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.720 | Test Acc: 53.00%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.733 | Test Acc: 53.01%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.746 | Test Acc: 53.02%\n",
      "Training model for P2_ComputationalLinguistics, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.602 | Test Acc: 80.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.560 | Test Acc: 80.39%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.536 | Test Acc: 80.40%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.522 | Test Acc: 80.42%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.513 | Test Acc: 80.43%\n",
      "Training model for P2_ComputerVision, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.686 | Test Acc: 65.21%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.689 | Test Acc: 64.86%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.690 | Test Acc: 64.50%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.692 | Test Acc: 64.21%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.693 | Test Acc: 61.16%\n"
     ]
    }
   ],
   "source": [
    "TEXT_P2 = Field(sequential=True, tokenize=\"spacy\", lower=True)\n",
    "LABEL_P2 = LabelField(dtype=torch.float)\n",
    "\n",
    "# Field - P2\n",
    "train_datafield_P2 = [(\"title\", TEXT_P2),\n",
    "                      (\"abstract\", None),\n",
    "                      (\"InformationTheory\", LABEL_P2),\n",
    "                      (\"ComputationalLinguistics\", LABEL_P2),\n",
    "                      (\"ComputerVision\", LABEL_P2)\n",
    "                      ]\n",
    "\n",
    "train_data_P2, test_data_P2 = TabularDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.csv\", test=\"test.csv\", format=\"csv\",\n",
    "    skip_header=True, fields=train_datafield_P2)\n",
    "\n",
    "\n",
    "# Building vocab - P2\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT_P2.build_vocab(train_data_P2, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL_P2.build_vocab(train_data_P2)\n",
    "\n",
    "# Create iterator #2 for P2 train_data\n",
    "\n",
    "train_iterator_IT_P2, test_iterator_IT_P2 = generate_label_iterator(train_data_1000_P2, \"InformationTheory\", validation=False)\n",
    "train_iterator_CL_P2, test_iterator_CL_P2 = generate_label_iterator(train_data_1000_P2, \"ComputationalLinguistics\", validation=False)\n",
    "train_iterator_CV_P2, test_iterator_CV_P2 = generate_label_iterator(train_data_1000_P2, \"ComputerVision\", validation=False)\n",
    "\n",
    "model_IT_P2, optimizer_IT_P2 = generate_model_and_optimizer()\n",
    "model_CL_P2, optimizer_CL_P2 = generate_model_and_optimizer()\n",
    "model_CV_P2, optimizer_CV_P2 = generate_model_and_optimizer()\n",
    "\n",
    "# Training Loop - P2\n",
    "\n",
    "models_P2 = [model_IT_P2, model_CL_P2, model_CV_P2]\n",
    "optimizers_P2 = [optimizer_IT_P2, optimizer_CL_P2, optimizer_CV_P2]\n",
    "iterators_P2 = [(train_iterator_IT_P2, test_iterator_IT_P2), (train_iterator_CL_P2, test_iterator_CL_P2), (train_iterator_CV_P2, test_iterator_CV_P2)]\n",
    "\n",
    "best_valid_losses_P2 = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names_P2 = [\"RNN_model_IT_P2.pt\", \"RNN_model_CL_P2.pt\", \"RNN_model_CV_P2.pt\"]\n",
    "\n",
    "train_loop(models_P2, optimizers_P2, iterators_P2, N_EPOCHS, label_names, target_field, model_file_names_P2, best_valid_losses_P2, \"P2\", \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e0daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For ABSTRACT USINIG 1000 DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e164f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", None), \n",
    "                   (\"abstract\", TEXT),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "train_data_1000, valid_data = split_dataset(train_data, 1000)\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT.build_vocab(train_data_1000, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data_1000)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = True)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20e5e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 15s\n",
      "InformationTheory Test Loss: 0.692 | Test Acc: 53.02%\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "InformationTheory Test Loss: 0.699 | Test Acc: 53.03%\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "InformationTheory Test Loss: 0.710 | Test Acc: 53.00%\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "InformationTheory Test Loss: 0.723 | Test Acc: 53.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_18300\\931299814.py\"\u001b[0m, line \u001b[0;32m11\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"1000\")\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_18300\\3336522896.py\"\u001b[0m, line \u001b[0;32m28\u001b[0m, in \u001b[0;35mtrain_loop\u001b[0m\n    predictions = model(getattr(batch, target_field)).squeeze(1)\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[0;32m1051\u001b[0m, in \u001b[0;35m_call_impl\u001b[0m\n    return forward_call(*input, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_18300\\610180420.py\"\u001b[0m, line \u001b[0;32m20\u001b[0m, in \u001b[0;35mforward\u001b[0m\n    output, hidden = self.rnn(embedded)\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[0;32m1051\u001b[0m, in \u001b[0;35m_call_impl\u001b[0m\n    return forward_call(*input, **kwargs)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\"\u001b[1;36m, line \u001b[1;32m268\u001b[1;36m, in \u001b[1;35mforward\u001b[1;36m\u001b[0m\n\u001b[1;33m    result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "target_field = \"abstract\"\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", None), \n",
    "                   (\"abstract\", TEXT),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "train_data_1000, valid_data = split_dataset(train_data, 1000)\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data_1000, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data_1000)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = True)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7a65000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 21s\n",
      "InformationTheory Test Loss: 0.691 | Test Acc: 53.08%\n",
      "Epoch: 02 | Epoch Time: 0m 19s\n",
      "InformationTheory Test Loss: 0.696 | Test Acc: 53.12%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "InformationTheory Test Loss: 0.706 | Test Acc: 53.15%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "InformationTheory Test Loss: 0.719 | Test Acc: 53.15%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "InformationTheory Test Loss: 0.731 | Test Acc: 53.15%\n",
      "Training model for P1_ComputationalLinguistics, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.654 | Test Acc: 80.13%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.594 | Test Acc: 80.16%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.559 | Test Acc: 80.19%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.538 | Test Acc: 80.20%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.524 | Test Acc: 80.22%\n",
      "Training model for P1_ComputerVision, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.679 | Test Acc: 65.53%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.684 | Test Acc: 65.53%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.688 | Test Acc: 65.55%\n",
      "Epoch: 04 | Epoch Time: 0m 19s\n",
      "ComputerVision Test Loss: 0.690 | Test Acc: 65.53%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.693 | Test Acc: 65.45%\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "target_field = \"abstract\"\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "843fece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P2_InformationTheory, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 19s\n",
      "InformationTheory Test Loss: 0.692 | Test Acc: 53.07%\n",
      "Epoch: 02 | Epoch Time: 0m 19s\n",
      "InformationTheory Test Loss: 0.692 | Test Acc: 53.07%\n",
      "Epoch: 03 | Epoch Time: 0m 19s\n",
      "InformationTheory Test Loss: 0.692 | Test Acc: 53.07%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "InformationTheory Test Loss: 0.692 | Test Acc: 53.07%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "InformationTheory Test Loss: 0.692 | Test Acc: 53.07%\n",
      "Training model for P2_ComputationalLinguistics, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.632 | Test Acc: 80.40%\n",
      "Epoch: 02 | Epoch Time: 0m 19s\n",
      "ComputationalLinguistics Test Loss: 0.632 | Test Acc: 80.40%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.632 | Test Acc: 80.40%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.632 | Test Acc: 80.40%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "ComputationalLinguistics Test Loss: 0.632 | Test Acc: 80.40%\n",
      "Training model for P2_ComputerVision, using 1000 data of abstract...\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.694 | Test Acc: 34.88%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.696 | Test Acc: 34.83%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.697 | Test Acc: 34.79%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.696 | Test Acc: 34.80%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "ComputerVision Test Loss: 0.697 | Test Acc: 34.79%\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=\"spacy\", lower=True)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", None), \n",
    "                    (\"abstract\", TEXT),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "train_data_1000, valid_data = split_dataset(train_data, 1000)\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data_1000, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data_1000)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT_P2, validation_IT_P2, test_iterator_IT_P2 = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL_P2, validation_CL_P2, test_iterator_CL_P2 = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV_P2, validation_CV_P2, test_iterator_CV_P2 = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = True)\n",
    "\n",
    "model_IT_P2, optimizer_IT_P2 = generate_model_and_optimizer()\n",
    "model_CL_P2, optimizer_CL_P2 = generate_model_and_optimizer()\n",
    "model_CV_P2, optimizer_CV_P2 = generate_model_and_optimizer()\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "models_P2 = [model_IT_P2, model_CL_P2, model_CV]\n",
    "optimizers_P2 = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators_P2 = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT_P2.pt\", \"RNN_model_CL_P2.pt\", \"RNN_model_CV_P2.pt\"]\n",
    "\n",
    "train_loop(models_P2, optimizers_P2, iterators_P2, N_EPOCHS, label_names, target_field, model_file_names_P2, best_valid_losses_P2, \"P2\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSTRACT USING ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e81f1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1\n",
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    "\n",
    "train_datafield = [(\"title\", None), \n",
    "                   (\"abstract\", TEXT),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, test_iterator_IT = generate_label_iterator(train_data, \"InformationTheory\", validation = False)\n",
    "train_iterator_CL, test_iterator_CL = generate_label_iterator(train_data, \"ComputationalLinguistics\", validation = False)\n",
    "train_iterator_CV, test_iterator_CV = generate_label_iterator(train_data, \"ComputerVision\", validation = False)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edc305e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using ALL data of abstract...\n",
      "Epoch: 01 | Epoch Time: 6m 25s\n",
      "InformationTheory Test Loss: 0.807 | Test Acc: 53.07%\n",
      "Epoch: 02 | Epoch Time: 5m 32s\n",
      "InformationTheory Test Loss: 0.822 | Test Acc: 53.06%\n",
      "Epoch: 03 | Epoch Time: 5m 28s\n",
      "InformationTheory Test Loss: 0.810 | Test Acc: 53.06%\n",
      "Epoch: 04 | Epoch Time: 5m 41s\n",
      "InformationTheory Test Loss: 0.815 | Test Acc: 53.06%\n",
      "Epoch: 05 | Epoch Time: 5m 58s\n",
      "InformationTheory Test Loss: 0.808 | Test Acc: 53.06%\n",
      "Training model for P1_ComputationalLinguistics, using ALL data of abstract...\n",
      "Epoch: 01 | Epoch Time: 5m 57s\n",
      "ComputationalLinguistics Test Loss: 0.491 | Test Acc: 81.38%\n",
      "Epoch: 02 | Epoch Time: 5m 56s\n",
      "ComputationalLinguistics Test Loss: 0.489 | Test Acc: 81.38%\n",
      "Epoch: 03 | Epoch Time: 5m 57s\n",
      "ComputationalLinguistics Test Loss: 0.489 | Test Acc: 81.38%\n",
      "Epoch: 04 | Epoch Time: 5m 56s\n",
      "ComputationalLinguistics Test Loss: 0.490 | Test Acc: 81.38%\n",
      "Epoch: 05 | Epoch Time: 5m 57s\n",
      "ComputationalLinguistics Test Loss: 0.489 | Test Acc: 81.38%\n",
      "Training model for P1_ComputerVision, using ALL data of abstract...\n",
      "Epoch: 01 | Epoch Time: 34m 32s\n",
      "ComputerVision Test Loss: 0.700 | Test Acc: 34.61%\n",
      "Epoch: 02 | Epoch Time: 5m 38s\n",
      "ComputerVision Test Loss: 0.712 | Test Acc: 34.41%\n",
      "Epoch: 03 | Epoch Time: 5m 36s\n",
      "ComputerVision Test Loss: 0.705 | Test Acc: 34.42%\n",
      "Epoch: 04 | Epoch Time: 5m 47s\n",
      "ComputerVision Test Loss: 0.706 | Test Acc: 34.42%\n",
      "Epoch: 05 | Epoch Time: 5m 56s\n",
      "ComputerVision Test Loss: 0.703 | Test Acc: 34.42%\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60adfef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P2_InformationTheory, using ALL data of abstract...\n",
      "Epoch: 01 | Epoch Time: 5m 58s\n",
      "InformationTheory Test Loss: 0.698 | Test Acc: 46.85%\n",
      "Epoch: 02 | Epoch Time: 5m 59s\n",
      "InformationTheory Test Loss: 0.698 | Test Acc: 46.85%\n",
      "Epoch: 03 | Epoch Time: 5m 59s\n",
      "InformationTheory Test Loss: 0.698 | Test Acc: 46.85%\n",
      "Epoch: 04 | Epoch Time: 6m 2s\n",
      "InformationTheory Test Loss: 0.698 | Test Acc: 46.85%\n",
      "Epoch: 05 | Epoch Time: 6m 1s\n",
      "InformationTheory Test Loss: 0.698 | Test Acc: 46.85%\n",
      "Training model for P2_ComputationalLinguistics, using ALL data of abstract...\n",
      "Epoch: 01 | Epoch Time: 309m 6s\n",
      "ComputationalLinguistics Test Loss: 0.707 | Test Acc: 18.68%\n",
      "Epoch: 02 | Epoch Time: 5m 36s\n",
      "ComputationalLinguistics Test Loss: 0.707 | Test Acc: 18.68%\n",
      "Epoch: 03 | Epoch Time: 5m 37s\n",
      "ComputationalLinguistics Test Loss: 0.707 | Test Acc: 18.68%\n",
      "Epoch: 04 | Epoch Time: 6m 0s\n",
      "ComputationalLinguistics Test Loss: 0.707 | Test Acc: 18.68%\n",
      "Epoch: 05 | Epoch Time: 5m 59s\n",
      "ComputationalLinguistics Test Loss: 0.707 | Test Acc: 18.68%\n",
      "Training model for P2_ComputerVision, using ALL data of abstract...\n",
      "Epoch: 01 | Epoch Time: 6m 4s\n",
      "ComputerVision Test Loss: 0.707 | Test Acc: 34.42%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_18300\\2249760207.py\"\u001b[0m, line \u001b[0;32m41\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    train_loop(models_P2, optimizers_P2, iterators_P2, N_EPOCHS, label_names, target_field, model_file_names_P2, best_valid_losses_P2, \"P2\", \"ALL\")\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_18300\\3336522896.py\"\u001b[0m, line \u001b[0;32m28\u001b[0m, in \u001b[0;35mtrain_loop\u001b[0m\n    predictions = model(getattr(batch, target_field)).squeeze(1)\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[0;32m1051\u001b[0m, in \u001b[0;35m_call_impl\u001b[0m\n    return forward_call(*input, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_18300\\610180420.py\"\u001b[0m, line \u001b[0;32m20\u001b[0m, in \u001b[0;35mforward\u001b[0m\n    output, hidden = self.rnn(embedded)\n",
      "  File \u001b[0;32m\"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[0;32m1051\u001b[0m, in \u001b[0;35m_call_impl\u001b[0m\n    return forward_call(*input, **kwargs)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Sunny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\"\u001b[1;36m, line \u001b[1;32m268\u001b[1;36m, in \u001b[1;35mforward\u001b[1;36m\u001b[0m\n\u001b[1;33m    result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#P2\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=\"spacy\", lower=True)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", None), \n",
    "                    (\"abstract\", TEXT),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT_P2, test_iterator_IT_P2 = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = False)\n",
    "train_iterator_CL_P2, test_iterator_CL_P2 = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = False)\n",
    "train_iterator_CV_P2, test_iterator_CV_P2 = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = False)\n",
    "\n",
    "model_IT_P2, optimizer_IT_P2 = generate_model_and_optimizer()\n",
    "model_CL_P2, optimizer_CL_P2 = generate_model_and_optimizer()\n",
    "model_CV_P2, optimizer_CV_P2 = generate_model_and_optimizer()\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "models_P2 = [model_IT_P2, model_CL_P2, model_CV]\n",
    "optimizers_P2 = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators_P2 = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT_P2.pt\", \"RNN_model_CL_P2.pt\", \"RNN_model_CV_P2.pt\"]\n",
    "\n",
    "train_loop(models_P2, optimizers_P2, iterators_P2, N_EPOCHS, label_names, target_field, model_file_names_P2, best_valid_losses_P2, \"P2\", \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0704d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbce58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf7e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1f52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad89d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f964e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712ada5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
