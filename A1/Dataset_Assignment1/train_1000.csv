title,abstract,InformationTheory,ComputationalLinguistics,ComputerVision
Objective-Dependent Uncertainty Driven Retinal Vessel Segmentation,"  From diagnosing neovascular diseases to detecting white matter lesions
accurate tiny vessel segmentation in fundus images is critical. Promising
results for accurate vessel segmentation have been known. However their
effectiveness in segmenting tiny vessels is still limited. In this paper we
study retinal vessel segmentation by incorporating tiny vessel segmentation
into our framework for the overall accurate vessel segmentation. To achieve
this we propose a new deep convolutional neural network (CNN) which divides
vessel segmentation into two separate objectives. Specifically we consider the
overall accurate vessel segmentation and tiny vessel segmentation as two
individual objectives. Then by exploiting the objective-dependent
(homoscedastic) uncertainty we enable the network to learn both objectives
simultaneously. Further to improve the individual objectives we propose: (a)
a vessel weight map based auxiliary loss for enhancing tiny vessel connectivity
(i.e. improving tiny vessel segmentation) and (b) an enhanced encoder-decoder
architecture for improved localization (i.e. for accurate vessel
segmentation). Using 3 public retinal vessel segmentation datasets (CHASE_DB1
DRIVE and STARE) we verify the superiority of our proposed framework in
segmenting tiny vessels (8.3% average improvement in sensitivity) while
achieving better area under the receiver operating characteristic curve (AUC)
compared to state-of-the-art methods.
",0,0,1
SMARTies: Sentiment Models for Arabic Target Entities,"  We consider entity-level sentiment analysis in Arabic a morphologically rich
language with increasing resources. We present a system that is applied to
complex posts written in response to Arabic newspaper articles. Our goal is to
identify important entity ""targets"" within the post along with the polarity
expressed about each target. We achieve significant improvements over multiple
baselines demonstrating that the use of specific morphological representations
improves the performance of identifying both important targets and their
sentiment and that the use of distributional semantic clusters further boosts
performances for these representations especially when richer linguistic
resources are not available.
",0,1,0
State-Aware Tracker for Real-Time Video Object Segmentation,"  In this work we address the task of semi-supervised video object
segmentation(VOS) and explore how to make efficient use of video property to
tackle the challenge of semi-supervision. We propose a novel pipeline called
State-Aware Tracker(SAT) which can produce accurate segmentation results with
real-time speed. For higher efficiency SAT takes advantage of the inter-frame
consistency and deals with each target object as a tracklet. For more stable
and robust performance over video sequences SAT gets awareness for each state
and makes self-adaptation via two feedback loops. One loop assists SAT in
generating more stable tracklets. The other loop helps to construct a more
robust and holistic target representation. SAT achieves a promising result of
72.3% J&F mean with 39 FPS on DAVIS2017-Val dataset which shows a decent
trade-off between efficiency and accuracy. Code will be released at
github.com/MegviiDetection/video_analyst.
",0,0,1
On the Performance of Optimized Dense Device-to-Device Wireless Networks,"  We consider a D2D wireless network where $n$ users are densely deployed in a
squared planar region and communicate with each other without the help of a
wired infrastructure. For this network we examine the 3-phase hierarchical
cooperation (HC) scheme and the 2-phase improved HC scheme based on the concept
of em network multiple access. Exploiting recent results on the optimality
of treating interference as noise in Gaussian interference channels we
optimize the achievable average per-link rate and not just its scaling law. In
addition we provide further improvements on both the previously proposed
hierarchical cooperation schemes by a more efficient use of TDMA and spatial
reuse. Thanks to our explicit achievable rate expressions we can compare HC
scheme with multihop routing (MR) where the latter can be regarded as the
current practice of D2D wireless networks. Our results show that the improved
and optimized HC schemes yield very significant rate gains over MR in realistic
conditions of channel propagation exponents signal to noise ratio and number
of users. This sheds light on the long-standing question about the real
advantage of HC scheme over MR beyond the well-known scaling laws analysis. In
contrast we also show that our rate optimization is non-trivial since when HC
is applied with off-the-shelf choice of the system parameters no significant
rate gain with respect to MR is achieved. We also show that for large pathloss
exponent the sum rate is a nearly linear function of the number of users $n$ in
the range of networks of practical size. This also sheds light on a
long-standing dispute on the effective achievability of linear sum rate scaling
with HC. Finally we notice that the achievable sum rate for large $alpha$ is
much larger than for small $alpha$. This suggests that HC scheme may be a very
effective approach for networks operating at mm-waves.
",1,0,0
"Design of Minimum Correlated Maximal Clique Sets of One-Dimensional
  Uni-polar (Optical) Orthogonal Codes","  This paper proposes an algorithm to search a family of multiple sets of
minimum correlated one dimensional uni-polar (optical) orthogonal codes
(1-DUOC) or optical orthogonal codes (OOC) with fixed as well as variable code
parameters. The cardinality of each set is equal to upper bound. The codes
within a set can be searched for general values of code length code weight
auto-correlation constraint and cross-correlation constraint. Each set forms a
maximal clique of the codes within given range of correlation properties .
These one-dimensional uni-polar orthogonal codes can find their application as
signature sequences for spectral spreading purpose in incoherent optical code
division multiple access (CDMA) systems.
",1,0,0
"General Automatic Human Shape and Motion Capture Using Volumetric
  Contour Cues","  Markerless motion capture algorithms require a 3D body with properly
personalized skeleton dimension and/or body shape and appearance to
successfully track a person. Unfortunately many tracking methods consider
model personalization a different problem and use manual or semi-automatic
model initialization which greatly reduces applicability. In this paper we
propose a fully automatic algorithm that jointly creates a rigged actor model
commonly used for animation - skeleton volumetric shape appearance and
optionally a body surface - and estimates the actor's motion from multi-view
video input only. The approach is rigorously designed to work on footage of
general outdoor scenes recorded with very few cameras and without background
subtraction. Our method uses a new image formation model with analytic
visibility and analytically differentiable alignment energy. For
reconstruction 3D body shape is approximated as Gaussian density field. For
pose and shape estimation we minimize a new edge-based alignment energy
inspired by volume raycasting in an absorbing medium. We further propose a new
statistical human body model that represents the body surface volumetric
Gaussian density as well as variability in skeleton shape. Given any
multi-view sequence our method jointly optimizes the pose and shape parameters
of this model fully automatically in a spatiotemporal way.
",0,0,1
Optimal Detection For Sparse Mixtures,"  Detection of sparse signals arises in a wide range of modern scientific
studies. The focus so far has been mainly on Gaussian mixture models. In this
paper we consider the detection problem under a general sparse mixture model
and obtain an explicit expression for the detection boundary. It is shown that
the fundamental limits of detection is governed by the behavior of the
log-likelihood ratio evaluated at an appropriate quantile of the null
distribution. We also establish the adaptive optimality of the higher criticism
procedure across all sparse mixtures satisfying certain mild regularity
conditions. In particular the general results obtained in this paper recover
and extend in a unified manner the previously known results on sparse detection
far beyond the conventional Gaussian model and other exponential families.
",1,0,0
Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling,"  Recently there has been growing interest in developing learning-based
methods to detect and utilize salient semi-global or global structures such as
junctions lines planes cuboids smooth surfaces and all types of
symmetries for 3D scene modeling and understanding. However the ground truth
annotations are often obtained via human labor which is particularly
challenging and inefficient for such tasks due to the large number of 3D
structure instances (e.g. line segments) and other factors such as viewpoints
and occlusions. In this paper we present a new synthetic dataset
Structured3D with the aim of providing large-scale photo-realistic images with
rich 3D structure annotations for a wide spectrum of structured 3D modeling
tasks. We take advantage of the availability of professional interior designs
and automatically extract 3D structures from them. We generate high-quality
images with an industry-leading rendering engine. We use our synthetic dataset
in combination with real images to train deep networks for room layout
estimation and demonstrate improved performance on benchmark datasets.
",0,0,1
Robust Learning by Self-Transition for Handling Noisy Labels,"  Real-world data inevitably contains noisy labels which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus to counter the label noise challenge we propose a novel
self-transitional learning method called MORPH which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase the network is updated using all the samples to collect a seed
of clean samples. Then in the evolution phase the network is updated using
only the set of arguably clean samples which precisely keeps expanding by the
updated network. Thus MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.
",0,0,1
Exploring Convolutional Networks for End-to-End Visual Servoing,"  Present image based visual servoing approaches rely on extracting hand
crafted visual features from an image. Choosing the right set of features is
important as it directly affects the performance of any approach. Motivated by
recent breakthroughs in performance of data driven methods on recognition and
localization tasks we aim to learn visual feature representations suitable for
servoing tasks in unstructured and unknown environments. In this paper we
present an end-to-end learning based approach for visual servoing in diverse
scenes where the knowledge of camera parameters and scene geometry is not
available a priori. This is achieved by training a convolutional neural network
over color images with synchronised camera poses. Through experiments performed
in simulation and on a quadrotor we demonstrate the efficacy and robustness of
our approach for a wide range of camera poses in both indoor as well as outdoor
environments.
",0,0,1
"Sequence-to-Sequence Predictive Model: From Prosody To Communicative
  Gestures","  Communicative gestures and speech acoustic are tightly linked. Our objective
is to predict the timing of gestures according to the acoustic. That is we
want to predict when a certain gesture occurs. We develop a model based on a
recurrent neural network with attention mechanism. The model is trained on a
corpus of natural dyadic interaction where the speech acoustic and the gesture
phases and types have been annotated. The input of the model is a sequence of
speech acoustic and the output is a sequence of gesture classes. The classes we
are using for the model output is based on a combination of gesture phases and
gesture types. We use a sequence comparison technique to evaluate the model
performance. We find that the model can predict better certain gesture classes
than others. We also perform ablation studies which reveal that fundamental
frequency is a relevant feature for gesture prediction task. In another
sub-experiment we find that including eyebrow movements as acting as beat
gesture improves the performance. Besides we also find that a model trained on
the data of one given speaker also works for the other speaker of the same
conversation. We also perform a subjective experiment to measure how
respondents judge the naturalness the time consistency and the semantic
consistency of the generated gesture timing of a virtual agent. Our respondents
rate the output of our model favorably.
",0,1,0
Aesthetic Attributes Assessment of Images,"  Image aesthetic quality assessment has been a relatively hot topic during the
last decade. Most recently comments type assessment (aesthetic captions) has
been proposed to describe the general aesthetic impression of an image using
text. In this paper we propose Aesthetic Attributes Assessment of Images
which means the aesthetic attributes captioning. This is a new formula of image
aesthetic assessment which predicts aesthetic attributes captions together
with the aesthetic score of each attribute. We introduce a new dataset named
emphDPC-Captions which contains comments of up to 5 aesthetic attributes of
one image through knowledge transfer from a full-annotated small-scale dataset.
Then we propose Aesthetic Multi-Attribute Network (AMAN) which is trained on
a mixture of fully-annotated small-scale PCCD dataset and weakly-annotated
large-scale DPC-Captions dataset. Our AMAN makes full use of transfer learning
and attention model in a single framework. The experimental results on our
DPC-Captions and PCCD dataset reveal that our method can predict captions of 5
aesthetic attributes together with numerical score assessment of each
attribute. We use the evaluation criteria used in image captions to prove that
our specially designed AMAN model outperforms traditional CNN-LSTM model and
modern SCA-CNN model of image captions.
",0,0,1
Toward Guaranteed Illumination Models for Non-Convex Objects,"  Illumination variation remains a central challenge in object detection and
recognition. Existing analyses of illumination variation typically pertain to
convex Lambertian objects and guarantee quality of approximation in an
average case sense. We show that it is possible to build V(vertex)-description
convex cone models with worst-case performance guarantees for non-convex
Lambertian objects. Namely a natural verification test based on the angle to
the constructed cone guarantees to accept any image which is sufficiently
well-approximated by an image of the object under some admissible lighting
condition and guarantees to reject any image that does not have a sufficiently
good approximation. The cone models are generated by sampling point
illuminations with sufficient density which follows from a new perturbation
bound for point images in the Lambertian model. As the number of point images
required for guaranteed verification may be large we introduce a new
formulation for cone preserving dimensionality reduction which leverages tools
from sparse and low-rank decomposition to reduce the complexity while
controlling the approximation error with respect to the original cone.
",0,0,1
"No Answer is Better Than Wrong Answer: A Reflection Model for Document
  Level Machine Reading Comprehension","  The Natural Questions (NQ) benchmark set brings new challenges to Machine
Reading Comprehension: the answers are not only at different levels of
granularity (long and short) but also of richer types (including no-answer
yes/no single-span and multi-span). In this paper we target at this challenge
and handle all answer types systematically. In particular we propose a novel
approach called Reflection Net which leverages a two-step training procedure to
identify the no-answer and wrong-answer cases. Extensive experiments are
conducted to verify the effectiveness of our approach. At the time of paper
writing (May.~20~2020) our approach achieved the top 1 on both long and short
answer leaderboard with F1 scores of 77.2 and 64.1 respectively.
",0,1,0
"Truly shift-equivariant convolutional neural networks with adaptive
  polyphase upsampling","  Convolutional neural networks lack shift equivariance due to the presence of
downsampling layers. In image classification adaptive polyphase downsampling
(APS-D) was recently proposed to make CNNs perfectly shift invariant. However
in networks used for image reconstruction tasks it can not by itself restore
shift equivariance. We address this problem by proposing adaptive polyphase
upsampling (APS-U) a non-linear extension of conventional upsampling which
allows CNNs with symmetric encoder-decoder architecture (for example U-Net) to
exhibit perfect shift equivariance. With MRI and CT reconstruction experiments
we show that networks containing APS-D/U layers exhibit state of the art
equivariance performance without sacrificing on image reconstruction quality.
In addition unlike prior methods like data augmentation and anti-aliasing the
gains in equivariance obtained from APS-D/U also extend to images outside the
training distribution.
",0,0,1
"On the Correlation between Boolean Functions of Sequences of Random
  Variables","  In this paper we establish a new inequality tying together the effective
length and the maximum correlation between the outputs of an arbitrary pair of
Boolean functions which operate on two sequences of correlated random
variables. We derive a new upper-bound on the correlation between the outputs
of these functions. The upper-bound is useful in various disciplines which deal
with common-information. We build upon Witsenhausen's bound on
maximum-correlation. The previous upper-bound did not take the effective length
of the Boolean functions into account.
",1,0,0
"Using Sentiment Induction to Understand Variation in Gendered Online
  Communities","  We analyze gendered communities defined in three different ways: text users
and sentiment. Differences across these representations reveal facets of
communities' distinctive identities such as social group topic and
attitudes. Two communities may have high text similarity but not user
similarity or vice versa and word usage also does not vary according to a
clearcut binary perspective of gender. Community-specific sentiment lexicons
demonstrate that sentiment can be a useful indicator of words' social meaning
and community values especially in the context of discussion content and user
demographics. Our results show that social platforms such as Reddit are active
settings for different constructions of gender.
",0,1,0
"Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic
  Reinforcement Learning","  One of the great promises of robot learning systems is that they will be able
to learn from their mistakes and continuously adapt to ever-changing
environments. Despite this potential most of the robot learning systems today
are deployed as a fixed policy and they are not being adapted after their
deployment. Can we efficiently adapt previously learned behaviors to new
environments objects and percepts in the real world? In this paper we present
a method and empirical evidence towards a robot learning framework that
facilitates continuous adaption. In particular we demonstrate how to adapt
vision-based robotic manipulation policies to new variations by fine-tuning via
off-policy reinforcement learning including changes in background object
shape and appearance lighting conditions and robot morphology. Further this
adaptation uses less than 0.2% of the data necessary to learn the task from
scratch. We find that our approach of adapting pre-trained policies leads to
substantial performance gains over the course of fine-tuning and that
pre-training via RL is essential: training from scratch or adapting from
supervised ImageNet features are both unsuccessful with such small amounts of
data. We also find that these positive results hold in a limited continual
learning setting in which we repeatedly fine-tune a single lineage of policies
using data from a succession of new tasks. Our empirical conclusions are
consistently supported by experiments on simulated manipulation tasks and by
52 unique fine-tuning experiments on a real robotic grasping system pre-trained
on 580000 grasps.
",0,0,1
"Answering questions by learning to rank -- Learning to rank by answering
  questions","  Answering multiple-choice questions in a setting in which no supporting
documents are explicitly provided continues to stand as a core problem in
natural language processing. The contribution of this article is two-fold.
First it describes a method which can be used to semantically rank documents
extracted from Wikipedia or similar natural language corpora. Second we
propose a model employing the semantic ranking that holds the first place in
two of the most popular leaderboards for answering multiple-choice questions:
ARC Easy and Challenge. To achieve this we introduce a self-attention based
neural network that latently learns to rank documents by their importance
related to a given question whilst optimizing the objective of predicting the
correct answer. These documents are considered relevant contexts for the
underlying question. We have published the ranked documents so that they can be
used off-the-shelf to improve downstream decision models.
",0,1,0
Progressive Pose Attention Transfer for Person Image Generation,"  This paper proposes a new generative adversarial network for pose transfer
i.e. transferring the pose of a given person to a target pose. The generator
of the network comprises a sequence of Pose-Attentional Transfer Blocks that
each transfers certain regions it attends to generating the person image
progressively. Compared with those in previous works our generated person
images possess better appearance consistency and shape consistency with the
input images thus significantly more realistic-looking. The efficacy and
efficiency of the proposed network are validated both qualitatively and
quantitatively on Market-1501 and DeepFashion. Furthermore the proposed
architecture can generate training images for person re-identification
alleviating data insufficiency. Codes and models are available at:
https://github.com/tengteng95/Pose-Transfer.git.
",0,0,1
"A Segmentation-Oriented Inter-Class Transfer Method: Application to
  Retinal Vessel Segmentation","  Retinal vessel segmentation as a principal nonintrusive diagnose method for
ophthalmology diseases or diabetics suffers from data scarcity due to
requiring pixel-wise labels. In this paper we proposed a convenient
patch-based two-stage transfer method. First based on the information
bottleneck theory we insert one dimensionality-reduced layer for task-specific
feature space. Next the semi-supervised clustering is conducted to select
instances from different sources databases possessing similarities in the
feature space. Surprisingly we empirically demonstrate that images from
different classes possessing similarities contribute to better performance than
some same-class instances. The proposed framework achieved an accuracy of 97%
96.8% and 96.77% on DRIVE STARE and HRF respectively outperforming current
methods and independent human observers (DRIVE (96.37%) and STARE (93.39%)).
",0,0,1
Structured Knowledge Discovery from Massive Text Corpus,"  Nowadays with the booming development of the Internet people benefit from
its convenience due to its open and sharing nature. A large volume of natural
language texts is being generated by users in various forms such as search
queries documents and social media posts. As the unstructured text corpus is
usually noisy and messy it becomes imperative to correctly identify and
accurately annotate structured information in order to obtain meaningful
insights or better understand unstructured texts. On the other hand the
existing structured information which embodies our knowledge such as entity or
concept relations often suffers from incompleteness or quality-related issues.
Given a gigantic collection of texts which offers rich semantic information it
is also important to harness the massiveness of the unannotated text corpus to
expand and refine existing structured knowledge with fewer annotation efforts.
  In this dissertation I will introduce principles models and algorithms for
effective structured knowledge discovery from the massive text corpus. We are
generally interested in obtaining insights and better understanding
unstructured texts with the help of structured annotations or by
structure-aware modeling. Also given the existing structured knowledge we are
interested in expanding its scale and improving its quality harnessing the
massiveness of the text corpus. In particular four problems are studied in
this dissertation: Structured Intent Detection for Natural Language
Understanding Structure-aware Natural Language Modeling Generative Structured
Knowledge Expansion and Synonym Refinement on Structured Knowledge.
",0,1,0
Targeted Attack for Deep Hashing based Retrieval,"  The deep hashing based retrieval method is widely adopted in large-scale
image and video retrieval. However there is little investigation on its
security. In this paper we propose a novel method dubbed deep hashing
targeted attack (DHTA) to study the targeted attack on such retrieval.
Specifically we first formulate the targeted attack as a point-to-set
optimization which minimizes the average distance between the hash code of an
adversarial example and those of a set of objects with the target label. Then
we design a novel component-voting scheme to obtain an anchor code as the
representative of the set of hash codes of objects with the target label whose
optimality guarantee is also theoretically derived. To balance the performance
and perceptibility we propose to minimize the Hamming distance between the
hash code of the adversarial example and the anchor code under the
$ell^infty$ restriction on the perturbation. Extensive experiments verify
that DHTA is effective in attacking both deep hashing based image retrieval and
video retrieval.
",0,0,1
Gated Fusion Network for Degraded Image Super Resolution,"  Single image super resolution aims to enhance image quality with respect to
spatial content which is a fundamental task in computer vision. In this work
we address the task of single frame super resolution with the presence of image
degradation e.g. blur haze or rain streaks. Due to the limitations of frame
capturing and formation processes image degradation is inevitable and the
artifacts would be exacerbated by super resolution methods. To address this
problem we propose a dual-branch convolutional neural network to extract base
features and recovered features separately. The base features contain local and
global information of the input image. On the other hand the recovered
features focus on the degraded regions and are used to remove the degradation.
Those features are then fused through a recursive gate module to obtain sharp
features for super resolution. By decomposing the feature extraction step into
two task-independent streams the dual-branch model can facilitate the training
process by avoiding learning the mixed degradation all-in-one and thus enhance
the final high-resolution prediction results. We evaluate the proposed method
in three degradation scenarios. Experiments on these scenarios demonstrate that
the proposed method performs more efficiently and favorably against the
state-of-the-art approaches on benchmark datasets.
",0,0,1
XVFI: eXtreme Video Frame Interpolation,"  In this paper we firstly present a dataset (X4K1000FPS) of 4K videos of 1000
fps with the extreme motion to the research community for video frame
interpolation (VFI) and propose an extreme VFI network called XVFI-Net that
first handles the VFI for 4K videos with large motion. The XVFI-Net is based on
a recursive multi-scale shared structure that consists of two cascaded modules
for bidirectional optical flow learning between two input frames (BiOF-I) and
for bidirectional optical flow learning from target to input frames (BiOF-T).
The optical flows are stably approximated by a complementary flow reversal
(CFR) proposed in BiOF-T module. During inference the BiOF-I module can start
at any scale of input while the BiOF-T module only operates at the original
input scale so that the inference can be accelerated while maintaining highly
accurate VFI performance. Extensive experimental results show that our XVFI-Net
can successfully capture the essential information of objects with extremely
large motions and complex textures while the state-of-the-art methods exhibit
poor performance. Furthermore our XVFI-Net framework also performs comparably
on the previous lower resolution benchmark dataset which shows a robustness of
our algorithm as well. All source codes pre-trained models and proposed
X4K1000FPS datasets are publicly available at
https://github.com/JihyongOh/XVFI.
",0,0,1
Detecting Hostile Posts using Relational Graph Convolutional Network,"  This work is based on the submission to the competition Hindi Constraint
conducted by AAAI@2021 for detection of hostile posts in Hindi on social media
platforms. Here a model is presented for detection and classification of
hostile posts and further classify into fake offensive hate and defamation
using Relational Graph Convolutional Networks. Unlike other existing work our
approach is focused on using semantic meaning along with contextutal
information for better classification. The results from AAAI@2021 indicates
that the proposed model is performing at par with Google's XLM-RoBERTa on the
given dataset. Our best submission with RGCN achieves an F1 score of 0.97 (7th
Rank) on coarse-grained evaluation and achieved best performance on identifying
fake posts. Among all submissions to the challenge our classification system
with XLM-Roberta secured 2nd rank on fine-grained classification.
",0,1,0
Regularized Forward-Backward Decoder for Attention Models,"  Nowadays attention models are one of the popular candidates for speech
recognition. So far many studies mainly focus on the encoder structure or the
attention module to enhance the performance of these models. However mostly
ignore the decoder. In this paper we propose a novel regularization technique
incorporating a second decoder during the training phase. This decoder is
optimized on time-reversed target labels beforehand and supports the standard
decoder during training by adding knowledge from future context. Since it is
only added during training we are not changing the basic structure of the
network or adding complexity during decoding. We evaluate our approach on the
smaller TEDLIUMv2 and the larger LibriSpeech dataset achieving consistent
improvements on both of them.
",0,1,0
"NICT's Neural and Statistical Machine Translation Systems for the WMT18
  News Translation Task","  This paper presents the NICT's participation to the WMT18 shared news
translation task. We participated in the eight translation directions of four
language pairs: Estonian-English Finnish-English Turkish-English and
Chinese-English. For each translation direction we prepared state-of-the-art
statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems
were trained with the transformer architecture using the provided parallel data
enlarged with a large quantity of back-translated monolingual data that we
generated with a new incremental training framework. Our primary submissions to
the task are the result of a simple combination of our SMT and NMT systems. Our
systems are ranked first for the Estonian-English and Finnish-English language
pairs (constraint) according to BLEU-cased.
",0,1,0
The Second-Order Coding Rate of the MIMO Rayleigh Block-Fading Channel,"  The second-order coding rate of the multiple-input multiple-output (MIMO)
quasi-static Rayleigh fading channel is studied. We tackle this problem via an
information-spectrum approach and statistical bounds based on recent random
matrix theory techniques. We derive a central limit theorem (CLT) to analyze
the information density in the regime where the block-length n and the number
of transmit and receive antennas K and N respectively grow simultaneously
large. This result leads to the characterization of closed-form upper and lower
bounds on the optimal average error probability when the coding rate is within
O((nK)^-1/2) of the asymptotic capacity.
",1,0,0
"Group-blind detection with very large antenna arrays in the presence of
  pilot contamination","  Massive MIMO is in general severely affected by pilot contamination. As
opposed to traditional detectors we propose a group-blind detector that takes
into account the presence of pilot contamination. While sticking to the
traditional structure of the training phase where orthogonal pilot sequences
are reused we use the excess antennas at each base station to partially remove
interference during the uplink data transmission phase. We analytically derive
the asymptotic SINR achievable with group-blind detection and confirm our
findings by simulations. We show in particular that in an
interference-limited scenario with one dominant interfering cell the SINR can
be doubled compared to non-group-blind detection.
",1,0,0
Analysis of Nakamoto Consensus Revisited,"  In the Bitcoin white paper Nakamoto proposed a very simple Byzantine fault
tolerant consensus algorithm that is also known as Nakamoto consensus. Despite
its simplicity some existing analysis of Nakamoto consensus appears to be long
and involved. In this technical report we aim to make such analysis simple and
transparent so that we can teach senior undergraduate students and graduate
students in our institutions. This report is largely based on a 3-hour tutorial
given by one of the authors in June 2019.
",1,0,0
"Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual
  Information Constraint","  Convolutional neural network (CNN) have proven its success for semantic
segmentation which is a core task of emerging industrial applications such as
autonomous driving. However most progress in semantic segmentation of urban
scenes is reported on standard scenarios i.e. daytime scenes with favorable
illumination conditions. In practical applications the outdoor weather and
illumination are changeable e.g. cloudy and nighttime which results in a
significant drop of semantic segmentation accuracy of CNN only trained with
daytime data. In this paper we propose a novel generative adversarial network
(namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained
neural network is applied to videos captured under adverse weather conditions.
The proposed Mutual-GAN adopts mutual information constraint to preserve
image-objects during cross-weather adaptation which is an unsolved problem for
most unsupervised image-to-image translation approaches (e.g. CycleGAN). The
proposed Mutual-GAN is evaluated on two publicly available driving video
datasets (i.e. CamVid and SYNTHIA). The experimental results demonstrate that
our Mutual-GAN can yield visually plausible translated images and significantly
improve the semantic segmentation accuracy of daytime-trained deep learning
network while processing videos under challenging weathers.
",0,0,1
Expectation Propagation Line Spectral Estimation,"  Line spectral estimation (LSE) is a fundamental problem in signal processing
fields as it arises in various fields such as radar signal processing and
communication fields. This paper develops expectation propagation (EP) based
LSE (EPLSE) method. The proposed method automatically estimates the model
order noise variance and can deal with the nonlinear measurements. Numerical
experiments show the excellent performance of EPLSE.
",1,0,0
SIFT Meets CNN: A Decade Survey of Instance Retrieval,"  In the early days content-based image retrieval (CBIR) was studied with
global features. Since 2003 image retrieval based on local descriptors (de
facto SIFT) has been extensively studied for over a decade due to the advantage
of SIFT in dealing with image transformations. Recently image representations
based on the convolutional neural network (CNN) have attracted increasing
interest in the community and demonstrated impressive performance. Given this
time of rapid evolution this article provides a comprehensive survey of
instance retrieval over the last decade. Two broad categories SIFT-based and
CNN-based methods are presented. For the former according to the codebook
size we organize the literature into using large/medium-sized/small codebooks.
For the latter we discuss three lines of methods i.e. using pre-trained or
fine-tuned CNN models and hybrid methods. The first two perform a single-pass
of an image to the network while the last category employs a patch-based
feature extraction scheme. This survey presents milestones in modern instance
retrieval reviews a broad selection of previous works in different categories
and provides insights on the connection between SIFT and CNN-based methods.
After analyzing and comparing retrieval performance of different categories on
several datasets we discuss promising directions towards generic and
specialized instance retrieval.
",0,0,1
"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural
  Networks","  We propose two efficient approximations to standard convolutional neural
networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks
the filters are approximated with binary values resulting in 32x memory saving.
In XNOR-Networks both the filters and the input to convolutional layers are
binary. XNOR-Networks approximate convolutions using primarily binary
operations. This results in 58x faster convolutional operations and 32x memory
savings. XNOR-Nets offer the possibility of running state-of-the-art networks
on CPUs (rather than GPUs) in real-time. Our binary networks are simple
accurate efficient and work on challenging visual tasks. We evaluate our
approach on the ImageNet classification task. The classification accuracy with
a Binary-Weight-Network version of AlexNet is only 2.9% less than the
full-precision AlexNet (in top-1 measure). We compare our method with recent
network binarization methods BinaryConnect and BinaryNets and outperform
these methods by large margins on ImageNet more than 16% in top-1 accuracy.
",0,0,1
"No-Frills Human-Object Interaction Detection: Factorization Layout
  Encodings and Training Techniques","  We show that for human-object interaction detection a relatively simple
factorized model with appearance and layout encodings constructed from
pre-trained object detectors outperforms more sophisticated approaches. Our
model includes factors for detection scores human and object appearance and
coarse (box-pair configuration) and optionally fine-grained layout (human
pose). We also develop training techniques that improve learning efficiency by:
(1) eliminating a train-inference mismatch; (2) rejecting easy negatives during
mini-batch training; and (3) using a ratio of negatives to positives that is
two orders of magnitude larger than existing approaches. We conduct a thorough
ablation study to understand the importance of different factors and training
techniques using the challenging HICO-Det dataset.
",0,0,1
DashCam Pay: A System for In-vehicle Payments Using Face and Voice,"  We present our ongoing work on developing a system called DashCam Pay that
enables in-vehicle payments in a seamless and secure manner using face and
voice biometrics. A plug-and-play device (dashcam) mounted in the vehicle is
used to capture face images and voice commands of passengers.
Privacy-preserving biometric comparison techniques are used to compare the
biometric data captured by the dashcam with the biometric data enrolled on the
users' mobile devices over a wireless interface (e.g. Bluetooth or Wi-Fi
Direct) to determine the payer. Once the payer is identified payment is
conducted using the enrolled payment credential on the mobile device of the
payer. We conduct preliminary analysis on data collected using a commercially
available dashcam to show the feasibility of building the proposed system. A
prototype of the proposed system is also developed in Android. DashCam Pay can
be integrated as a software solution by dashcam or vehicle manufacturers to
enable open loop in-vehicle payments.
",0,0,1
Causally Regularized Learning with Agnostic Data Selection Bias,"  Most of previous machine learning algorithms are proposed based on the i.i.d.
hypothesis. However this ideal assumption is often violated in real
applications where selection bias may arise between training and testing
process. Moreover in many scenarios the testing data is not even available
during the training process which makes the traditional methods like transfer
learning infeasible due to their need on prior of test distribution. Therefore
how to address the agnostic selection bias for robust model learning is of
paramount importance for both academic research and real applications. In this
paper under the assumption that causal relationships among variables are
robust across domains we incorporate causal technique into predictive modeling
and propose a novel Causally Regularized Logistic Regression (CRLR) algorithm
by jointly optimize global confounder balancing and weighted logistic
regression. Global confounder balancing helps to identify causal features
whose causal effect on outcome are stable across domains then performing
logistic regression on those causal features constructs a robust predictive
model against the agnostic bias. To validate the effectiveness of our CRLR
algorithm we conduct comprehensive experiments on both synthetic and real
world datasets. Experimental results clearly demonstrate that our CRLR
algorithm outperforms the state-of-the-art methods and the interpretability of
our method can be fully depicted by the feature visualization.
",0,0,1
Dictionary learning of sound speed profiles,"  To provide constraints on their inversion ocean sound speed profiles (SSPs)
often are modeled using empirical orthogonal functions (EOFs). However this
regularization which uses the leading order EOFs with a minimum-energy
constraint on their coefficients often yields low resolution SSP estimates. In
this paper it is shown that dictionary learning a form of unsupervised
machine learning can improve SSP resolution by generating a dictionary of
shape functions for sparse processing (e.g. compressive sensing) that optimally
compress SSPs; both minimizing the reconstruction error and the number of
coefficients. These learned dictionaries (LDs) are not constrained to be
orthogonal and thus fit the given signals such that each signal example is
approximated using few LD entries. Here LDs describing SSP observations from
the HF-97 experiment and the South China Sea are generated using the K-SVD
algorithm. These LDs better explain SSP variability and require fewer
coefficients than EOFs describing much of the variability with one
coefficient. Thus LDs improve the resolution of SSP estimates with negligible
computational burden.
",1,0,0
Cognitive Green Radio for Energy-Aware Communications,"  In 5G networks the number of connected devices data rate and data volume
per area as well as the variety of QoS requirements will attain unprecedented
scales. The achievement of these goals will rely on new technologies and
disruptive changes in network architecture and node design. Energy efficiency
is believed to play a key role in complementing the 5G technologies and
optimizing their deployment dynamic configuration and management [1]. Within
the framework of green communications and networks especially for
next-generation green cellular radio access networks the GREAT (Green
Cognitive Radio for Energy-Aware wireless communication Technologies evolution)
initiative a CominLabs Excellence Center (Laboratoire d'Excellence) and
Universit'e Europ'eenne de Bretagne (UEB)project has mainly addressed the
fundamental issues of energy efficiency from various perspectives and angles
leveraging on cognitive techniques at networking level as well as at
thephysical layer level.
",1,0,0
"Why ReLU networks yield high-confidence predictions far away from the
  training data and how to mitigate the problem","  Classifiers used in the wild in particular for safety-critical systems
should not only have good generalization properties but also should know when
they don't know in particular make low confidence predictions far away from
the training data. We show that ReLU type neural networks which yield a
piecewise linear classifier function fail in this regard as they produce almost
always high confidence predictions far away from the training data. For bounded
domains like images we propose a new robust optimization technique similar to
adversarial training which enforces low confidence predictions far away from
the training data. We show that this technique is surprisingly effective in
reducing the confidence of predictions far away from the training data while
maintaining high confidence predictions and test error on the original
classification task compared to standard training.
",0,0,1
"Plan Optimization to Bilingual Dictionary Induction for Low-Resource
  Language Families","  Creating bilingual dictionary is the first crucial step in enriching
low-resource languages. Especially for the closely-related ones it has been
shown that the constraint-based approach is useful for inducing bilingual
lexicons from two bilingual dictionaries via the pivot language. However if
there are no available machine-readable dictionaries as input we need to
consider manual creation by bilingual native speakers. To reach a goal of
comprehensively create multiple bilingual dictionaries even if we already have
several existing machine-readable bilingual dictionaries it is still difficult
to determine the execution order of the constraint-based approach to reducing
the total cost. Plan optimization is crucial in composing the order of
bilingual dictionaries creation with the consideration of the methods and their
costs. We formalize the plan optimization for creating bilingual dictionaries
by utilizing Markov Decision Process (MDP) with the goal to get a more accurate
estimation of the most feasible optimal plan with the least total cost before
fully implementing the constraint-based bilingual lexicon induction. We model a
prior beta distribution of bilingual lexicon induction precision with language
similarity and polysemy of the topology as $alpha$ and $beta$ parameters. It
is further used to model cost function and state transition probability. We
estimated the cost of all investment plan as a baseline for evaluating the
proposed MDP-based approach with total cost as an evaluation metric. After
utilizing the posterior beta distribution in the first batch of experiments to
construct the prior beta distribution in the second batch of experiments the
result shows 61.5% of cost reduction compared to the estimated all investment
plan and 39.4% of cost reduction compared to the estimated MDP optimal plan.
The MDP-based proposal outperformed the baseline on the total cost.
",0,1,0
"What Doesn't Kill You Makes You Robust(er): How to Adversarially Train
  against Data Poisoning","  Data poisoning is a threat model in which a malicious actor tampers with
training data to manipulate outcomes at inference time. A variety of defenses
against this threat model have been proposed but each suffers from at least
one of the following flaws: they are easily overcome by adaptive attacks they
severely reduce testing performance or they cannot generalize to diverse data
poisoning threat models. Adversarial training and its variants are currently
considered the only empirically strong defense against (inference-time)
adversarial attacks. In this work we extend the adversarial training framework
to defend against (training-time) data poisoning including targeted and
backdoor attacks. Our method desensitizes networks to the effects of such
attacks by creating poisons during training and injecting them into training
batches. We show that this defense withstands adaptive attacks generalizes to
diverse threat models and incurs a better performance trade-off than previous
defenses such as DP-SGD or (evasion) adversarial training.
",0,0,1
A Universal Model for Cross Modality Mapping by Relational Reasoning,"  With the aim of matching a pair of instances from two different modalities
cross modality mapping has attracted growing attention in the computer vision
community. Existing methods usually formulate the mapping function as the
similarity measure between the pair of instance features which are embedded to
a common space. However we observe that the relationships among the instances
within a single modality (intra relations) and those between the pair of
heterogeneous instances (inter relations) are insufficiently explored in
previous approaches. Motivated by this we redefine the mapping function with
relational reasoning via graph modeling and further propose a GCN-based
Relational Reasoning Network (RR-Net) in which inter and intra relations are
efficiently computed to universally resolve the cross modality mapping problem.
Concretely we first construct two kinds of graph i.e. Intra Graph and Inter
Graph to respectively model intra relations and inter relations. Then RR-Net
updates all the node features and edge features in an iterative manner for
learning intra and inter relations simultaneously. Last RR-Net outputs the
probabilities over the edges which link a pair of heterogeneous instances to
estimate the mapping results. Extensive experiments on three example tasks
i.e. image classification social recommendation and sound recognition
clearly demonstrate the superiority and universality of our proposed model.
",0,0,1
"An Efficient Approximate kNN Graph Method for Diffusion on Image
  Retrieval","  The application of the diffusion in many computer vision and artificial
intelligence projects has been shown to give excellent improvements in
performance. One of the main bottlenecks of this technique is the quadratic
growth of the kNN graph size due to the high-quantity of new connections
between nodes in the graph resulting in long computation times. Several
strategies have been proposed to address this but none are effective and
efficient. Our novel technique based on LSH projections obtains the same
performance as the exact kNN graph after diffusion but in less time
(approximately 18 times faster on a dataset of a hundred thousand images). The
proposed method was validated and compared with other state-of-the-art on
several public image datasets including Oxford5k Paris6k and Oxford105k.
",0,0,1
Diving deeper into mentee networks,"  Modern computer vision is all about the possession of powerful image
representations. Deeper and deeper convolutional neural networks have been
built using larger and larger datasets and are made publicly available. A large
swath of computer vision scientists use these pre-trained networks with varying
degrees of successes in various tasks. Even though there is tremendous success
in copying these networks the representational space is not learnt from the
target dataset in a traditional manner. One of the reasons for opting to use a
pre-trained network over a network learnt from scratch is that small datasets
provide less supervision and require meticulous regularization smaller and
careful tweaking of learning rates to even achieve stable learning without
weight explosion. It is often the case that large deep networks are not
portable which necessitates the ability to learn mid-sized networks from
scratch.
  In this article we dive deeper into training these mid-sized networks on
small datasets from scratch by drawing additional supervision from a large
pre-trained network. Such learning also provides better generalization
accuracies than networks trained with common regularization techniques such as
l2 l1 and dropouts. We show that features learnt thus are more general than
those learnt independently. We studied various characteristics of such networks
and found some interesting behaviors.
",0,0,1
GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training,"  Anomaly detection is a classical problem in computer vision namely the
determination of the normal from the abnormal when datasets are highly biased
towards one class (normal) due to the insufficient sample size of the other
class (abnormal). While this can be addressed as a supervised learning problem
a significantly more challenging problem is that of detecting the
unknown/unseen anomaly case that takes us instead into the space of a
one-class semi-supervised learning paradigm. We introduce such a novel anomaly
detection model by using a conditional generative adversarial network that
jointly learns the generation of high-dimensional image space and the inference
of latent space. Employing encoder-decoder-encoder sub-networks in the
generator network enables the model to map the input image to a lower dimension
vector which is then used to reconstruct the generated output image. The use
of the additional encoder network maps this generated image to its latent
representation. Minimizing the distance between these images and the latent
vectors during training aids in learning the data distribution for the normal
samples. As a result a larger distance metric from this learned data
distribution at inference time is indicative of an outlier from that
distribution - an anomaly. Experimentation over several benchmark datasets
from varying domains shows the model efficacy and superiority over previous
state-of-the-art approaches.
",0,0,1
"Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs
  for Robust Speech Recognition","  The goal of self-supervised learning (SSL) for automatic speech recognition
(ASR) is to learn good speech representations from a large amount of unlabeled
speech for the downstream ASR task. However most SSL frameworks do not
consider noise robustness which is crucial for real-world applications. In this
paper we propose wav2vec-Switch a method to encode noise robustness into
contextualized representations of speech via contrastive learning.
Specifically we feed original-noisy speech pairs simultaneously into the
wav2vec 2.0 network. In addition to the existing contrastive learning task we
switch the quantized representations of the original and noisy speech as
additional prediction targets of each other. By doing this it enforces the
network to have consistent predictions for the original and noisy speech thus
allows to learn contextualized representation with noise robustness. Our
experiments on synthesized and real noisy data show the effectiveness of our
method: it achieves 2.9--4.9% relative word error rate (WER) reduction on the
synthesized noisy LibriSpeech data without deterioration on the original data
and 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation
baseline even with a strong language model for decoding. Our results on CHiME-4
can match or even surpass those with well-designed speech enhancement
components.
",0,1,0
Context-Aware Zero-Shot Learning for Object Recognition,"  Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging
auxiliary knowledge such as semantic representations. A limitation of previous
approaches is that only intrinsic properties of objects e.g. their visual
appearance are taken into account while their context e.g. the surrounding
objects in the image is ignored. Following the intuitive principle that
objects tend to be found in certain contexts but not others we propose a new
and challenging approach context-aware ZSL that leverages semantic
representations in a new way to model the conditional likelihood of an object
to appear in a given context. Finally through extensive experiments conducted
on Visual Genome we show that contextual information can substantially improve
the standard ZSL approach and is robust to unbalanced classes.
",0,0,1
"Texture Enhancement via High-Resolution Style Transfer for Single-Image
  Super-Resolution","  Recently various deep-neural-network (DNN)-based approaches have been
proposed for single-image super-resolution (SISR). Despite their promising
results on major structure regions such as edges and lines they still suffer
from limited performance on texture regions that consist of very complex and
fine patterns. This is because during the acquisition of a low-resolution (LR)
image via down-sampling these regions lose most of the high frequency
information necessary to represent the texture details. In this paper we
present a novel texture enhancement framework for SISR to effectively improve
the spatial resolution in the texture regions as well as edges and lines. We
call our method high-resolution (HR) style transfer algorithm. Our framework
consists of three steps: (i) generate an initial HR image from an interpolated
LR image via an SISR algorithm (ii) generate an HR style image from the
initial HR image via down-scaling and tiling and (iii) combine the HR style
image with the initial HR image via a customized style transfer algorithm.
Here the HR style image is obtained by down-scaling the initial HR image and
then repetitively tiling it into an image of the same size as the HR image.
This down-scaling and tiling process comes from the idea that texture regions
are often composed of small regions that similar in appearance albeit sometimes
different in scale. This process creates an HR style image that is rich in
details which can be used to restore high-frequency texture details back into
the initial HR image via the style transfer algorithm. Experimental results on
a number of texture datasets show that our proposed HR style transfer algorithm
provides more visually pleasing results compared with competitive methods.
",0,0,1
Controlling Weather Field Synthesis Using Variational Autoencoders,"  One of the consequences of climate change is anobserved increase in the
frequency of extreme cli-mate events. That poses a challenge for
weatherforecast and generation algorithms which learnfrom historical data but
should embed an often un-certain bias to create correct scenarios. This
paperinvestigates how mapping climate data to a knowndistribution using
variational autoencoders mighthelp explore such biases and control the
synthesisof weather fields towards more extreme climatescenarios. We
experimented using a monsoon-affected precipitation dataset from southwest
In-dia which should give a roughly stable pattern ofrainy days and ease our
investigation. We reportcompelling results showing that mapping complexweather
data to a known distribution implementsan efficient control for weather field
synthesis to-wards more (or less) extreme scenarios.
",0,0,1
"Spitzoid Lesions Diagnosis based on GA feature selection and Random
  Forest","  Spitzoid lesions broadly categorized into Spitz Nevus (SN) Atypical Spitz
Tumors (AST) and Spitz Melanomas (SM). The accurate diagnosis of these lesions
is one of the most challenges for dermapathologists; this is due to the high
similarities between them. Data mining techniques are successfully applied to
situations like these where complexity exists. This study aims to develop an
artificial intelligence model to support the diagnosis of Spitzoid lesions. A
private spitzoid lesions dataset have been used to evaluate the system proposed
in this study. The proposed system has three stages. In the first stage SMOTE
method applied to solve the imbalance data problem in the second stage in
order to eliminate irrelevant features; genetic algorithm is used to select
significant features. This later reduces the computational complexity and speed
up the data mining process. In the third stage Random forest classifier is
employed to make a decision for two different categories of lesions (Spitz
nevus or Atypical Spitz Tumors). The performance of our proposed scheme is
evaluated using accuracy sensitivity specificity G-mean F- measure ROC and
AUC. Results obtained with our SMOTE-GA-RF model with GA-based 16 features show
a great performance with accuracy 0.97 F-measure 0.98 AUC 0.98 and G-mean
0.97.Results obtained in this study have potential to open new opportunities in
diagnosis of spitzoid lesions.
",0,0,1
"9-variable Boolean Functions with Nonlinearity 242 in the Generalized
  Rotation Class","  In 2006 9-variable Boolean functions having nonlinearity 241 which is
strictly greater than the bent concatenation bound of 240 have been discovered
in the class of Rotation Symmetric Boolean Functions (RSBFs) by Kavut Maitra
and Yucel. To improve this nonlinearity result we have firstly defined some
subsets of the n-variable Boolean functions as the ""generalized classes of
k-RSBFs and k-DSBFs (k-Dihedral Symmetric Boolean Functions)"" where k is a
positive integer dividing n and k-RSBFs is a subset of l-RSBFs if k < l.
Secondly utilizing the steepest-descent like iterative heuristic search
algorithm used previously to identify the 9-variable RSBFs with nonlinearity
241 we have made a search within the classes of 3-RSBFs and 3-DSBFs. The
search has accomplished to find 9-variable Boolean functions with nonlinearity
242 in both of these classes. It should be emphasized that although the class
of 3-RSBFs contains functions with nonlinearity 242; 1-RSBFs or simply RSBFs
which is a subset of 3-RSBFs does not contain any. This result also shows that
the covering radius of the first order Reed-Muller code R(1 9) is at least
equal to 242. Thirdly motivated by the fact that RSBFs are invariant under a
special permutation of the input vector we have classified all possible
permutations up to the linear equivalence of Boolean functions that are
invariant under those permutations. Specifically for 9-variable Boolean
functions 9! possible permutations are classified into 30 classes; and the
search algorithm identifies some of these classes as ""rich"". The rich classes
yield new Boolean functions with nonlinearity 242 having different
autocorrelation spectra from those of the functions found in the generalized
3-RSBF and 3-DSBF classes.
",1,0,0
"Relationship-Embedded Representation Learning for Grounding Referring
  Expressions","  Grounding referring expressions in images aims to locate the object instance
in an image described by a referring expression. It involves a joint
understanding of natural language and image content and is essential for a
range of visual tasks related to human-computer interaction. As a
language-to-vision matching task the core of this problem is to not only
extract all the necessary information (i.e. objects and the relationships
among them) in both the image and referring expression but also make full use
of context information to align cross-modal semantic concepts in the extracted
information. Unfortunately existing work on grounding referring expressions
fails to accurately extract multi-order relationships from the referring
expression and associate them with the objects and their related contexts in
the image. In this paper we propose a Cross-Modal Relationship Extractor
(CMRE) to adaptively highlight objects and relationships (spatial and semantic
relations) related to the given expression with a cross-modal attention
mechanism and represent the extracted information as a language-guided visual
relation graph. In addition we propose a Gated Graph Convolutional Network
(GGCN) to compute multimodal semantic contexts by fusing information from
different modes and propagating multimodal information in the structured
relation graph. Experimental results on three common benchmark datasets show
that our Cross-Modal Relationship Inference Network which consists of CMRE and
GGCN significantly surpasses all existing state-of-the-art methods. Code is
available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models
",0,1,0
Ergodic Layered Erasure One-Sided Interference Channels,"  The sum capacity of a class of layered erasure one-sided interference
channels is developed under the assumption of no channel state information at
the transmitters. Outer bounds are presented for this model and are shown to be
tight for the following sub-classes: i) weak ii) strong (mix of strong but not
very strong (SnVS) and very strong (VS)) iii) ergodic very strong (mix of
strong and weak) and (iv) a sub-class of mixed interference (mix of SnVS and
weak). Each sub-class is uniquely defined by the fading statistics.
",1,0,0
"A systematic review of Hate Speech automatic detection using Natural
  Language Processing","  With the multiplication of social media platforms which offer anonymity
easy access and online community formation and online debate the issue of
hate speech detection and tracking becomes a growing challenge to society
individual policy-makers and researchers. Despite efforts for leveraging
automatic techniques for automatic detection and monitoring their performances
are still far from satisfactory which constantly calls for future research on
the issue. This paper provides a systematic review of literature in this field
with a focus on natural language processing and deep learning technologies
highlighting the terminology processing pipeline core methods employed with
a focal point on deep learning architecture. From a methodological perspective
we adopt PRISMA guideline of systematic review of the last 10 years literature
from ACM Digital Library and Google Scholar. In the sequel existing surveys
limitations and future research directions are extensively discussed.
",0,1,0
"Causally Estimating the Sensitivity of Neural NLP Models to Spurious
  Features","  Recent work finds modern natural language processing (NLP) models relying on
spurious features for prediction. Mitigating such effects is thus important.
Despite this need there is no quantitative measure to evaluate or compare the
effects of different forms of spurious features in NLP. We address this gap in
the literature by quantifying model sensitivity to spurious features with a
causal estimand dubbed CENT which draws on the concept of average treatment
effect from the causality literature. By conducting simulations with four
prominent NLP models -- TextRNN BERT RoBERTa and XLNet -- we rank the models
against their sensitivity to artificial injections of eight spurious features.
We further hypothesize and validate that models that are more sensitive to a
spurious feature will be less robust against perturbations with this feature
during inference. Conversely data augmentation with this feature improves
robustness to similar perturbations. We find statistically significant inverse
correlations between sensitivity and robustness providing empirical support
for our hypothesis.
",0,1,0
Exploring Factors for Improving Low Resolution Face Recognition,"  State-of-the-art deep face recognition approaches report near perfect
performance on popular benchmarks e.g. Labeled Faces in the Wild. However
their performance deteriorates significantly when they are applied on low
quality images such as those acquired by surveillance cameras. A further
challenge for low resolution face recognition for surveillance applications is
the matching of recorded low resolution probe face images with high resolution
reference images which could be the case in watchlist scenarios. In this
paper we have addressed these problems and investigated the factors that would
contribute to the identification performance of the state-of-the-art deep face
recognition models when they are applied to low resolution face recognition
under mismatched conditions. We have observed that the following factors affect
performance in a positive way: appearance variety and resolution distribution
of the training dataset resolution matching between the gallery and probe
images and the amount of information included in the probe images. By
leveraging this information we have utilized deep face models trained on
MS-Celeb-1M and fine-tuned on VGGFace2 dataset and achieved state-of-the-art
accuracies on the SCFace and ICB-RW benchmarks even without using any training
data from the datasets of these benchmarks.
",0,0,1
"UAV-Sensing-Assisted Cellular Interference Coordination: A Cognitive
  Radio Approach","  Aerial-ground interference mitigation has been deemed as the main challenge
in realizing cellular-connected unmanned aerial vehicle (UAV) communications.
Due to the line-of-sight (LoS)-dominant air-ground channels the UAV
generates/suffers much stronger interference to/from cellular base stations
(BSs) over a much larger region in its uplink/downlink communication as
compared to the terrestrial users. As a result conventional inter-cell
interference coordination (ICIC) techniques catered for terrestrial networks
become ineffective in mitigating the more severe UAV-induced interference. To
deal with this new challenge this letter introduces a cognitive radio based
solution by treating the UAV and terrestrial users as secondary and primary
users in the network respectively. In particular the LoS channels with
terrestrial BSs/users endow the UAV with a powerful spectrum sensing capability
for detecting the terrestrial signals over a much larger region than its
serving BS. By exploiting this unique feature we propose a new
UAV-sensing-assisted ICIC design for both the UAV downlink and uplink
communications. Specifically the UAV senses its received interference and the
transmissions of terrestrial users in the downlink and uplink respectively
over the resource blocks (RBs) available at its serving BS to assist its RB
allocation to the UAV for avoiding the interference with co-channel terrestrial
communications. Numerical results demonstrate that the proposed UAV-assisted
ICIC outperforms the conventional terrestrial ICIC by engaging the neighboring
BSs for cooperation only.
",1,0,0
A simple neural network module for relational reasoning,"  Relational reasoning is a central component of generally intelligent
behavior but has proven difficult for neural networks to learn. In this paper
we describe how to use Relation Networks (RNs) as a simple plug-and-play module
to solve problems that fundamentally hinge on relational reasoning. We tested
RN-augmented networks on three tasks: visual question answering using a
challenging dataset called CLEVR on which we achieve state-of-the-art
super-human performance; text-based question answering using the bAbI suite of
tasks; and complex reasoning about dynamic physical systems. Then using a
curated dataset called Sort-of-CLEVR we show that powerful convolutional
networks do not have a general capacity to solve relational questions but can
gain this capacity when augmented with RNs. Our work shows how a deep learning
architecture equipped with an RN module can implicitly discover and learn to
reason about entities and their relations.
",0,1,0
A probabilistic top-down parser for minimalist grammars,"  This paper describes a probabilistic top-down parser for minimalist grammars.
Top-down parsers have the great advantage of having a certain predictive power
during the parsing which takes place in a left-to-right reading of the
sentence. Such parsers have already been well-implemented and studied in the
case of Context-Free Grammars which are already top-down but these are
difficult to adapt to Minimalist Grammars which generate sentences bottom-up.
I propose here a way of rewriting Minimalist Grammars as Linear Context-Free
Rewriting Systems allowing to easily create a top-down parser. This rewriting
allows also to put a probabilistic field on these grammars which can be used
to accelerate the parser. Finally I propose a method of refining the
probabilistic field by using algorithms used in data compression.
",0,1,0
"LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling
  in Urban Environments","  High Definition (HD) maps play an important role in modern traffic scenes.
However the development of HD maps coverage grows slowly because of the cost
limitation. To efficiently model HD maps we proposed a convolutional neural
network with a novel prediction layer and a zoom module called LineNet. It is
designed for state-of-the-art lane detection in an unordered crowdsourced image
dataset. And we introduced TTLane a dataset for efficient lane detection in
urban road modeling applications. Combining LineNet and TTLane we proposed a
pipeline to model HD maps with crowdsourced data for the first time. And the
maps can be constructed precisely even with inaccurate crowdsourced data.
",0,0,1
Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN),"  Despite the significant advances in recent years Generative Adversarial
Networks (GANs) are still notoriously hard to train. In this paper we propose
three novel curriculum learning strategies for training GANs. All strategies
are first based on ranking the training images by their difficulty scores
which are estimated by a state-of-the-art image difficulty predictor. Our first
strategy is to divide images into gradually more difficult batches. Our second
strategy introduces a novel curriculum loss function for the discriminator that
takes into account the difficulty scores of the real images. Our third strategy
is based on sampling from an evolving distribution which favors the easier
images during the initial training stages and gradually converges to a uniform
distribution in which samples are equally likely regardless of difficulty. We
compare our curriculum learning strategies with the classic training procedure
on two tasks: image generation and image translation. Our experiments indicate
that all strategies provide faster convergence and superior results. For
example our best curriculum learning strategy applied on spectrally normalized
GANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like
images are real in 25.0% of the presented cases while the SNGANs trained using
the classic procedure fooled the annotators in only 18.4% cases. Similarly in
image translation the human annotators preferred the images produced by the
Cycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5%
cases and those produced by CycleGAN based on classic training in only 19.8%
cases 39.7% cases being labeled as ties.
",0,0,1
"Classification of Neurological Gait Disorders Using Multi-task Feature
  Learning","  As our population ages neurological impairments and degeneration of the
musculoskeletal system yield gait abnormalities which can significantly reduce
quality of life. Gait rehabilitative therapy has been widely adopted to help
patients maximize community participation and living independence. To further
improve the precision and efficiency of rehabilitative therapy more objective
methods need to be developed based on sensory data. In this paper an
algorithmic framework is proposed to provide classification of gait disorders
caused by two common neurological diseases stroke and Parkinson's Disease
(PD) from ground contact force (GCF) data. An advanced machine learning
method multi-task feature learning (MTFL) is used to jointly train
classification models of a subject's gait in three classes post-stroke PD and
healthy gait. Gait parameters related to mobility balance strength and rhythm
are used as features for the classification. Out of all the features used the
MTFL models capture the more important ones per disease which will help
provide better objective assessment and therapy progress tracking. To evaluate
the proposed methodology we use data from a human participant study which
includes five PD patients three post-stroke patients and three healthy
subjects. Despite the diversity of abnormalities the evaluation shows that the
proposed approach can successfully distinguish post-stroke and PD gait from
healthy gait as well as post-stroke from PD gait with Area Under the Curve
(AUC) score of at least 0.96. Moreover the methodology helps select important
gait features to better understand the key characteristics that distinguish
abnormal gaits and design personalized treatment.
",0,0,1
Synchronizing Probability Measures on Rotations via Optimal Transport,"  We introduce a new paradigm $textitmeasure synchronization$ for
synchronizing graphs with measure-valued edges. We formulate this problem as
maximization of the cycle-consistency in the space of probability measures over
relative rotations. In particular we aim at estimating marginal distributions
of absolute orientations by synchronizing the $textitconditional$ ones
which are defined on the Riemannian manifold of quaternions. Such graph
optimization on distributions-on-manifolds enables a natural treatment of
multimodal hypotheses ambiguities and uncertainties arising in many computer
vision applications such as SLAM SfM and object pose estimation. We first
formally define the problem as a generalization of the classical rotation graph
synchronization where in our case the vertices denote probability measures
over rotations. We then measure the quality of the synchronization by using
Sinkhorn divergences which reduces to other popular metrics such as
Wasserstein distance or the maximum mean discrepancy as limit cases. We propose
a nonparametric Riemannian particle optimization approach to solve the problem.
Even though the problem is non-convex by drawing a connection to the recently
proposed sparse optimization methods we show that the proposed algorithm
converges to the global optimum in a special case of the problem under certain
conditions. Our qualitative and quantitative experiments show the validity of
our approach and we bring in new perspectives to the study of synchronization.
",0,0,1
"Experimenting with Convolutional Neural Network Architectures for the
  automatic characterization of Solitary Pulmonary Nodules' malignancy rating","  Lung Cancer is the most common cause of cancer-related death worldwide. Early
and automatic diagnosis of Solitary Pulmonary Nodules (SPN) in Computer
Tomography (CT) chest scans can provide early treatment as well as doctor
liberation from time-consuming procedures. Deep Learning has been proven as a
popular and influential method in many medical imaging diagnosis areas. In this
study we consider the problem of diagnostic classification between benign and
malignant lung nodules in CT images derived from a PET/CT scanner. More
specifically we intend to develop experimental Convolutional Neural Network
(CNN) architectures and conduct experiments by tuning their parameters to
investigate their behavior and to define the optimal setup for the accurate
classification. For the experiments we utilize PET/CT images obtained from the
Laboratory of Nuclear Medicine of the University of Patras and the publically
available database called Lung Image Database Consortium Image Collection
(LIDC-IDRI). Furthermore we apply simple data augmentation to generate new
instances and to inspect the performance of the developed networks.
Classification accuracy of 91% and 93% on the PET/CT dataset and on a selection
of nodule images form the LIDC-IDRI dataset is achieved accordingly. The
results demonstrate that CNNs are a trustworth method for nodule
classification. Also the experiment confirms that data augmentation enhances
the robustness of the CNNs.
",0,0,1
Robust Assessment of Real-World Adversarial Examples,"  We explore rigorous systematic and controlled experimental evaluation of
adversarial examples in the real world and propose a testing regimen for
evaluation of real world adversarial objects. We show that for small scene/
environmental perturbations large adversarial performance differences exist.
Current state of adversarial reporting exists largely as a frequency count over
a dynamic collections of scenes. Our work underscores the need for either a
more complete report or a score that incorporates scene changes and baseline
performance for models and environments tested by adversarial developers. We
put forth a score that attempts to address the above issues in a
straight-forward exemplar application for multiple generated adversary
examples. We contribute the following: 1. a testbed for adversarial assessment
2. a score for adversarial examples and 3. a collection of additional
evaluations on testbed data.
",0,0,1
"Deep Learning based NAS Score and Fibrosis Stage Prediction from CT and
  Pathology Data","  Non-Alcoholic Fatty Liver Disease (NAFLD) is becoming increasingly prevalent
in the world population. Without diagnosis at the right time NAFLD can lead to
non-alcoholic steatohepatitis (NASH) and subsequent liver damage. The diagnosis
and treatment of NAFLD depend on the NAFLD activity score (NAS) and the liver
fibrosis stage which are usually evaluated from liver biopsies by
pathologists. In this work we propose a novel method to automatically predict
NAS score and fibrosis stage from CT data that is non-invasive and inexpensive
to obtain compared with liver biopsy. We also present a method to combine the
information from CT and H&E stained pathology data to improve the performance
of NAS score and fibrosis stage prediction when both types of data are
available. This is of great value to assist the pathologists in computer-aided
diagnosis process. Experiments on a 30-patient dataset illustrate the
effectiveness of our method.
",0,0,1
"TomoGAN: Low-Dose Synchrotron X-Ray Tomography with Generative
  Adversarial Networks","  Synchrotron-based x-ray tomography is a noninvasive imaging technique that
allows for reconstructing the internal structure of materials at high spatial
resolutions from tens of micrometers to a few nanometers. In order to resolve
sample features at smaller length scales however a higher radiation dose is
required. Therefore the limitation on the achievable resolution is set
primarily by noise at these length scales. We present TOMOGAN a denoising
technique based on generative adversarial networks for improving the quality
of reconstructed images for low-dose imaging conditions. We evaluate our
approach in two photon-budget-limited experimental conditions: (1) sufficient
number of low-dose projections (based on Nyquist sampling) and (2)
insufficient or limited number of high-dose projections. In both cases the
angular sampling is assumed to be isotropic and the photon budget throughout
the experiment is fixed based on the maximum allowable radiation dose on the
sample. Evaluation with both simulated and experimental datasets shows that our
approach can significantly reduce noise in reconstructed images improving the
structural similarity score of simulation and experimental data from 0.18 to
0.9 and from 0.18 to 0.41 respectively. Furthermore the quality of the
reconstructed images with filtered back projection followed by our denoising
approach exceeds that of reconstructions with the simultaneous iterative
reconstruction technique showing the computational superiority of our
approach.
",0,0,1
Sequence to Sequence Learning for Optical Character Recognition,"  We propose an end-to-end recurrent encoder-decoder based sequence learning
approach for printed text Optical Character Recognition (OCR). In contrast to
present day existing state-of-art OCR solution which uses connectionist
temporal classification (CTC) output layer our approach makes minimalistic
assumptions on the structure and length of the sequence. We use a two step
encoder-decoder approach -- (a) A recurrent encoder reads a variable length
printed text word image and encodes it to a fixed dimensional embedding. (b)
This fixed dimensional embedding is subsequently comprehended by decoder
structure which converts it into a variable length text output. Our
architecture gives competitive performance relative to connectionist temporal
classification (CTC) output layer while being executed in more natural
settings. The learnt deep word image embedding from encoder can be used for
printed text based retrieval systems. The expressive fixed dimensional
embedding for any variable length input expedites the task of retrieval and
makes it more efficient which is not possible with other recurrent neural
network architectures. We empirically investigate the expressiveness and the
learnability of long short term memory (LSTMs) in the sequence to sequence
learning regime by training our network for prediction tasks in segmentation
free printed text OCR. The utility of the proposed architecture for printed
text is demonstrated by quantitative and qualitative evaluation of two tasks --
word prediction and retrieval.
",0,0,1
Layered Coding for Energy Harvesting Communication Without CSIT,"  Due to stringent constraints on resources it may be infeasible to acquire
the current channel state information at the transmitter in energy harvesting
communication systems. In this paper we optimize an energy harvesting
transmitter communicating over a slow fading channel using layered coding.
The transmitter has access to the channel statistics but does not know the
exact channel state. In layered coding the codewords are first designed for
each of the channel states at different rates and then the codewords are
either time-multiplexed or superimposed before the transmission leading to two
transmission strategies. The receiver then decodes the information adaptively
based on the realized channel state. The transmitter is equipped with a
finite-capacity battery having non-zero internal resistance. In each of the
transmission strategies we first formulate and study an average rate
maximization problem with non-causal knowledge of the harvested power
variations. Further assuming statistical knowledge and causal information of
the harvested power variations we propose a sub-optimal algorithm and compare
with the stochastic dynamic programming based solution and a greedy policy.
",1,0,0
Locally Repairable Codes,"  Distributed storage systems for large-scale applications typically use
replication for reliability. Recently erasure codes were used to reduce the
large storage overhead while increasing data reliability. A main limitation of
off-the-shelf erasure codes is their high-repair cost during single node
failure events. A major open problem in this area has been the design of codes
that it i) are repair efficient and it ii) achieve arbitrarily high data
rates.
  In this paper we explore the repair metric of it locality which
corresponds to the number of disk accesses required during a
colorblacksingle node repair. Under this metric we characterize an
information theoretic trade-off that binds together locality code distance
and the storage capacity of each node. We show the existence of optimal it
locally repairable codes (LRCs) that achieve this trade-off. The achievability
proof uses a locality aware flow-graph gadget which leads to a randomized code
construction. Finally we present an optimal and explicit LRC that achieves
arbitrarily high data-rates. Our locality optimal construction is based on
simple combinations of Reed-Solomon blocks.
",1,0,0
"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in
  Video Classification","  Despite the steady progress in video analysis led by the adoption of
convolutional neural networks (CNNs) the relative improvement has been less
drastic as that in 2D static image classification. Three main challenges exist
including spatial (image) feature representation temporal information
representation and model/computation complexity. It was recently shown by
Carreira and Zisserman that 3D CNNs inflated from 2D networks and pretrained
on ImageNet could be a promising way for spatial and temporal representation
learning. However as for model/computation complexity 3D CNNs are much more
expensive than 2D CNNs and prone to overfit. We seek a balance between speed
and accuracy by building an effective and efficient video classification system
through systematic exploration of critical network design choices. In
particular we show that it is possible to replace many of the 3D convolutions
by low-cost 2D convolutions. Rather surprisingly best result (in both speed
and accuracy) is achieved when replacing the 3D convolutions at the bottom of
the network suggesting that temporal representation learning on high-level
semantic features is more useful. Our conclusion generalizes to datasets with
very different properties. When combined with several other cost-effective
designs including separable spatial/temporal convolution and feature gating
our system results in an effective video classification system that that
produces very competitive results on several action classification benchmarks
(Kinetics Something-something UCF101 and HMDB) as well as two action
detection (localization) benchmarks (JHMDB and UCF101-24).
",0,0,1
Physical Layer Security over Fluctuating Two-Ray Fading Channels,"  Ensuring the physical layer security (PHY-security) of millimeter wave
(mmWave) communications is one of the key factors for the success of 5G. Recent
field measurements show that conventional fading models cannot accurately model
the random fluctuations of mmWave signals. To tackle this challenge the
fluctuating two-ray (FTR) fading model has been proposed. In this
correspondence we comprehensively analyze the PHY-security in mmWave
communications over FTR fading channels. More specifically we derive
analytical expressions for significant PHY-security metrics such as average
secrecy capacity secrecy outage probability and the probability of strictly
positive secrecy capacity with simple functions. The effect of channel
parameters on the PHY-security has been validated by numerical results.
",1,0,0
"Unsupervised and interpretable scene discovery with
  Discrete-Attend-Infer-Repeat","  In this work we present Discrete Attend Infer Repeat (Discrete-AIR) a
Recurrent Auto-Encoder with structured latent distributions containing discrete
categorical distributions continuous attribute distributions and factorised
spatial attention. While inspired by the original AIR model andretaining AIR
model's capability in identifying objects in an image Discrete-AIR provides
direct interpretability of the latent codes. We show that for Multi-MNIST and a
multiple-objects version of dSprites dataset the Discrete-AIR model needs just
one categorical latent variable one attribute variable (for Multi-MNIST only)
together with spatial attention variables for efficient inference. We perform
analysis to show that the learnt categorical distributions effectively capture
the categories of objects in the scene for Multi-MNIST and for Multi-Sprites.
",0,0,1
"Semi-Supervised Neural Text Generation by Joint Learning of Natural
  Language Generation and Natural Language Understanding Models","  In Natural Language Generation (NLG) End-to-End (E2E) systems trained
through deep learning have recently gained a strong interest. Such deep models
need a large amount of carefully annotated data to reach satisfactory
performance. However acquiring such datasets for every new NLG application is
a tedious and time-consuming task. In this paper we propose a semi-supervised
deep learning scheme that can learn from non-annotated data and annotated data
when available. It uses an NLG and a Natural Language Understanding (NLU)
sequence-to-sequence models which are learned jointly to compensate for the
lack of annotation. Experiments on two benchmark datasets show that with
limited amount of annotated data the method can achieve very competitive
results while not using any pre-processing or re-scoring tricks. These findings
open the way to the exploitation of non-annotated datasets which is the current
bottleneck for the E2E NLG system development to new applications.
",0,1,0
Revisiting Binary Local Image Description for Resource Limited Devices,"  The advent of a panoply of resource limited devices opens up new challenges
in the design of computer vision algorithms with a clear compromise between
accuracy and computational requirements. In this paper we present new binary
image descriptors that emerge from the application of triplet ranking loss
hard negative mining and anchor swapping to traditional features based on pixel
differences and image gradients. These descriptors BAD (Box Average
Difference) and HashSIFT establish new operating points in the
state-of-the-art's accuracy vs. resources trade-off curve. In our experiments
we evaluate the accuracy execution time and energy consumption of the proposed
descriptors. We show that BAD bears the fastest descriptor implementation in
the literature while HashSIFT approaches in accuracy that of the top deep
learning-based descriptors being computationally more efficient. We have made
the source code public.
",0,0,1
Sequential vessel segmentation via deep channel attention network,"  This paper develops a novel encoder-decoder deep network architecture which
exploits the several contextual frames of 2D+t sequential images in a sliding
window centered at current frame to segment 2D vessel masks from the current
frame. The architecture is equipped with temporal-spatial feature extraction in
encoder stage feature fusion in skip connection layers and channel attention
mechanism in decoder stage. In the encoder stage a series of 3D convolutional
layers are employed to hierarchically extract temporal-spatial features. Skip
connection layers subsequently fuse the temporal-spatial feature maps and
deliver them to the corresponding decoder stages. To efficiently discriminate
vessel features from the complex and noisy backgrounds in the XCA images the
decoder stage effectively utilizes channel attention blocks to refine the
intermediate feature maps from skip connection layers for subsequently decoding
the refined features in 2D ways to produce the segmented vessel masks.
Furthermore Dice loss function is implemented to train the proposed deep
network in order to tackle the class imbalance problem in the XCA data due to
the wide distribution of complex background artifacts. Extensive experiments by
comparing our method with other state-of-the-art algorithms demonstrate the
proposed method's superior performance over other methods in terms of the
quantitative metrics and visual validation. The source codes are at
https://github.com/Binjie-Qin/SVS-net
",0,0,1
Pairwise Quantization,"  We consider the task of lossy compression of high-dimensional vectors through
quantization. We propose the approach that learns quantization parameters by
minimizing the distortion of scalar products and squared distances between
pairs of points. This is in contrast to previous works that obtain these
parameters through the minimization of the reconstruction error of individual
points. The proposed approach proceeds by finding a linear transformation of
the data that effectively reduces the minimization of the pairwise distortions
to the minimization of individual reconstruction errors. After such
transformation any of the previously-proposed quantization approaches can be
used. Despite the simplicity of this transformation the experiments
demonstrate that it achieves considerable reduction of the pairwise distortions
compared to applying quantization directly to the untransformed data.
",0,0,1
"A Simple Post-Processing Technique for Improving Readability Assessment
  of Texts using Word Mover's Distance","  Assessing the proper difficulty levels of reading materials or texts in
general is the first step towards effective comprehension and learning. In this
study we improve the conventional methodology of automatic readability
assessment by incorporating the Word Mover's Distance (WMD) of ranked texts as
an additional post-processing technique to further ground the difficulty level
given by a model. Results of our experiments on three multilingual datasets in
Filipino German and English show that the post-processing technique
outperforms previous vanilla and ranking-based models using SVM.
",0,1,0
Discrete MDL Predicts in Total Variation,"  The Minimum Description Length (MDL) principle selects the model that has the
shortest code for data plus model. We show that for a countable class of
models MDL predictions are close to the true distribution in a strong sense.
The result is completely general. No independence ergodicity stationarity
identifiability or other assumption on the model class need to be made. More
formally we show that for any countable class of models the distributions
selected by MDL (or MAP) asymptotically predict (merge with) the true measure
in the class in total variation distance. Implications for non-i.i.d. domains
like time-series forecasting discriminative learning and reinforcement
learning are discussed.
",1,0,0
"Stochastic Optimization and Control Framework for 5G Network Slicing
  with Effective Isolation","  Network slicing is an emerging technique for providing resources to diverse
wireless services with heterogeneous quality-of-service needs. However beyond
satisfying end-to-end requirements of network users network slicing needs to
also provide isolation between slices so as to prevent one slice's faults and
congestion from affecting other slices. In this paper the problem of network
slicing is studied in the context of a wireless system having a time-varying
number of users that require two types of slices: reliable low latency (RLL)
and self-managed (capacity limited) slices. To address this problem a novel
control framework for stochastic optimization is proposed based on the Lyapunov
drift-plus-penalty method. This new framework enables the system to minimize
power maintain slice isolation and provide reliable and low latency
end-to-end communication for RLL slices. Simulation results show that the
proposed approach can maintain the system's reliability while providing
effective slice isolation in the event of sudden changes in the network
environment.
",1,0,0
BPE-Dropout: Simple and Effective Subword Regularization,"  Subword segmentation is widely used to address the open vocabulary problem in
machine translation. The dominant approach to subword segmentation is Byte Pair
Encoding (BPE) which keeps the most frequent words intact while splitting the
rare ones into multiple tokens. While multiple segmentations are possible even
with the same vocabulary BPE splits words into unique sequences; this may
prevent a model from better learning the compositionality of words and being
robust to segmentation errors. So far the only way to overcome this BPE
imperfection its deterministic nature was to create another subword
segmentation algorithm (Kudo 2018). In contrast we show that BPE itself
incorporates the ability to produce multiple segmentations of the same word. We
introduce BPE-dropout - simple and effective subword regularization method
based on and compatible with conventional BPE. It stochastically corrupts the
segmentation procedure of BPE which leads to producing multiple segmentations
within the same fixed BPE framework. Using BPE-dropout during training and the
standard BPE during inference improves translation quality up to 3 BLEU
compared to BPE and up to 0.9 BLEU compared to the previous subword
regularization.
",0,1,0
Neyman-Pearson Test for Zero-Rate Multiterminal Hypothesis Testing,"  The problem of zero-rate multiterminal hypothesis testing is revisited from
the perspective of information-spectrum approach and finite blocklength
analysis. A Neyman-Pearson-like test is proposed and its non-asymptotic
performance is clarified; for a short block length it is numerically
determined that the proposed test is superior to the previously reported
Hoeffding-like test proposed by Han-Kobayashi. For a large deviation regime it
is shown that our proposed test achieves an optimal trade-off between the type
I and type II exponents presented by Han-Kobayashi. Among the class of
symmetric (type based) testing schemes when the type I error probability is
non-vanishing the proposed test is optimal up to the second-order term of the
type II error exponent; the latter term is characterized in terms of the
variance of the projected relative entropy density. The information geometry
method plays an important role in the analysis as well as the construction of
the test.
",1,0,0
Text Generation with Efficient (Soft) Q-Learning,"  Maximum likelihood estimation (MLE) is the predominant algorithm for training
text generation models. This paradigm relies on direct supervision examples
which is not applicable to many emerging applications such as generating
adversarial attacks or generating prompts to control language models.
Reinforcement learning (RL) on the other hand offers a more flexible solution
by allowing users to plug in arbitrary task metrics as reward. Yet previous RL
algorithms for text generation such as policy gradient (on-policy RL) and
Q-learning (off-policy RL) are often notoriously inefficient or unstable to
train due to the large sequence space and the sparse reward received only at
the end of sequences. In this paper we introduce a new RL formulation for text
generation from the soft Q-learning (SQL) perspective. It enables us to draw
from the latest RL advances such as path consistency learning to combine the
best of on-/off-policy updates and learn effectively from sparse reward. We
apply the approach to a wide range of text generation tasks including learning
from noisy/negative examples adversarial attacks and prompt generation.
Experiments show our approach consistently outperforms both task-specialized
algorithms and the previous RL methods.
",0,1,0
"GPU Accelerated Cascade Hashing Image Matching for Large Scale 3D
  Reconstruction","  Image feature point matching is a key step in Structure from Motion(SFM).
However it is becoming more and more time consuming because the number of
images is getting larger and larger. In this paper we proposed a GPU
accelerated image matching method with improved Cascade Hashing. Firstly we
propose a Disk-Memory-GPU data exchange strategy and optimize the load order of
data so that the proposed method can deal with big data. Next we parallelize
the Cascade Hashing method on GPU. An improved parallel reduction and an
improved parallel hashing ranking are proposed to fulfill this task. Finally
extensive experiments show that our image matching is about 20 times faster
than SiftGPU on the same graphics card nearly 100 times faster than the CPU
CasHash method and hundreds of times faster than the CPU Kd-Tree based matching
method. Further more we introduce the epipolar constraint to the proposed
method and use the epipolar geometry to guide the feature matching procedure
which further reduces the matching cost.
",0,0,1
"Unconstrained Biometric Recognition: Summary of Recent SOCIA Lab
  Research","  The development of biometric recognition solutions able to work in visual
surveillance conditions i.e. in unconstrained data acquisition conditions and
under covert protocols has been motivating growing efforts from the research
community. Among the various laboratories schools and research institutes
concerned about this problem the SOCIA: Soft Computing and Image Analysis
Lab. of the University of Beira Interior Portugal has been among the most
active in pursuing disruptive solutions for obtaining such extremely ambitious
kind of automata. This report summarises the research works published by
elements of the SOCIA Lab. in the last decade in the scope of biometric
recognition in unconstrained conditions. The idea is that it can be used as
basis for someone wishing to entering in this research topic.
",0,0,1
End-to-End Segmentation-based News Summarization,"  In this paper we bring a new way of digesting news content by introducing
the task of segmenting a news article into multiple sections and generating the
corresponding summary to each section. We make two contributions towards this
new task. First we create and make available a dataset SegNews consisting of
27k news articles with sections and aligned heading-style section summaries.
Second we propose a novel segmentation-based language generation model adapted
from pre-trained language models that can jointly segment a document and
produce the summary for each section. Experimental results on SegNews
demonstrate that our model can outperform several state-of-the-art
sequence-to-sequence generation models for this new task.
",0,1,0
"Exact partial information decompositions for Gaussian systems based on
  dependency constraints","  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here we apply
the Idep approach to Gaussian systems for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
",1,0,0
"Efficient Learning of Model Weights via Changing Features During
  Training","  In this paper we propose a machine learning model which dynamically changes
the features during training. Our main motivation is to update the model in a
small content during the training process with replacing less descriptive
features to new ones from a large pool. The main benefit is coming from the
fact that opposite to the common practice we do not start training a new model
from the scratch but can keep the already learned weights. This procedure
allows the scan of a large feature pool which together with keeping the
complexity of the model leads to an increase of the model accuracy within the
same training time. The efficiency of our approach is demonstrated in several
classic machine learning scenarios including linear regression and neural
network-based training. As a specific analysis towards signal processing we
have successfully tested our approach on the database MNIST for digit
classification considering single pixel and pixel-pairs intensities as possible
features.
",0,0,1
3D Pose Estimation for Fine-Grained Object Categories,"  Existing object pose estimation datasets are related to generic object types
and there is so far no dataset for fine-grained object categories. In this
work we introduce a new large dataset to benchmark pose estimation for
fine-grained objects thanks to the availability of both 2D and 3D fine-grained
data recently. Specifically we augment two popular fine-grained recognition
datasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for
each sub-category and manually annotating each object in images with 3D pose.
We show that with enough training data a full perspective model with
continuous parameters can be estimated using 2D appearance information alone.
We achieve this via a framework based on Faster/Mask R-CNN. This goes beyond
previous works on category-level pose estimation which only estimate
discrete/continuous viewpoint angles or recover rotation matrices often with
the help of key points. Furthermore with fine-grained 3D models available we
incorporate a dense 3D representation named as location field into the
CNN-based pose estimation framework to further improve the performance. The new
dataset is available at www.umiacs.umd.edu/~wym/3dpose.html
",0,0,1
On the Cohomology of 3D Digital Images,"  We propose a method for computing the cohomology ring of three--dimensional
(3D) digital binary-valued pictures. We obtain the cohomology ring of a 3D
digital binary--valued picture $I$ via a simplicial complex K(I)topologically
representing (up to isomorphisms of pictures) the picture I. The usefulness of
a simplicial description of the ""digital"" cohomology ring of 3D digital
binary-valued pictures is tested by means of a small program visualizing the
different steps of the method. Some examples concerning topological thinning
the visualization of representative (co)cycles of (co)homology generators and
the computation of the cup product on the cohomology of simple pictures are
showed.
",0,0,1
"Quantifying point cloud realism through adversarially learned latent
  representations","  Judging the quality of samples synthesized by generative models can be
tedious and time consuming especially for complex data structures such as
point clouds. This paper presents a novel approach to quantify the realism of
local regions in LiDAR point clouds. Relevant features are learned from
real-world and synthetic point clouds by training on a proxy classification
task. Inspired by fair networks we use an adversarial technique to discourage
the encoding of dataset-specific information. The resulting metric can assign a
quality score to samples without requiring any task specific annotations.
  In a series of experiments we confirm the soundness of our metric by
applying it in controllable task setups and on unseen data. Additional
experiments show reliable interpolation capabilities of the metric between data
with varying degree of realism. As one important application we demonstrate
how the local realism score can be used for anomaly detection in point clouds.
",0,0,1
"A Large-Scale Multi-Length Headline Corpus for Analyzing
  Length-Constrained Headline Generation Model Evaluation","  Browsing news articles on multiple devices is now possible. The lengths of
news article headlines have precise upper bounds dictated by the size of the
display of the relevant device or interface. Therefore controlling the length
of headlines is essential when applying the task of headline generation to news
production. However because there is no corpus of headlines of multiple
lengths for a given article previous research on controlling output length in
headline generation has not discussed whether the system outputs could be
adequately evaluated without multiple references of different lengths. In this
paper we introduce two corpora which are Japanese News Corpus (JNC) and
JApanese MUlti-Length Headline Corpus (JAMUL) to confirm the validity of
previous evaluation settings. The JNC provides common supervision data for
headline generation. The JAMUL is a large-scale evaluation dataset for
headlines of three different lengths composed by professional editors. We
report new findings on these corpora; for example although the longest length
reference summary can appropriately evaluate the existing methods controlling
output length this evaluation setting has several problems.
",0,1,0
CRTS: A type system for representing clinical recommendations,"  Background: Clinical guidelines and recommendations are the driving wheels of
the evidence-based medicine (EBM) paradigm but these are available primarily
as unstructured text and are generally highly heterogeneous in nature. This
significantly reduces the dissemination and automatic application of these
recommendations at the point of care. A comprehensive structured representation
of these recommendations is highly beneficial in this regard. Objective: The
objective of this paper to present Clinical Recommendation Type System (CRTS)
a common type system that can effectively represent a clinical recommendation
in a structured form. Methods: CRTS is built by analyzing 125 recommendations
and 195 research articles corresponding to 6 different diseases available from
UpToDate a publicly available clinical knowledge system and from the National
Guideline Clearinghouse a public resource for evidence-based clinical practice
guidelines. Results: We show that CRTS not only covers the recommendations but
also is flexible to be extended to represent information from primary
literature. We also describe how our developed type system can be applied for
clinical decision support medical knowledge summarization and citation
retrieval. Conclusion: We showed that our proposed type system is precise and
comprehensive in representing a large sample of recommendations available for
various disorders. CRTS can now be used to build interoperable information
extraction systems that automatically extract clinical recommendations and
related data elements from clinical evidence resources guidelines systematic
reviews and primary publications.
  Keywords: guidelines and recommendations type system clinical decision
support evidence-based medicine information storage and retrieval
",0,1,0
Downlink Interference Alignment,"  We develop an interference alignment (IA) technique for a downlink cellular
system. In the uplink IA schemes need channel-state-information exchange
across base-stations of different cells but our downlink IA technique requires
feedback only within a cell. As a result the proposed scheme can be
implemented with a few changes to an existing cellular system where the
feedback mechanism (within a cell) is already being considered for supporting
multi-user MIMO. Not only is our proposed scheme implementable with little
effort it can in fact provide substantial gain especially when interference
from a dominant interferer is significantly stronger than the remaining
interference: it is shown that in the two-isolated cell layout our scheme
provides four-fold gain in throughput performance over a standard multi-user
MIMO technique. We show through simulations that our technique provides
respectable gain under a more realistic scenario: it gives approximately 20%
gain for a 19 hexagonal wrap-around-cell layout. Furthermore we show that our
scheme has the potential to provide substantial gain for macro-pico cellular
networks where pico-users can be significantly interfered with by the nearby
macro-BS.
",1,0,0
"Transmit Power Minimization for MIMO Systems of Exponential Average BER
  with Fixed Outage Probability","  This paper is concerned with a wireless system operating in MIMO fading
channels with channel state information being known at both transmitter and
receiver. By spatiotemporal subchannel selection and power control it aims to
minimize the average transmit power (ATP) of the MIMO system while achieving an
exponential type of average bit error rate (BER) for each data stream. Under
the constraints of a given fixed individual outage probability (OP) and average
BER for each subchannel based on a traditional upper bound and a dynamic upper
bound of Q function two closed-form ATP expressions are derived respectively
and they correspond to two different power allocation schemes. Numerical
results are provided to validate the theoretical analysis and show that the
power allocation scheme with the dynamic upper bound can achieve more power
savings than the one with the traditional upper bound.
",1,0,0
"Shannon Entropy based Randomness Measurement and Test for Image
  Encryption","  The quality of image encryption is commonly measured by the Shannon entropy
over the ciphertext image. However this measurement does not consider to the
randomness of local image blocks and is inappropriate for scrambling based
image encryption methods. In this paper a new information entropy-based
randomness measurement for image encryption is introduced which for the first
time answers the question of whether a given ciphertext image is sufficiently
random-like. It measures the randomness over the ciphertext in a fairer way by
calculating the averaged entropy of a series of small image blocks within the
entire test image. In order to fulfill both quantitative and qualitative
measurement the expectation and the variance of this averaged block entropy
for a true-random image are strictly derived and corresponding numerical
reference tables are also provided. Moreover a hypothesis test at
significance-level is given to help accept or reject the hypothesis that the
test image is ideally encrypted/random-like. Simulation results show that the
proposed test is able to give both effectively quantitative and qualitative
results for image encryption. The same idea can also be applied to measure
other digital data like audio and video.
",1,0,0
NINEPINS: Nuclei Instance Segmentation with Point Annotations,"  Deep learning-based methods are gaining traction in digital pathology with
an increasing number of publications and challenges that aim at easing the work
of systematically and exhaustively analyzing tissue slides. These methods often
achieve very high accuracies at the cost of requiring large annotated datasets
to train. This requirement is especially difficult to fulfill in the medical
field where expert knowledge is essential. In this paper we focus on nuclei
segmentation which generally requires experienced pathologists to annotate the
nuclear areas in gigapixel histological images. We propose an algorithm for
instance segmentation that uses pseudo-label segmentations generated
automatically from point annotations as a method to reduce the burden for
pathologists. With the generated segmentation masks the proposed method trains
a modified version of HoVer-Net model to achieve instance segmentation.
Experimental results show that the proposed method is robust to inaccuracies in
point annotations and comparison with Hover-Net trained with fully annotated
instance masks shows that a degradation in segmentation performance does not
always imply a degradation in higher order tasks such as tissue classification.
",0,0,1
Deep learning for image segmentation: veritable or overhyped?,"  Deep learning has achieved great success as a powerful classification tool
and also made great progress in sematic segmentation. As a result many
researchers also believe that deep learning is the most powerful tool for pixel
level image segmentation. Could deep learning achieve the same pixel level
accuracy as traditional image segmentation techniques by mapping the features
of the object into a non-linear function? This paper gives a short survey of
the accuracies achieved by deep learning so far in image classification and
image segmentation. Compared to the high accuracies achieved by deep learning
in classifying limited categories in international vision challenges the image
segmentation accuracies achieved by deep learning in the same challenges are
only about eighty percent. On the contrary the image segmentation accuracies
achieved in international biomedical challenges are close to ninty five
percent. Why the difference is so big? Since the accuracies of the competitors
methods are only evaluated based on their submitted results instead of
reproducing the results by submitting the source codes or the software are the
achieved accuracies verifiable or overhyped? We are going to find it out by
analyzing the working principle of deep learning. Finally we compared the
accuracies of state of the art deep learning methods with a threshold selection
method quantitatively. Experimental results showed that the threshold selection
method could achieve significantly higher accuracy than deep learning methods
in image segmentation.
",0,0,1
On Strategic Multi-Antenna Jamming in Centralized Detection Networks,"  In this paper we model a complete-information zero-sum game between a
centralized detection network with a multiple access channel (MAC) between the
sensors and the fusion center (FC) and a jammer with multiple transmitting
antennas. We choose error probability at the FC as the performance metric and
investigate pure strategy equilibria for this game and show that the jammer
has no impact on the FC's error probability by employing pure strategies at the
Nash equilibrium. Furthermore we also show that the jammer has an impact on
the expected utility if it employs mixed strategies.
",1,0,0
"CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for
  Embedded Vision Systems and Applications","  The field of view (FOV) of convolutional neural networks is highly related to
the accuracy of inference. Dilated convolutions are known as an effective
solution to the problems which require large FOVs. However for general-purpose
hardware or dedicated hardware it usually takes extra time to handle dilated
convolutions compared with standard convolutions. In this paper we propose a
network module Cascaded and Separable Structure of Dilated (CASSOD)
Convolution and a special hardware system to handle the CASSOD networks
efficiently. A CASSOD-Net includes multiple cascaded $2 times 2$ dilated
filters which can be used to replace the traditional $3 times 3$ dilated
filters without decreasing the accuracy of inference. Two example applications
face detection and image segmentation are tested with dilated convolutions and
the proposed CASSOD modules. The new network for face detection achieves higher
accuracy than the previous work with only 47% of filter weights in the dilated
convolution layers of the context module. Moreover the proposed hardware
system can accelerate the computations of dilated convolutions and it is 2.78
times faster than traditional hardware systems when the filter size is $3
times 3$.
",0,0,1
"Deep Learning Approaches to Classification of Production Technology for
  19th Century Books","  Cultural research is dedicated to understanding the processes of knowledge
dissemination and the social and technological practices in the book industry.
Research on children books in the 19th century can be supported by computer
systems. Specifically the advances in digital image processing seem to offer
great opportunities for analyzing and quantifying the visual components in the
books. The production technology for illustrations in books in the 19th century
was characterized by a shift from wood or copper engraving to lithography. We
report classification experiments which intend to classify images based on the
production technology. For a classification task that is also difficult for
humans the classification quality reaches only around 70%. We analyze some
further error sources and identify reasons for the low performance.
",0,0,1
"HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative
  Models","  Generative models often use human evaluations to measure the perceived
quality of their outputs. Automated metrics are noisy indirect proxies because
they rely on heuristics or pretrained embeddings. However up until now direct
human evaluation strategies have been ad-hoc neither standardized nor
validated. Our work establishes a gold standard human benchmark for generative
realism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark
that is (1) grounded in psychophysics research in perception (2) reliable
across different sets of randomly sampled outputs from a model (3) able to
produce separable model performances and (4) efficient in cost and time. We
introduce two variants: one that measures visual perception under adaptive time
constraints to determine the threshold at which a model's outputs appear real
(e.g. 250ms) and the other a less expensive variant that measures human error
rate on fake and real images sans time constraints. We test HYPE across six
state-of-the-art generative adversarial networks and two sampling techniques on
conditional and unconditional image generation using four datasets: CelebA
FFHQ CIFAR-10 and ImageNet. We find that HYPE can track model improvements
across training epochs and we confirm via bootstrap sampling that HYPE
rankings are consistent and replicable.
",0,0,1
Measuring Quantum Entropy,"  The entropy of a quantum system is a measure of its randomness and has
applications in measuring quantum entanglement. We study the problem of
measuring the von Neumann entropy $S(rho)$ and R'enyi entropy
$S_alpha(rho)$ of an unknown mixed quantum state $rho$ in $d$ dimensions
given access to independent copies of $rho$.
  We provide an algorithm with copy complexity $O(d^2/alpha)$ for estimating
$S_alpha(rho)$ for $alpha<1$ and copy complexity $O(d^2)$ for estimating
$S(rho)$ and $S_alpha(rho)$ for non-integral $alpha>1$. These bounds are
at least quadratic in $d$ which is the order dependence on the number of
copies required for learning the entire state $rho$. For integral $alpha>1$
on the other hand we provide an algorithm for estimating $S_alpha(rho)$ with
a sub-quadratic copy complexity of $O(d^2-2/alpha)$. We characterize the
copy complexity for integral $alpha>1$ up to constant factors by providing
matching lower bounds. For other values of $alpha$ and the von Neumann
entropy we show lower bounds on the algorithm that achieves the upper bound.
This shows that we either need new algorithms for better upper bounds or
better lower bounds to tighten the results.
  For non-integral $alpha$ and the von Neumann entropy we consider the well
known Empirical Young Diagram (EYD) algorithm which is the analogue of
empirical plug-in estimator in classical distribution estimation. As a
corollary we strengthen a lower bound on the copy complexity of the EYD
algorithm for learning the maximally mixed state by showing that the lower
bound holds with exponential probability (which was previously known to hold
with a constant probability). For integral $alpha>1$ we provide new
concentration results of certain polynomials that arise in Kerov algebra of
Young diagrams.
",1,0,0
Relation Module for Non-answerable Prediction on Question Answering,"  Machine reading comprehension(MRC) has attracted significant amounts of
research attention recently due to an increase of challenging reading
comprehension datasets. In this paper we aim to improve a MRC model's ability
to determine whether a question has an answer in a given context (e.g. the
recently proposed SQuAD 2.0 task). Our solution is a relation module that is
adaptable to any MRC model. The relation module consists of both semantic
extraction and relational information. We first extract high level semantics as
objects from both question and context with multi-head self-attentive pooling.
These semantic objects are then passed to a relation network which generates
relationship scores for each object pair in a sentence. These scores are used
to determine whether a question is non-answerable. We test the relation module
on the SQuAD 2.0 dataset using both BiDAF and BERT models as baseline readers.
We obtain 1.8% gain of F1 on top of the BiDAF reader and 1.0% on top of the
BERT base model. These results show the effectiveness of our relation module on
MRC
",0,1,0
"Imposing Label-Relational Inductive Bias for Extremely Fine-Grained
  Entity Typing","  Existing entity typing systems usually exploit the type hierarchy provided by
knowledge base (KB) schema to model label correlations and thus improve the
overall performance. Such techniques however are not directly applicable to
more open and practical scenarios where the type set is not restricted by KB
schema and includes a vast number of free-form types. To model the underly-ing
label correlations without access to manually annotated label structures we
introduce a novel label-relational inductive bias represented by a graph
propagation layer that effectively encodes both global label co-occurrence
statistics and word-level similarities.On a large dataset with over 10000
free-form types the graph-enhanced model equipped with an attention-based
matching module is able to achieve a much higher recall score while maintaining
a high-level precision. Specifically it achieves a 15.3% relative F1
improvement and also less inconsistency in the outputs. We further show that a
simple modification of our proposed graph layer can also improve the
performance on a conventional and widely-tested dataset that only includes
KB-schema types.
",0,1,0
Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing,"  We introduce Synscapes -- a synthetic dataset for street scene parsing
created using photorealistic rendering techniques and show state-of-the-art
results for training and validation as well as new types of analysis. We study
the behavior of networks trained on real data when performing inference on
synthetic data: a key factor in determining the equivalence of simulation
environments. We also compare the behavior of networks trained on synthetic
data and evaluated on real-world data. Additionally by analyzing pre-trained
existing segmentation and detection models we illustrate how uncorrelated
images along with a detailed set of annotations open up new avenues for
analysis of computer vision systems providing fine-grain information about how
a model's performance changes according to factors such as distance occlusion
and relative object orientation.
",0,0,1
"MIPE: A Metric Independent Pipeline for Effective Code-Mixed NLG
  Evaluation","  Code-mixing is a phenomenon of mixing words and phrases from two or more
languages in a single utterance of speech and text. Due to the high linguistic
diversity code-mixing presents several challenges in evaluating standard
natural language generation (NLG) tasks. Various widely popular metrics perform
poorly with the code-mixed NLG tasks. To address this challenge we present a
metric independent evaluation pipeline MIPE that significantly improves the
correlation between evaluation metrics and human judgments on the generated
code-mixed text. As a use case we demonstrate the performance of MIPE on the
machine-generated Hinglish (code-mixing of Hindi and English languages)
sentences from the HinGE corpus. We can extend the proposed evaluation strategy
to other code-mixed language pairs NLG tasks and evaluation metrics with
minimal to no effort.
",0,1,0
Weakly-Supervised Spatial Context Networks,"  We explore the power of spatial context as a self-supervisory signal for
learning visual representations. In particular we propose spatial context
networks that learn to predict a representation of one image patch from another
image patch within the same image conditioned on their real-valued relative
spatial offset. Unlike auto-encoders that aim to encode and reconstruct
original image patches our network aims to encode and reconstruct intermediate
representations of the spatially offset patches. As such the network learns a
spatially conditioned contextual representation. By testing performance with
various patch selection mechanisms we show that focusing on object-centric
patches is important and that using object proposal as a patch selection
mechanism leads to the highest improvement in performance. Further unlike
auto-encoders context encoders [21] or other forms of unsupervised feature
learning we illustrate that contextual supervision (with pre-trained model
initialization) can improve on existing pre-trained model performance. We build
our spatial context networks on top of standard VGG_19 and CNN_M architectures
and among other things show that we can achieve improvements (with no
additional explicit supervision) over the original ImageNet pre-trained VGG_19
and CNN_M models in object categorization and detection on VOC2007.
",0,0,1
Knowledgeable Dialogue Reading Comprehension on Key Turns,"  Multi-choice machine reading comprehension (MRC) requires models to choose
the correct answer from candidate options given a passage and a question. Our
research focuses dialogue-based MRC where the passages are multi-turn
dialogues. It suffers from two challenges the answer selection decision is
made without support of latently helpful commonsense and the multi-turn
context may hide considerable irrelevant information. This work thus makes the
first attempt to tackle those two challenges by extracting substantially
important turns and utilizing external knowledge to enhance the representation
of context. In this paper the relevance of each turn to the question are
calculated to choose key turns. Besides terms related to the context and the
question in a knowledge graph are extracted as external knowledge. The original
context question and external knowledge are encoded with the pre-trained
language model then the language representation and key turns are combined
together with a will-designed mechanism to predict the answer. Experimental
results on a DREAM dataset show that our proposed model achieves great
improvements on baselines.
",0,1,0
Semantic Correspondence via 2D-3D-2D Cycle,"  Visual semantic correspondence is an important topic in computer vision and
could help machine understand objects in our daily life. However most previous
methods directly train on correspondences in 2D images which is end-to-end but
loses plenty of information in 3D spaces. In this paper we propose a new
method on predicting semantic correspondences by leveraging it to 3D domain and
then project corresponding 3D models back to 2D domain with their semantic
labels. Our method leverages the advantages in 3D vision and can explicitly
reason about objects self-occlusion and visibility. We show that our method
gives comparative and even superior results on standard semantic benchmarks. We
also conduct thorough and detailed experiments to analyze our network
components. The code and experiments are publicly available at
https://github.com/qq456cvb/SemanticTransfer.
",0,0,1
"Dynamic Coronary Roadmapping via Catheter Tip Tracking in X-ray
  Fluoroscopy with Deep Learning Based Bayesian Filtering","  Percutaneous coronary intervention (PCI) is typically performed with image
guidance using X-ray angiograms in which coronary arteries are opacified with
X-ray opaque contrast agents. Interventional cardiologists typically navigate
instruments using non-contrast-enhanced fluoroscopic images since higher use
of contrast agents increases the risk of kidney failure. When using
fluoroscopic images the interventional cardiologist needs to rely on a mental
anatomical reconstruction. This paper reports on the development of a novel
dynamic coronary roadmapping approach for improving visual feedback and
reducing contrast use during PCI. The approach compensates cardiac and
respiratory induced vessel motion by ECG alignment and catheter tip tracking in
X-ray fluoroscopy respectively. In particular for accurate and robust
tracking of the catheter tip we proposed a new deep learning based Bayesian
filtering method that integrates the detection outcome of a convolutional
neural network and the motion estimation between frames using a particle
filtering framework. The proposed roadmapping and tracking approaches were
validated on clinical X-ray images achieving accurate performance on both
catheter tip tracking and dynamic coronary roadmapping experiments. In
addition our approach runs in real-time on a computer with a single GPU and
has the potential to be integrated into the clinical workflow of PCI
procedures providing cardiologists with visual guidance during interventions
without the need of extra use of contrast agent.
",0,0,1
Child Face Age-Progression via Deep Feature Aging,"  Given a gallery of face images of missing children state-of-the-art face
recognition systems fall short in identifying a child (probe) recovered at a
later age. We propose a feature aging module that can age-progress deep face
features output by a face matcher. In addition the feature aging module guides
age-progression in the image space such that synthesized aged faces can be
utilized to enhance longitudinal face recognition performance of any face
matcher without requiring any explicit training. For time lapses larger than 10
years (the missing child is found after 10 or more years) the proposed
age-progression module improves the closed-set identification accuracy of
FaceNet from 16.53% to 21.44% and CosFace from 60.72% to 66.12% on a child
celebrity dataset namely ITWCC. The proposed method also outperforms
state-of-the-art approaches with a rank-1 identification rate of 95.91%
compared to 94.91% on a public aging dataset FG-NET and 99.58% compared to
99.50% on CACD-VS. These results suggest that aging face features enhances the
ability to identify young children who are possible victims of child
trafficking or abduction.
",0,0,1
One-shot Learning with Absolute Generalization,"  One-shot learning is proposed to make a pretrained classifier workable on a
new dataset based on one labeled samples from each pattern. However few of
researchers consider whether the dataset itself supports one-shot learning. In
this paper we propose a set of definitions to explain what kind of datasets
can support one-shot learning and propose the concept ""absolute
generalization"". Based on these definitions we proposed a method to build an
absolutely generalizable classifier. The proposed method concatenates two
samples as a new single sample and converts a classification problem to an
identity identification problem or a similarity metric problem. Experiments
demonstrate that the proposed method is superior to baseline on one-shot
learning datasets and artificial datasets.
",0,0,1
"Attentive Deep Regression Networks for Real-Time Visual Face Tracking in
  Video Surveillance","  Visual face tracking is one of the most important tasks in video surveillance
systems. However due to the variations in pose scale expression and
illumination it is considered to be a difficult task. Recent studies show that
deep learning methods have a significant potential in object tracking tasks and
adaptive feature selection methods can boost their performance. Motivated by
these we propose an end-to-end attentive deep learning based tracker that is
build on top of the state-of-the-art GOTURN tracker for the task of real-time
visual face tracking in video surveillance. Our method outperforms the
state-of-the-art GOTURN and IVT trackers by very large margins and it achieves
speeds that are very far beyond the requirements of real-time tracking.
Additionally to overcome the scarce data problem in visual face tracking we
also provide bounding box annotations for the G1 and G2 sets of ChokePoint
dataset and make it suitable for further studies in face tracking under
surveillance conditions.
",0,0,1
A Morphological Analyzer for Japanese Nouns Verbs and Adjectives,"  We present an open source morphological analyzer for Japanese nouns verbs
and adjectives. The system builds upon the morphological analyzing capabilities
of MeCab to incorporate finer details of classification such as politeness
tense mood and voice attributes. We implemented our analyzer in the form of a
finite state transducer using the open source finite state compiler FOMA
toolkit. The source code and tool is available at
https://bitbucket.org/skylander/yc-nlplab/.
",0,1,0
"As long as you talk about me: The importance of family firm brands and
  the contingent role of family-firm identity","  This study explores the role of external audiences in determining the
importance of family firm brands and the relationship with firm performance.
Drawing on text mining and social network analysis techniques and considering
the brand prevalence diversity and connectivity dimensions we use the
semantic brand score to measure the importance the media give to family firm
brands. The analysis of a sample of 52555 news articles published in 2017
about 63 Italian entrepreneurial families reveals that brand importance is
positively associated with family firm revenues and this relationship is
stronger when there is identity match between the family and the firm. This
study advances current literature by offering a rich and multifaceted
perspective on how external audiences perceptions of the brand shape family
firm performance.
",0,1,0
PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph,"  Despite some exciting progress on high-quality image generation from
structured(scene graphs) or free-form(sentences) descriptions most of them
only guarantee the image-level semantical consistency i.e. the generated image
matching the semantic meaning of the description. They still lack the
investigations on synthesizing the images in a more controllable way like
finely manipulating the visual appearance of every object. Therefore to
generate the images with preferred objects and rich interactions we propose a
semi-parametric method PasteGAN for generating the image from the scene graph
and the image crops where spatial arrangements of the objects and their
pair-wise relationships are defined by the scene graph and the object
appearances are determined by the given object crops. To enhance the
interactions of the objects in the output we design a Crop Refining Network
and an Object-Image Fuser to embed the objects as well as their relationships
into one map. Multiple losses work collaboratively to guarantee the generated
images highly respecting the crops and complying with the scene graphs while
maintaining excellent image quality. A crop selector is also proposed to pick
the most-compatible crops from our external object tank by encoding the
interactions around the objects in the scene graph if the crops are not
provided. Evaluated on Visual Genome and COCO-Stuff dataset our proposed
method significantly outperforms the SOTA methods on Inception Score Diversity
Score and Fr'echet Inception Distance. Extensive experiments also demonstrate
our method's ability to generate complex and diverse images with given objects.
",0,0,1
"Asymmetry Helps: Improved Private Information Retrieval Protocols for
  Distributed Storage","  We consider private information retrieval (PIR) for distributed storage
systems (DSSs) with noncolluding nodes where data is stored using a non maximum
distance separable (MDS) linear code. It was recently shown that if data is
stored using a particular class of non-MDS linear codes the MDS-PIR capacity
i.e. the maximum possible PIR rate for MDS-coded DSSs can be achieved. For
this class of codes we prove that the PIR capacity is indeed equal to the
MDS-PIR capacity giving the first family of non-MDS codes for which the PIR
capacity is known. For other codes we provide asymmetric PIR protocols that
achieve a strictly larger PIR rate compared to existing symmetric PIR
protocols.
",1,0,0
On The Compound MIMO Wiretap Channel with Mean Feedback,"  Compound MIMO wiretap channel with double sided uncertainty is considered
under channel mean information model. In mean information model channel
variations are centered around its mean value which is fed back to the
transmitter. We show that the worst case main channel is anti-parallel to the
channel mean information resulting in an overall unit rank channel. Further
the worst eavesdropper channel is shown to be isotropic around its mean
information. Accordingly we provide the capacity achieving beamforming
direction. We show that the saddle point property holds under mean information
model and thus compound secrecy capacity equals to the worst case capacity
over the class of uncertainty. Moreover capacity achieving beamforming
direction is found to require matrix inversion thus we derive the null
steering (NS) beamforming as an alternative suboptimal solution that does not
require matrix inversion. NS beamformer is in the direction orthogonal to the
eavesdropper mean channel that maintains the maximum possible gain in mean main
channel direction. Extensive computer simulation reveals that NS performs very
close to the optimal solution. It also verifies that NS beamforming
outperforms both maximum ratio transmission (MRT) and zero forcing (ZF)
beamforming approaches over the entire SNR range. Finally An equivalence
relation with MIMO wiretap channel in Rician fading environment is established.
",1,0,0
"Deep Representation Learning Characterized by Inter-class Separation for
  Image Clustering","  Despite significant advances in clustering methods in recent years the
outcome of clustering of a natural image dataset is still unsatisfactory due to
two important drawbacks. Firstly clustering of images needs a good feature
representation of an image and secondly we need a robust method which can
discriminate these features for making them belonging to different clusters
such that intra-class variance is less and inter-class variance is high. Often
these two aspects are dealt with independently and thus the features are not
sufficient enough to partition the data meaningfully. In this paper we propose
a method where we discover these features required for the separation of the
images using deep autoencoder. Our method learns the image representation
features automatically for the purpose of clustering and also select a coherent
image and an incoherent image simultaneously for a given image so that the
feature representation learning can learn better discriminative features for
grouping the similar images in a cluster and at the same time separating the
dissimilar images across clusters. Experiment results show that our method
produces significantly better result than the state-of-the-art methods and we
also show that our method is more generalized across different dataset without
using any pre-trained model like other existing methods.
",0,0,1
"Exploiting Computation Replication for Mobile Edge Computing: A
  Fundamental Computation-Communication Tradeoff Study","  Existing works on task offloading in mobile edge computing (MEC) networks
often assume a task is executed once at a single edge node (EN). Downloading
the computed result from the EN back to the mobile user may suffer long delay
if the downlink channel experiences strong interference or deep fading. This
paper exploits the idea of computation replication in MEC networks to speed up
the downloading phase. Computation replication allows each user to offload its
task to multiple ENs for repetitive execution so as to create multiple copies
of the computed result at different ENs which can then enable transmission
cooperation and hence reduce the communication latency for result downloading.
Yet computation replication may also increase the communication latency for
task uploading despite the obvious increase in computation load. The main
contribution of this work is to characterize asymptotically an order-optimal
upload-download communication latency pair for a given computation load in a
multi-user multi-server MEC network. Analysis shows when the computation load
increases within a certain range the downloading time decreases in an
inversely proportional way if it is binary offloading or decreases linearly if
it is partial offloading both at the expense of linear increase in the
uploading time.
",1,0,0
Capacity Bounds for Peak-Constrained Multiantenna Wideband Channels,"  We derive bounds on the noncoherent capacity of a very general class of
multiple-input multiple-output channels that allow for selectivity in time and
frequency as well as for spatial correlation. The bounds apply to
peak-constrained inputs; they are explicit in the channel's scattering
function are useful for a large range of bandwidth and allow to coarsely
identify the capacity-optimal combination of bandwidth and number of transmit
antennas. Furthermore we obtain a closed-form expression for the first-order
Taylor series expansion of capacity in the limit of infinite bandwidth. From
this expression we conclude that in the wideband regime: (i) it is optimal to
use only one transmit antenna when the channel is spatially uncorrelated; (ii)
rank-one statistical beamforming is optimal if the channel is spatially
correlated; and (iii) spatial correlation be it at the transmitter the
receiver or both is beneficial.
",1,0,0
Visible Feature Guidance for Crowd Pedestrian Detection,"  Heavy occlusion and dense gathering in crowd scene make pedestrian detection
become a challenging problem because it's difficult to guess a precise full
bounding box according to the invisible human part. To crack this nut we
propose a mechanism called Visible Feature Guidance (VFG) for both training and
inference. During training we adopt visible feature to regress the
simultaneous outputs of visible bounding box and full bounding box. Then we
perform NMS only on visible bounding boxes to achieve the best fitting full box
in inference. This manner can alleviate the incapable influence brought by NMS
in crowd scene and make full bounding box more precisely. Furthermore in order
to ease feature association in the post application process such as pedestrian
tracking we apply Hungarian algorithm to associate parts for a human instance.
Our proposed method can stably bring about 2~3% improvements in mAP and AP50
for both two-stage and one-stage detector. It's also more effective for MR-2
especially with the stricter IoU. Experiments on Crowdhuman Cityperson
Caltech and KITTI datasets show that visible feature guidance can help detector
achieve promisingly better performances. Moreover parts association produces a
strong benchmark on Crowdhuman for the vision community.
",0,0,1
"Launching into clinical space with medspaCy: a new clinical text
  processing toolkit in Python","  Despite impressive success of machine learning algorithms in clinical natural
language processing (cNLP) rule-based approaches still have a prominent role.
In this paper we introduce medspaCy an extensible open-source cNLP library
based on spaCy framework that allows flexible integration of rule-based and
machine learning-based algorithms adapted to clinical text. MedspaCy includes a
variety of components that meet common cNLP needs such as context analysis and
mapping to standard terminologies. By utilizing spaCy's clear and easy-to-use
conventions medspaCy enables development of custom pipelines that integrate
easily with other spaCy-based modules. Our toolkit includes several core
components and facilitates rapid development of pipelines for clinical text.
",0,1,0
Nutri-bullets: Summarizing Health Studies by Composing Segments,"  We introduce emphNutri-bullets a multi-document summarization task for
health and nutrition. First we present two datasets of food and health
summaries from multiple scientific studies. Furthermore we propose a novel
emphextract-compose model to solve the problem in the regime of limited
parallel data. We explicitly select key spans from several abstracts using a
policy network followed by composing the selected spans to present a summary
via a task specific language model. Compared to state-of-the-art methods our
approach leads to more faithful relevant and diverse summarization --
properties imperative to this application. For instance on the BreastCancer
dataset our approach gets a more than 50% improvement on relevance and
faithfulness.footnoteOur code and data is available at
urlhttps://github.com/darsh10/Nutribullets.
",0,1,0
Decoding Reed-Muller Codes Using Minimum-Weight Parity Checks,"  Reed-Muller (RM) codes exhibit good performance under maximum-likelihood (ML)
decoding due to their highly-symmetric structure. In this paper we explore the
question of whether the code symmetry of RM codes can also be exploited to
achieve near-ML performance in practice. The main idea is to apply iterative
decoding to a highly-redundant parity-check (PC) matrix that contains only the
minimum-weight dual codewords as rows. As examples we consider the peeling
decoder for the binary erasure channel linear-programming and belief
propagation (BP) decoding for the binary-input additive white Gaussian noise
channel and bit-flipping and BP decoding for the binary symmetric channel. For
short block lengths it is shown that near-ML performance can indeed be
achieved in many cases. We also propose a method to tailor the PC matrix to the
received observation by selecting only a small fraction of useful
minimum-weight PCs before decoding begins. This allows one to both improve
performance and significantly reduce complexity compared to using the full set
of minimum-weight PCs.
",1,0,0
On the Capacity of MIMO Optical Wireless Channels,"  This paper studies the capacity of a general multiple-input multiple-output
(MIMO) free-space optical intensity channel under a per-input-antenna
peak-power constraint and a total average-power constraint over all input
antennas. The focus is on the scenario with more transmit than receive
antennas. In this scenario different input vectors can yield identical
distributions at the output when they result in the same image vector under
multiplication by the channel matrix. We first determine the most
energy-efficient input vectors that attain each of these image vectors. Based
on this we derive an equivalent capacity expression in terms of the image
vector and establish new lower and upper bounds on the capacity of this
channel. The bounds match when the signal-to-noise ratio (SNR) tends to
infinity establishing the high-SNR asymptotic capacity. We also characterize
the low-SNR slope of the capacity of this channel.
",1,0,0
"Multiuser Joint Energy-Bandwidth Allocation with Energy Harvesting -
  Part II: Multiple Broadcast Channels & Proportional Fairness","  In this paper we consider the energy-bandwidth allocation for a network with
multiple broadcast channels where the transmitters access the network
orthogonally on the assigned frequency band and each transmitter communicates
with multiple receivers orthogonally or non-orthogonally. We assume that the
energy harvesting state and channel gain of each transmitter can be predicted
for $K$ slots em a priori. To maximize the weighted throughput we formulate
an optimization problem with $O(MK)$ constraints where $M$ is the number of
the receivers and decompose it into the energy and bandwidth allocation
subproblems. In order to use the iterative algorithm proposed in [1] to solve
the problem we propose efficient algorithms to solve the two subproblems so
that the optimal energy-bandwidth allocation can be obtained with an overall
complexity of $cal O(MK^2)$ even though the problem is non-convex when the
broadcast channel is non-orthogonal. For the orthogonal broadcast channel we
further formulate a proportionally-fair (PF) throughput maximization problem
and derive the equivalence conditions such that the optimal solution can be
obtained by solving a weighted throughput maximization problem. Further the
algorithm to obtain the proper weights is proposed. Simulation results show
that the proposed algorithm can make efficient use of the harvested energy and
the available bandwidth and achieve significantly better performance than some
heuristic policies for energy and bandwidth allocation. Moreover it is seen
that with energy-harvesting transmitters non-orthogonal broadcast offers
limited gain over orthogonal broadcast.
",1,0,0
Bootstrapping a Crosslingual Semantic Parser,"  Recent progress in semantic parsing scarcely considers languages other than
English but professional translation can be prohibitively expensive. We adapt a
semantic parser trained on a single language such as English to new languages
and multiple domains with minimal annotation. We query if machine translation
is an adequate substitute for training data and extend this to investigate
bootstrapping using joint training with English paraphrasing and multilingual
pre-trained models. We develop a Transformer-based parser combining paraphrases
by ensembling attention over multiple encoders and present new versions of ATIS
and Overnight in German and Chinese for evaluation. Experimental results
indicate that MT can approximate training data in a new language for accurate
parsing when augmented with paraphrasing through multiple MT engines.
Considering when MT is inadequate we also find that using our approach
achieves parsing accuracy within 2% of complete translation using only 50% of
training data.
",0,1,0
"Grammatical Gender Neo-Whorfianism and Word Embeddings: A Data-Driven
  Approach to Linguistic Relativity","  The relation between language and thought has occupied linguists for at least
a century. Neo-Whorfianism a weak version of the controversial Sapir-Whorf
hypothesis holds that our thoughts are subtly influenced by the grammatical
structures of our native language. One area of investigation in this vein
focuses on how the grammatical gender of nouns affects the way we perceive the
corresponding objects. For instance does the fact that key is masculine in
German (der Schl""ussel) but feminine in Spanish (la llave) change the
speakers' views of those objects? Psycholinguistic evidence presented by
Boroditsky et al. (2003 S4) suggested the answer might be yes: When asked
to produce adjectives that best described a key German and Spanish speakers
named more stereotypically masculine and feminine ones respectively. However
recent attempts to replicate those experiments have failed (Mickan et al.
2014). In this work we offer a computational analogue of Boroditsky et al.
(2003 S4)'s experimental design on 9 languages finding evidence against
neo-Whorfianism.
",0,1,0
Partial Convolution based Padding,"  In this paper we present a simple yet effective padding scheme that can be
used as a drop-in module for existing convolutional neural networks. We call it
partial convolution based padding with the intuition that the padded region
can be treated as holes and the original input as non-holes. Specifically
during the convolution operation the convolution results are re-weighted near
image borders based on the ratios between the padded area and the convolution
sliding window area. Extensive experiments with various deep network models on
ImageNet classification and semantic segmentation demonstrate that the proposed
padding scheme consistently outperforms standard zero padding with better
accuracy.
",0,0,1
"AVLnet: Learning Audio-Visual Language Representations from
  Instructional Videos","  Current methods for learning visually grounded language from videos often
rely on text annotation such as human generated captions or machine generated
automatic speech recognition (ASR) transcripts. In this work we introduce the
Audio-Video Language Network (AVLnet) a self-supervised network that learns a
shared audio-visual embedding space directly from raw video inputs. To
circumvent the need for text annotation we learn audio-visual representations
from randomly segmented video clips and their raw audio waveforms. We train
AVLnet on HowTo100M a large corpus of publicly available instructional videos
and evaluate on image retrieval and video retrieval tasks achieving
state-of-the-art performance. We perform analysis of AVLnet's learned
representations showing our model utilizes speech and natural sounds to learn
audio-visual concepts. Further we propose a tri-modal model that jointly
processes raw audio video and text captions from videos to learn a
multi-modal semantic embedding space useful for text-video retrieval. Our code
data and trained models will be released at avlnet.csail.mit.edu
",0,0,1
Domain Adversarial Training for Infrared-colour Person Re-Identification,"  Person re-identification (re-ID) is a very active area of research in
computer vision due to the role it plays in video surveillance. Currently
most methods only address the task of matching between colour images. However
in poorly-lit environments CCTV cameras switch to infrared imaging hence
developing a system which can correctly perform matching between infrared and
colour images is a necessity. In this paper we propose a part-feature
extraction network to better focus on subtle unique signatures on the person
which are visible across both infrared and colour modalities. To train the
model we propose a novel variant of the domain adversarial feature-learning
framework. Through extensive experimentation we show that our approach
outperforms state-of-the-art methods.
",0,0,1
Fast Marginalized Block Sparse Bayesian Learning Algorithm,"  The performance of sparse signal recovery from noise corrupted
underdetermined measurements can be improved if both sparsity and correlation
structure of signals are exploited. One typical correlation structure is the
intra-block correlation in block sparse signals. To exploit this structure a
framework called block sparse Bayesian learning (BSBL) has been proposed
recently. Algorithms derived from this framework showed superior performance
but they are not very fast which limits their applications. This work derives
an efficient algorithm from this framework using a marginalized likelihood
maximization method. Compared to existing BSBL algorithms it has close
recovery performance but is much faster. Therefore it is more suitable for
large scale datasets and applications requiring real-time implementation.
",1,0,0
"Augmenting Implicit Neural Shape Representations with Explicit
  Deformation Fields","  Implicit neural representation is a recent approach to learn shape
collections as zero level-sets of neural networks where each shape is
represented by a latent code. So far the focus has been shape reconstruction
while shape generalization was mostly left to generic encoder-decoder or
auto-decoder regularization.
  In this paper we advocate deformation-aware regularization for implicit
neural representations aiming at producing plausible deformations as latent
code changes. The challenge is that implicit representations do not capture
correspondences between different shapes which makes it difficult to represent
and regularize their deformations. Thus we propose to pair the implicit
representation of the shapes with an explicit piecewise linear deformation
field learned as an auxiliary function. We demonstrate that by regularizing
these deformation fields we can encourage the implicit neural representation
to induce natural deformations in the learned shape space such as
as-rigid-as-possible deformations.
",0,0,1
Computing rank-revealing factorizations of matrices stored out-of-core,"  This paper describes efficient algorithms for computing rank-revealing
factorizations of matrices that are too large to fit in RAM and must instead
be stored on slow external memory devices such as solid-state or spinning disk
hard drives (out-of-core or out-of-memory). Traditional algorithms for
computing rank revealing factorizations such as the column pivoted QR
factorization or techniques for computing a full singular value decomposition
of a matrix are very communication intensive. They are naturally expressed as
a sequence of matrix-vector operations which become prohibitively expensive
when data is not available in main memory. Randomization allows these methods
to be reformulated so that large contiguous blocks of the matrix can be
processed in bulk. The paper describes two distinct methods. The first is a
blocked version of column pivoted Householder QR organized as a ""left-looking""
method to minimize the number of write operations (which are more expensive
than read operations on a spinning disk drive). The second method results in a
so called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where
$U$ and $V$ are unitary and $T$ is triangular. This method is organized as an
algorithm-by-blocks in which floating point operations overlap read and write
operations. The second method incorporates power iterations and is
exceptionally good at revealing the numerical rank; it can often be used as a
substitute for a full singular value decomposition. Numerical experiments
demonstrate that the new algorithms are almost as fast when processing data
stored on a hard drive as traditional algorithms are for data stored in main
memory. To be precise the computational time for fully factorizing an $ntimes
n$ matrix scales as $cn^3$ with a scaling constant $c$ that is only
marginally larger when the matrix is stored out of core.
",0,1,0
Learning to segment microscopy images with lazy labels,"  The need for labour intensive pixel-wise annotation is a major limitation of
many fully supervised learning methods for segmenting bioimages that can
contain numerous object instances with thin separations. In this paper we
introduce a deep convolutional neural network for microscopy image
segmentation. Annotation issues are circumvented by letting the network being
trainable on coarse labels combined with only a very small number of images
with pixel-wise annotations. We call this new labelling strategy `lazy' labels.
Image segmentation is stratified into three connected tasks: rough inner region
detection object separation and pixel-wise segmentation. These tasks are
learned in an end-to-end multi-task learning framework. The method is
demonstrated on two microscopy datasets where we show that the model gives
accurate segmentation results even if exact boundary labels are missing for a
majority of annotated data. It brings more flexibility and efficiency for
training deep neural networks that are data hungry and is applicable to
biomedical images with poor contrast at the object boundaries or with diverse
textures and repeated patterns.
",0,0,1
"Towards Neural Speaker Modeling in Multi-Party Conversation: The Task
  Dataset and Models","  Neural network-based dialog systems are attracting increasing attention in
both academia and industry. Recently researchers have begun to realize the
importance of speaker modeling in neural dialog systems but there lacks
established tasks and datasets. In this paper we propose speaker
classification as a surrogate task for general speaker modeling and collect
massive data to facilitate research in this direction. We further investigate
temporal-based and content-based models of speakers and propose several
hybrids of them. Experiments show that speaker classification is feasible and
that hybrid models outperform each single component.
",0,1,0
CPR: Classifier-Projection Regularization for Continual Learning,"  We propose a general yet simple patch that can be applied to existing
regularization-based continual learning methods called classifier-projection
regularization (CPR). Inspired by both recent results on neural networks with
wide local minima and information theory CPR adds an additional regularization
term that maximizes the entropy of a classifier's output probability. We
demonstrate that this additional term can be interpreted as a projection of the
conditional probability given by a classifier's output to the uniform
distribution. By applying the Pythagorean theorem for KL divergence we then
prove that this projection may (in theory) improve the performance of continual
learning methods. In our extensive experimental results we apply CPR to
several state-of-the-art regularization-based continual learning methods and
benchmark performance on popular image recognition datasets. Our results
demonstrate that CPR indeed promotes a wide local minima and significantly
improves both accuracy and plasticity while simultaneously mitigating the
catastrophic forgetting of baseline continual learning methods. The codes and
scripts for this work are available at https://github.com/csm9493/CPR_CL.
",0,0,1
"SegCodeNet: Color-Coded Segmentation Masks for Activity Detection from
  Wearable Cameras","  Activity detection from first-person videos (FPV) captured using a wearable
camera is an active research field with potential applications in many sectors
including healthcare law enforcement and rehabilitation. State-of-the-art
methods use optical flow-based hybrid techniques that rely on features derived
from the motion of objects from consecutive frames. In this work we developed
a two-stream network the emphSegCodeNet that uses a network branch
containing video-streams with color-coded semantic segmentation masks of
relevant objects in addition to the original RGB video-stream. We also include
a stream-wise attention gating that prioritizes between the two streams and a
frame-wise attention module that prioritizes the video frames that contain
relevant features. Experiments are conducted on an FPV dataset containing $18$
activity classes in office environments. In comparison to a single-stream
network the proposed two-stream method achieves an absolute improvement of
$14.366%$ and $10.324%$ for averaged F1 score and accuracy respectively
when average results are compared for three different frame sizes
$224times224$ $112times112$ and $64times64$. The proposed method provides
significant performance gains for lower-resolution images with absolute
improvements of $17%$ and $26%$ in F1 score for input dimensions of
$112times112$ and $64times64$ respectively. The best performance is achieved
for a frame size of $224times224$ yielding an F1 score and accuracy of
$90.176%$ and $90.799%$ which outperforms the state-of-the-art Inflated 3D
ConvNet (I3D) citecarreira2017quo method by an absolute margin of $4.529%$
and $2.419%$ respectively.
",0,0,1
Lensless computational imaging through deep learning,"  Deep learning has been proven to yield reliably generalizable answers to
numerous classification and decision tasks. Here we demonstrate for the first
time to our knowledge that deep neural networks (DNNs) can be trained to
solve inverse problems in computational imaging. We experimentally demonstrate
a lens-less imaging system where a DNN was trained to recover a phase object
given a raw intensity image recorded some distance away.
",0,0,1
"Population-based Respiratory 4D Motion Atlas Construction and its
  Application for VR Simulations of Liver Punctures","  Virtual reality (VR) training simulators of liver needle insertion in the
hepatic area of breathing virtual patients currently need 4D data acquisitions
as a prerequisite. Here first a population-based breathing virtual patient 4D
atlas can be built and second the requirement of a dose-relevant or expensive
acquisition of a 4D data set for a new static 3D patient can be mitigated by
warping the mean atlas motion. The breakthrough contribution of this work is
the construction and reuse of population-based learned 4D motion models.
",0,0,1
End-To-End Face Detection and Recognition,"  Plenty of face detection and recognition methods have been proposed and got
delightful results in decades. Common face recognition pipeline consists of: 1)
face detection 2) face alignment 3) feature extraction 4) similarity
calculation which are separated and independent from each other. The separated
face analyzing stages lead the model redundant calculation and are hard for
end-to-end training. In this paper we proposed a novel end-to-end trainable
convolutional network framework for face detection and recognition in which a
geometric transformation matrix was directly learned to align the faces
instead of predicting the facial landmarks. In training stage our single CNN
model is supervised only by face bounding boxes and personal identities which
are publicly available from WIDER FACE citeYang2016 dataset and
CASIA-WebFace citeYi2014 dataset. Tested on Face Detection Dataset and
Benchmark (FDDB) citeJain2010 dataset and Labeled Face in the Wild (LFW)
citeHuang2007 dataset we have achieved 89.24% recall for face detection
task and 98.63% verification accuracy for face recognition task
simultaneously which are comparable to state-of-the-art results.
",0,0,1
A Simple Design of IRS-NOMA Transmission,"  This letter proposes a simple design of intelligent reflecting surface (IRS)
assisted non-orthogonal multiple access (NOMA) transmission which can ensure
that more users are served on each orthogonal spatial direction than spatial
division multiple access (SDMA). In particular by employing IRS the
directions of users' channel vectors can be effectively aligned which
facilitates the implementation of NOMA. Both analytical and simulation results
are provided to demonstrate the performance of the proposed IRS-NOMA scheme and
also study the impact of hardware impairments on IRS-NOMA.
",1,0,0
"Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for
  Digital Holography","  In Digital Holography (DH) it is crucial to extract the object distance from
a hologram in order to reconstruct its amplitude and phase. This step is called
auto-focusing and it is conventionally solved by first reconstructing a stack
of images and then by sharpening each reconstructed image using a focus metric
such as entropy or variance. The distance corresponding to the sharpest image
is considered the focal position. This approach while effective is
computationally demanding and time-consuming. In this paper the determination
of the distance is performed by Deep Learning (DL). Two deep learning (DL)
architectures are compared: Convolutional Neural Network (CNN) and Vision
Transformer (ViT). ViT and CNN are used to cope with the problem of
auto-focusing as a classification problem. Compared to a first attempt [11] in
which the distance between two consecutive classes was 100$mu$m our proposal
allows us to drastically reduce this distance to 1$mu$m. Moreover ViT reaches
similar accuracy and is more robust than CNN.
",0,0,1
"Morphological Analysis of the Bishnupriya Manipuri Language using Finite
  State Transducers","  In this work we present a morphological analysis of Bishnupriya Manipuri
language an Indo-Aryan language spoken in the north eastern India. As of now
there is no computational work available for the language. Finite state
morphology is one of the successful approaches applied in a wide variety of
languages over the year. Therefore we adapted the finite state approach to
analyse morphology of the Bishnupriya Manipuri language.
",0,1,0
"GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image
  Interpretation","  This paper describes Georeference Contrastive Learning of visual
Representation (GeoCLR) for efficient training of deep-learning Convolutional
Neural Networks (CNNs). The method leverages georeference information by
generating a similar image pair using images taken of nearby locations and
contrasting these with an image pair that is far apart. The underlying
assumption is that images gathered within a close distance are more likely to
have similar visual appearance where this can be reasonably satisfied in
seafloor robotic imaging applications where image footprints are limited to
edge lengths of a few metres and are taken so that they overlap along a
vehicle's trajectory whereas seafloor substrates and habitats have patch sizes
that are far larger. A key advantage of this method is that it is
self-supervised and does not require any human input for CNN training. The
method is computationally efficient where results can be generated between
dives during multi-day AUV missions using computational resources that would be
accessible during most oceanic field trials. We apply GeoCLR to habitat
classification on a dataset that consists of ~86k images gathered using an
Autonomous Underwater Vehicle (AUV). We demonstrate how the latent
representations generated by GeoCLR can be used to efficiently guide human
annotation efforts where the semi-supervised framework improves classification
accuracy by an average of 11.8 % compared to state-of-the-art transfer learning
using the same CNN and equivalent number of human annotations for training.
",0,0,1
Minimizing Base Station Power Consumption,"  We propose a new radio resource management algorithm which aims at minimizing
the base station supply power consumption for multi-user MIMO-OFDM. Given a
base station power model that establishes a relation between the RF transmit
power and the supply power consumption the algorithm optimizes the trade-off
between three basic power-saving mechanisms: antenna adaptation power control
and discontinuous transmission. The algorithm comprises two steps: a) the first
step estimates sleep mode duration resource shares and antenna configuration
based on average channel conditions and b) the second step exploits
instantaneous channel knowledge at the transmitter for frequency selective
time-variant channels. The proposed algorithm finds the number of transmit
antennas the RF transmission power per resource unit and spatial channel the
number of discontinuous transmission time slots and the multi-user resource
allocation such that supply power consumption is minimized. Simulation results
indicate that the proposed algorithm is capable of reducing the supply power
consumption by between 25% and 40% dependend on the system load.
",1,0,0
"Efficient Volumetric Fusion of Airborne and Street-Side Data for Urban
  Reconstruction","  Airborne acquisition and on-road mobile mapping provide complementary 3D
information of an urban landscape: the former acquires roof structures ground
and vegetation at a large scale but lacks the facade and street-side details
while the latter is incomplete for higher floors and often totally misses out
on pedestrian-only areas or undriven districts. In this work we introduce an
approach that efficiently unifies a detailed street-side Structure-from-Motion
(SfM) or Multi-View Stereo (MVS) point cloud and a coarser but more complete
point cloud from airborne acquisition in a joint surface mesh. We propose a
point cloud blending and a volumetric fusion based on ray casting across a 3D
tetrahedralization (3DT) extended with data reduction techniques to handle
large datasets. To the best of our knowledge we are the first to adopt a 3DT
approach for airborne/street-side data fusion. Our pipeline exploits typical
characteristics of airborne and ground data and produces a seamless
watertight mesh that is both complete and detailed. Experiments on 3D urban
data from multiple sources and different data densities show the effectiveness
and benefits of our approach.
",0,0,1
"Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular
  Diseases","  Purpose: To demonstrate that retinal microvasculature per se is a reliable
biomarker for Diabetic Retinopathy (DR) and by extension cardiovascular
diseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to
color fundus images for semantic segmentation of the blood vessels and severity
classification on both vascular and full images. Vessel reconstruction through
harmonic descriptors is also used as a smoothing and de-noising tool. The
mathematical background of the theory is also outlined. Results: For diabetic
patients at least 93.8% of DR No-Refer vs. Refer classification can be related
to vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening
case the ratio is as high as 96.7%. Conclusion: In the case of DR most of the
disease biomarkers are related topologically to the vasculature. Translational
Relevance: Experiments conducted on eye blood vasculature reconstruction as a
biomarker shows a strong correlation between vasculature shape and later stages
of DR.
",0,0,1
"Practical Auto-Calibration for Spatial Scene-Understanding from
  Crowdsourced Dashcamera Videos","  Spatial scene-understanding including dense depth and ego-motion estimation
is an important problem in computer vision for autonomous vehicles and advanced
driver assistance systems. Thus it is beneficial to design perception modules
that can utilize crowdsourced videos collected from arbitrary vehicular onboard
or dashboard cameras. However the intrinsic parameters corresponding to such
cameras are often unknown or change over time. Typical manual calibration
approaches require objects such as a chessboard or additional scene-specific
information. On the other hand automatic camera calibration does not have such
requirements. Yet the automatic calibration of dashboard cameras is
challenging as forward and planar navigation results in critical motion
sequences with reconstruction ambiguities. Structure reconstruction of complete
visual-sequences that may contain tens of thousands of images is also
computationally untenable. Here we propose a system for practical monocular
onboard camera auto-calibration from crowdsourced videos. We show the
effectiveness of our proposed system on the KITTI raw Oxford RobotCar and the
crowdsourced D$^2$-City datasets in varying conditions. Finally we demonstrate
its application for accurate monocular dense depth and ego-motion estimation on
uncalibrated videos.
",0,0,1
One-bit LFMCW Radar: Spectrum Analysis and Target Detection,"  One-bit radar performing signal sampling and quantization by a one-bit ADC
is a promising technology for many civilian applications due to its low-cost
and low-power consumptions. In this paper problems encountered by one-bit
LFMCW radar are studied and a two-stage target detection method termed as the
dimension-reduced generalized approximate message passing (DR-GAMP) approach is
proposed. Firstly the spectrum of one-bit quantized signals in a scenario with
multiple targets is analyzed. It is indicated that high-order harmonics may
result in false alarms (FAs) and cannot be neglected. Secondly based on the
spectrum analysis the DR-GAMP approach is proposed to carry out target
detection. Specifically linear preprocessing methods and target predetection
are firstly adopted to perform the dimension reduction and then the GAMP
algorithm is utilized to suppress high-order harmonics and recover true
targets. Finally numerical simulations are conducted to evaluate the
performance of one-bit LFMCW radar under typical parameters. It is shown that
compared to the conventional radar applying linear processing methods one-bit
LFMCW radar has about $1.3$ dB performance gain when the input signal-to-noise
ratios (SNRs) of targets are low. In the presence of a strong target it has
about $1.0$ dB performance loss.
",1,0,0
Unpaired Learning for High Dynamic Range Image Tone Mapping,"  High dynamic range (HDR) photography is becoming increasingly popular and
available by DSLR and mobile-phone cameras. While deep neural networks (DNN)
have greatly impacted other domains of image manipulation their use for HDR
tone-mapping is limited due to the lack of a definite notion of ground-truth
solution which is needed for producing training data.
  In this paper we describe a new tone-mapping approach guided by the distinct
goal of producing low dynamic range (LDR) renditions that best reproduce the
visual characteristics of native LDR images. This goal enables the use of an
unpaired adversarial training based on unrelated sets of HDR and LDR images
both of which are widely available and easy to acquire.
  In order to achieve an effective training under this minimal requirements we
introduce the following new steps and components: (i) a range-normalizing
pre-process which estimates and applies a different level of curve-based
compression (ii) a loss that preserves the input content while allowing the
network to achieve its goal and (iii) the use of a more concise discriminator
network designed to promote the reproduction of low-level attributes native
LDR possess.
  Evaluation of the resulting network demonstrates its ability to produce
photo-realistic artifact-free tone-mapped images and state-of-the-art
performance on different image fidelity indices and visual distances.
",0,0,1
End-to-end learning potentials for structured attribute prediction,"  We present a structured inference approach in deep neural networks for
multiple attribute prediction. In attribute prediction a common approach is to
learn independent classifiers on top of a good feature representation. However
such classifiers assume conditional independence on features and do not
explicitly consider the dependency between attributes in the inference process.
We propose to formulate attribute prediction in terms of marginal inference in
the conditional random field. We model potential functions by deep neural
networks and apply the sum-product algorithm to solve for the approximate
marginal distribution in feed-forward networks. Our message passing layer
implements sparse pairwise potentials by a softplus-linear function that is
equivalent to a higher-order classifier and learns all the model parameters by
end-to-end back propagation. The experimental results using SUN attributes and
CelebA datasets suggest that the structured inference improves the attribute
prediction performance and possibly uncovers the hidden relationship between
attributes.
",0,0,1
"Manual Evaluation Matters: Reviewing Test Protocols of Distantly
  Supervised Relation Extraction","  Distantly supervised (DS) relation extraction (RE) has attracted much
attention in the past few years as it can utilize large-scale auto-labeled
data. However its evaluation has long been a problem: previous works either
took costly and inconsistent methods to manually examine a small sample of
model predictions or directly test models on auto-labeled data -- which by
our check produce as much as 53% wrong labels at the entity pair level in the
popular NYT10 dataset. This problem has not only led to inaccurate evaluation
but also made it hard to understand where we are and what's left to improve in
the research of DS-RE. To evaluate DS-RE models in a more credible way we
build manually-annotated test sets for two DS-RE datasets NYT10 and Wiki20
and thoroughly evaluate several competitive models especially the latest
pre-trained ones. The experimental results show that the manual evaluation can
indicate very different conclusions from automatic ones especially some
unexpected observations e.g. pre-trained models can achieve dominating
performance while being more susceptible to false-positives compared to
previous methods. We hope that both our manual test sets and novel observations
can help advance future DS-RE research.
",0,1,0
An operational information decomposition via synergistic disclosure,"  Multivariate information decompositions hold promise to yield insight into
complex systems and stand out for their ability to identify synergistic
phenomena. However the adoption of these approaches has been hindered by there
being multiple possible decompositions and no precise guidance for preferring
one over the others. At the heart of this disagreement lies the absence of a
clear operational interpretation of what synergistic information is. Here we
fill this gap by proposing a new information decomposition based on a novel
operationalisation of informational synergy which leverages recent
developments in the literature of data privacy. Our decomposition is defined
for any number of information sources and its atoms can be calculated using
elementary optimisation techniques. The decomposition provides a natural
coarse-graining that scales gracefully with the system's size and is
applicable in a wide range of scenarios of practical interest.
",1,0,0
"A Camera That CNNs: Towards Embedded Neural Networks on Pixel Processor
  Arrays","  We present a convolutional neural network implementation for pixel processor
array (PPA) sensors. PPA hardware consists of a fine-grained array of
general-purpose processing elements each capable of light capture data
storage program execution and communication with neighboring elements. This
allows images to be stored and manipulated directly at the point of light
capture rather than having to transfer images to external processing hardware.
Our CNN approach divides this array up into 4x4 blocks of processing elements
essentially trading-off image resolution for increased local memory capacity
per 4x4 ""pixel"". We implement parallel operations for image addition
subtraction and bit-shifting images in this 4x4 block format. Using these
components we formulate how to perform ternary weight convolutions upon these
images compactly store results of such convolutions perform max-pooling and
transfer the resulting sub-sampled data to an attached micro-controller. We
train ternary weight filter CNNs for digit recognition and a simple tracking
task and demonstrate inference of these networks upon the SCAMP5 PPA system.
This work represents a first step towards embedding neural network processing
capability directly onto the focal plane of a sensor.
",0,0,1
"SummaRuNNer: A Recurrent Neural Network based Sequence Model for
  Extractive Summarization of Documents","  We present SummaRuNNer a Recurrent Neural Network (RNN) based sequence model
for extractive summarization of documents and show that it achieves performance
better than or comparable to state-of-the-art. Our model has the additional
advantage of being very interpretable since it allows visualization of its
predictions broken up by abstract features such as information content
salience and novelty. Another novel contribution of our work is abstractive
training of our extractive model that can train on human generated reference
summaries alone eliminating the need for sentence-level extractive labels.
",0,1,0
Articulated Hand Pose Estimation Review,"  With the increase number of companies focusing on commercializing Augmented
Reality (AR) Virtual Reality (VR) and wearable devices the need for a hand
based input mechanism is becoming essential in order to make the experience
natural seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers) close similarity between
fingers dexterity of the hands speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques it is a subset of some of the recent state-of-the-art
techniques.
",0,0,1
"Marine Wireless Big Data: Efficient Transmission Related Applications
  and Challenges","  The vast volume of marine wireless sampling data and its continuously
explosive growth herald the coming of the era of marine wireless big data. Two
challenges imposed by these data are how to fast reliably and sustainably
deliver them in extremely hostile marine environments and how to apply them
after collection. In this article we first propose an architecture of
heterogeneous marine networks that flexibly exploits the existing underwater
wireless techniques as a potential solution for fast data transmission. We then
investigate the possibilities of and develop the schemes for energy-efficient
and reliable undersea transmission without or slightly with data rate
reduction. After discussing the data transmission we summarize the possible
applications of the collected big data and particularly focus on the problems
of applying these data in sea-surface object detection and marine object
recognition. Open issues and challenges that need to be further explored
regarding transmission and detection/recognition are also discussed in the
article.
",1,0,0
Cross-modal Deep Face Normals with Deactivable Skip Connections,"  We present an approach for estimating surface normals from in-the-wild color
images of faces. While data-driven strategies have been proposed for single
face images limited available ground truth data makes this problem difficult.
To alleviate this issue we propose a method that can leverage all available
image and normal data whether paired or not thanks to a novel cross-modal
learning architecture. In particular we enable additional training with single
modality data either color or normal by using two encoder-decoder networks
with a shared latent space. The proposed architecture also enables face details
to be transferred between the image and normal domains given paired data
through skip connections between the image encoder and normal decoder. Core to
our approach is a novel module that we call deactivable skip connections which
allows integrating both the auto-encoded and image-to-normal branches within
the same architecture that can be trained end-to-end. This allows learning of a
rich latent space that can accurately capture the normal information. We
compare against state-of-the-art methods and show that our approach can achieve
significant improvements both quantitative and qualitative with natural face
images.
",0,0,1
Detecting Blurred Ground-based Sky/Cloud Images,"  Ground-based whole sky imagers (WSIs) are being used by researchers in
various fields to study the atmospheric events. These ground-based sky cameras
capture visible-light images of the sky at regular intervals of time. Owing to
the atmospheric interference and camera sensor noise the captured images often
exhibit noise and blur. This may pose a problem in subsequent image processing
stages. Therefore it is important to accurately identify the blurred images.
This is a difficult task as clouds have varying shapes textures and soft
edges whereas the sky acts as a homogeneous and uniform background. In this
paper we propose an efficient framework that can identify the blurred
sky/cloud images. Using a static external marker our proposed methodology has
a detection accuracy of 94%. To the best of our knowledge our approach is the
first of its kind in the automatic identification of blurred images for
ground-based sky/cloud images.
",0,0,1
"Structure-Tags Improve Text Classification for Scholarly Document
  Quality Prediction","  Training recurrent neural networks on long texts in particular scholarly
documents causes problems for learning. While hierarchical attention networks
(HANs) are effective in solving these problems they still lose important
information about the structure of the text. To tackle these problems we
propose the use of HANs combined with structure-tags which mark the role of
sentences in the document. Adding tags to sentences marking them as
corresponding to title abstract or main body text yields improvements over
the state-of-the-art for scholarly document quality prediction. The proposed
system is applied to the task of accept/reject prediction on the PeerRead
dataset and compared against a recent BiLSTM-based model and joint
textual+visual model as well as against plain HANs. Compared to plain HANs
accuracy increases on all three domains. On the computation and language domain
our new model works best overall and increases accuracy 4.7% over the best
literature result. We also obtain improvements when introducing the tags for
prediction of the number of citations for 88k scientific publications that we
compiled from the Allen AI S2ORC dataset. For our HAN-system with
structure-tags we reach 28.5% explained variance an improvement of 1.8% over
our reimplementation of the BiLSTM-based model as well as 1.0% improvement over
plain HANs.
",0,1,0
Answering Open-Domain Questions of Varying Reasoning Steps from Text,"  We develop a unified system to answer directly from text open-domain
questions that may require a varying number of retrieval steps. We employ a
single multi-task transformer model to perform all the necessary subtasks --
retrieving supporting facts reranking them and predicting the answer from all
retrieved documents -- in an iterative fashion. We avoid crucial assumptions of
previous work that do not transfer well to real-world settings including
exploiting knowledge of the fixed number of retrieval steps required to answer
each question or using structured metadata like knowledge bases or web links
that have limited availability. Instead we design a system that can answer
open-domain questions on any text collection without prior knowledge of
reasoning complexity. To emulate this setting we construct a new benchmark
called BeerQA by combining existing one- and two-step datasets with a new
collection of 530 questions that require three Wikipedia pages to answer
unifying Wikipedia corpora versions in the process. We show that our model
demonstrates competitive performance on both existing benchmarks and this new
benchmark. We make the new benchmark available at https://beerqa.github.io/.
",0,1,0
Content-Based Weak Supervision for Ad-Hoc Re-Ranking,"  One challenge with neural ranking is the need for a large amount of
manually-labeled relevance judgments for training. In contrast with prior work
we examine the use of weak supervision sources for training that yield pseudo
query-document pairs that already exhibit relevance (e.g. newswire
headline-content pairs and encyclopedic heading-paragraph pairs). We also
propose filtering techniques to eliminate training samples that are too far out
of domain using two techniques: a heuristic-based approach and novel supervised
filter that re-purposes a neural ranker. Using several leading neural ranking
architectures and multiple weak supervision datasets we show that these
sources of training pairs are effective on their own (outperforming prior weak
supervision techniques) and that filtering can further improve performance.
",0,1,0
Conceptualization Topic Modeling,"  Recently topic modeling has been widely used to discover the abstract topics
in text corpora. Most of the existing topic models are based on the assumption
of three-layer hierarchical Bayesian structure i.e. each document is modeled
as a probability distribution over topics and each topic is a probability
distribution over words. However the assumption is not optimal. Intuitively
it's more reasonable to assume that each topic is a probability distribution
over concepts and then each concept is a probability distribution over words
i.e. adding a latent concept layer between topic layer and word layer in
traditional three-layer assumption. In this paper we verify the proposed
assumption by incorporating the new assumption in two representative topic
models and obtain two novel topic models. Extensive experiments were conducted
among the proposed models and corresponding baselines and the results show
that the proposed models significantly outperform the baselines in terms of
case study and perplexity which means the new assumption is more reasonable
than traditional one.
",0,1,0
"Statistical Blockage Modeling and Robustness of Beamforming in
  Millimeter Wave Systems","  There has been a growing interest in the commercialization of millimeter wave
(mmW) technology as a part of the Fifth-Generation New Radio (5G-NR) wireless
standardization efforts. In this direction many sets of independent
measurement campaigns show that wireless propagation at mmW carrier frequencies
is only marginally worse than propagation at sub-6 GHz carrier frequencies for
small-cell coverage --- one of the most important use-cases for 5G-NR. On the
other hand the biggest determinants of viability of mmW systems in practice
are penetration and blockage of mmW signals through different materials in the
scattering environment. With this background the focus of this paper is on
understanding the impact of blockage of mmW signals and reduced spatial
coverage due to penetration through the human hand body vehicles etc.
Leveraging measurements with a 28 GHz mmW experimental prototype and
electromagnetic simulation studies we first propose statistical blockage
models to capture the impact of the hand human body and vehicles. We then
study the time-scales at which mmW signals are disrupted by blockage (hand and
human body). Our results show that these events can be attributed to physical
movements and the time-scales corresponding to blockage are hence on the order
of a few 100 ms or more. Building on this fundamental understanding we finally
consider the broader question of robustness of mmW beamforming to handle
blockage. Network densification subarray switching in a user equipment (UE)
designed with multiple subarrays fall back mechanisms such as codebook
enhancements and switching to legacy carriers in non-standalone deployments
etc. can address blockage before it leads to a deleterious impact on the mmW
link margin.
",1,0,0
IgNet A Super-precise Convolutional Neural Network,"  Convolutional neural networks (CNN) are known to be an effective means to
detect and analyze images. Their power is essentially based on the ability to
extract out images common features. There exist however images involving
unique irregular features or details. Such is a collection of unusual children
drawings reflecting the kids imagination and individuality. These drawings were
analyzed by means of a CNN constructed by means of Keras-TensorFlow. The same
problem - on a significantly higher level - was solved with newly developed
family of networks called IgNet that is described in this paper. It proved able
to learn by 100 % all the categorical characteristics of the drawings. In the
case of a regression task (learning the young artists ages) IgNet performed
with an error of no more than 0.4 %. The principles are discussed of IgNet
design that made it possible to reach such substantial results with rather
simple network topology.
",0,0,1
Context-Aware Embeddings for Automatic Art Analysis,"  Automatic art analysis aims to classify and retrieve artistic representations
from a collection of images by using computer vision and machine learning
techniques. In this work we propose to enhance visual representations from
neural networks with contextual artistic information. Whereas visual
representations are able to capture information about the content and the style
of an artwork our proposed context-aware embeddings additionally encode
relationships between different artistic attributes such as author school or
historical period. We design two different approaches for using context in
automatic art analysis. In the first one contextual data is obtained through a
multi-task learning model in which several attributes are trained together to
find visual relationships between elements. In the second approach context is
obtained through an art-specific knowledge graph which encodes relationships
between artistic attributes. An exhaustive evaluation of both of our models in
several art analysis problems such as author identification type
classification or cross-modal retrieval show that performance is improved by
up to 7.3% in art classification and 37.24% in retrieval when context-aware
embeddings are used.
",0,0,1
SemEval-2014 Task 9: Sentiment Analysis in Twitter,"  We describe the Sentiment Analysis in Twitter task ran as part of
SemEval-2014. It is a continuation of the last year's task that ran
successfully as part of SemEval-2013. As in 2013 this was the most popular
SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21
teams) and 50 submissions for subtask B (44 teams). This year we introduced
three new test sets: (i) regular tweets (ii) sarcastic tweets and (iii)
LiveJournal sentences. We further tested on (iv) 2013 tweets and (v) 2013 SMS
messages. The highest F1-score on (i) was achieved by NRC-Canada at 86.63 for
subtask A and by TeamX at 70.96 for subtask B.
",0,1,0
Syntactic Patterns Improve Information Extraction for Medical Search,"  Medical professionals search the published literature by specifying the type
of patients the medical intervention(s) and the outcome measure(s) of
interest. In this paper we demonstrate how features encoding syntactic patterns
improve the performance of state-of-the-art sequence tagging models (both
linear and neural) for information extraction of these medically relevant
categories. We present an analysis of the type of patterns exploited and the
semantic space induced for these i.e. the distributed representations learned
for identified multi-token patterns. We show that these learned representations
differ substantially from those of the constituent unigrams suggesting that
the patterns capture contextual information that is otherwise lost.
",0,1,0
"Generation of Multimodal Justification Using Visual Word Constraint
  Model for Explainable Computer-Aided Diagnosis","  The ambiguity of the decision-making process has been pointed out as the main
obstacle to applying the deep learning-based method in a practical way in spite
of its outstanding performance. Interpretability could guarantee the confidence
of deep learning system therefore it is particularly important in the medical
field. In this study a novel deep network is proposed to explain the
diagnostic decision with visual pointing map and diagnostic sentence justifying
result simultaneously. For the purpose of increasing the accuracy of sentence
generation a visual word constraint model is devised in training justification
generator. To verify the proposed method comparative experiments were
conducted on the problem of the diagnosis of breast masses. Experimental
results demonstrated that the proposed deep network could explain diagnosis
more accurately with various textual justifications.
",0,0,1
Robust Subspace Clustering with Compressed Data,"  Dimension reduction is widely regarded as an effective way for decreasing the
computation storage and communication loads of data-driven intelligent
systems leading to a growing demand for statistical methods that allow
analysis (e.g. clustering) of compressed data. We therefore study in this
paper a novel problem called compressive robust subspace clustering which is
to perform robust subspace clustering with the compressed data and which is
generated by projecting the original high-dimensional data onto a
lower-dimensional subspace chosen at random. Given only the compressed data and
sensing matrix the proposed method row space pursuit (RSP) recovers the
authentic row space that gives correct clustering results under certain
conditions. Extensive experiments show that RSP is distinctly better than the
competing methods in terms of both clustering accuracy and computational
efficiency.
",0,0,1
Future Person Localization in First-Person Videos,"  We present a new task that predicts future locations of people observed in
first-person videos. Consider a first-person video stream continuously recorded
by a wearable camera. Given a short clip of a person that is extracted from the
complete stream we aim to predict that person's location in future frames. To
facilitate this future person localization ability we make the following three
key observations: a) First-person videos typically involve significant
ego-motion which greatly affects the location of the target person in future
frames; b) Scales of the target person act as a salient cue to estimate a
perspective effect in first-person videos; c) First-person videos often capture
people up-close making it easier to leverage target poses (e.g. where they
look) for predicting their future locations. We incorporate these three
observations into a prediction framework with a multi-stream
convolution-deconvolution architecture. Experimental results reveal our method
to be effective on our new dataset as well as on a public social interaction
dataset.
",0,0,1
Encoding Robustness to Image Style via Adversarial Feature Perturbations,"  Adversarial training is the industry standard for producing models that are
robust to small adversarial perturbations. However machine learning
practitioners need models that are robust to other kinds of changes that occur
naturally such as changes in the style or illumination of input images. Such
changes in input distribution have been effectively modeled as shifts in the
mean and variance of deep image features. We adapt adversarial training by
directly perturbing feature statistics rather than image pixels to produce
models that are robust to various unseen distributional shifts. We explore the
relationship between these perturbations and distributional shifts by
visualizing adversarial features. Our proposed method Adversarial Batch
Normalization (AdvBN) is a single network layer that generates worst-case
feature perturbations during training. By fine-tuning neural networks on
adversarial feature distributions we observe improved robustness of networks
to various unseen distributional shifts including style variations and image
corruptions. In addition we show that our proposed adversarial feature
perturbation can be complementary to existing image space data augmentation
methods leading to improved performance. The source code and pre-trained
models are released at urlhttps://github.com/azshue/AdvBN.
",0,0,1
"A Joint Probabilistic Classification Model of Relevant and Irrelevant
  Sentences in Mathematical Word Problems","  Estimating the difficulty level of math word problems is an important task
for many educational applications. Identification of relevant and irrelevant
sentences in math word problems is an important step for calculating the
difficulty levels of such problems. This paper addresses a novel application of
text categorization to identify two types of sentences in mathematical word
problems namely relevant and irrelevant sentences. A novel joint probabilistic
classification model is proposed to estimate the joint probability of
classification decisions for all sentences of a math word problem by utilizing
the correlation among all sentences along with the correlation between the
question sentence and other sentences and sentence text. The proposed model is
compared with i) a SVM classifier which makes independent classification
decisions for individual sentences by only using the sentence text and ii) a
novel SVM classifier that considers the correlation between the question
sentence and other sentences along with the sentence text. An extensive set of
experiments demonstrates the effectiveness of the joint probabilistic
classification model for identifying relevant and irrelevant sentences as well
as the novel SVM classifier that utilizes the correlation between the question
sentence and other sentences. Furthermore empirical results and analysis show
that i) it is highly beneficial not to remove stopwords and ii) utilizing part
of speech tagging does not make a significant improvement although it has been
shown to be effective for the related task of math word problem type
classification.
",0,1,0
"On the Approximate Analysis of Energy Detection over n*Rayleigh Fading
  Channels through Cooperative Spectrum Sensing","  In this letter we consider the problem of energy detection of unknown
signals in an intervehicular communication (IVC) system over n*Rayleigh fading
channels (also known as cascaded Rayleigh). Novel tight approximations for the
probability of detection are derived for the no-diversity and the maximum ratio
combining (MRC)) diversity schemes. Moreover we investigate the system
performance when cooperative spectrum sensing (CSS) is considered with and
without imperfect reporting channels. The analytical results show that the
detection reliability is decreased as the fading severity parameter n increases
but reliability is substantially improved when CSS employs MRC schemes.
",1,0,0
Beat-Event Detection in Action Movie Franchises,"  While important advances were recently made towards temporally localizing and
recognizing specific human actions or activities in videos efficient detection
and classification of long video chunks belonging to semantically defined
categories such as ""pursuit"" or ""romance"" remains challenging.We introduce a
new dataset Action Movie Franchises consisting of a collection of Hollywood
action movie franchises. We define 11 non-exclusive semantic categories -
called beat-categories - that are broad enough to cover most of the movie
footage. The corresponding beat-events are annotated as groups of video shots
possibly overlapping.We propose an approach for localizing beat-events based on
classifying shots into beat-categories and learning the temporal constraints
between shots. We show that temporal constraints significantly improve the
classification performance. We set up an evaluation protocol for beat-event
localization as well as for shot classification depending on whether movies
from the same franchise are present or not in the training data.
",0,0,1
Object Recognition with Imperfect Perception and Redundant Description,"  This paper deals with a scene recognition system in a robotics contex. The
general problem is to match images with <I>a priori</I> descriptions. A typical
mission would consist in identifying an object in an installation with a vision
system situated at the end of a manipulator and with a human operator provided
description formulated in a pseudo-natural language and possibly redundant.
The originality of this work comes from the nature of the description from the
special attention given to the management of imprecision and uncertainty in the
interpretation process and from the way to assess the description redundancy so
as to reinforce the overall matching likelihood.
",0,0,1
Hyperspectral CNN Classification with Limited Training Samples,"  Hyperspectral imaging sensors are becoming increasingly popular in robotics
applications such as agriculture and mining and allow per-pixel thematic
classification of materials in a scene based on their unique spectral
signatures. Recently convolutional neural networks have shown remarkable
performance for classification tasks but require substantial amounts of
labelled training data. This data must sufficiently cover the variability
expected to be encountered in the environment. For hyperspectral data one of
the main variations encountered outdoors is due to incident illumination which
can change in spectral shape and intensity depending on the scene geometry. For
example regions occluded from the sun have a lower intensity and their
incident irradiance skewed towards shorter wavelengths.
  In this work a data augmentation strategy based on relighting is used during
training of a hyperspectral convolutional neural network. It allows training to
occur in the outdoor environment given only a small labelled region which does
not need to sufficiently represent the geometric variability of the entire
scene. This is important for applications where obtaining large amounts of
training data is labourious hazardous or difficult such as labelling pixels
within shadows. Radiometric normalisation approaches for pre-processing the
hyperspectral data are analysed and it is shown that methods based on the raw
pixel data are sufficient to be used as input for the classifier. This removes
the need for external hardware such as calibration boards which can restrict
the application of hyperspectral sensors in robotics applications. Experiments
to evaluate the classification system are carried out on two datasets captured
from a field-based platform.
",0,0,1
"Distributionally Robust Semi-Supervised Learning for People-Centric
  Sensing","  Semi-supervised learning is crucial for alleviating labelling burdens in
people-centric sensing. However human-generated data inherently suffer from
distribution shift in semi-supervised learning due to the diverse biological
conditions and behavior patterns of humans. To address this problem we propose
a generic distributionally robust model for semi-supervised learning on
distributionally shifted data. Considering both the discrepancy and the
consistency between the labeled data and the unlabeled data we learn the
latent features that reduce person-specific discrepancy and preserve
task-specific consistency. We evaluate our model in a variety of people-centric
recognition tasks on real-world datasets including intention recognition
activity recognition muscular movement recognition and gesture recognition.
The experiment results demonstrate that the proposed model outperforms the
state-of-the-art methods.
",0,0,1
"Clinical Predictive Keyboard using Statistical and Neural Language
  Modeling","  A language model can be used to predict the next word during authoring to
correct spelling or to accelerate writing (e.g. in sms or emails). Language
models however have only been applied in a very small scale to assist
physicians during authoring (e.g. discharge summaries or radiology reports).
But along with the assistance to the physician computer-based systems which
expedite the patient's exit also assist in decreasing the hospital infections.
We employed statistical and neural language modeling to predict the next word
of a clinical text and assess all the models in terms of accuracy and keystroke
discount in two datasets with radiology reports. We show that a neural language
model can achieve as high as 51.3% accuracy in radiology reports (one out of
two words predicted correctly). We also show that even when the models are
employed only for frequent words the physician can save valuable time.
",0,1,0
Constraint Based Refinement of Optical Flow,"  The goal of this paper is to formulate a general framework for a
constraint-based refinement of the optical flow using variational methods. We
demonstrate that for a particular choice of the constraint formulated as a
minimization problem with the quadratic regularization our results are close
to the continuity equation based fluid flow. This closeness to the continuity
model is theoretically justified through a modified augmented Lagrangian method
and validated numerically. Further along with the continuity constraint our
model can include geometric constraints as well. The correctness of our process
is studied in the Hilbert space setting. Moreover a special feature of our
system is the possibility of a diagonalization by the Cauchy-Riemann operator
and transforming it to a diffusion process on the curl and the divergence of
the flow. Using the theory of semigroups on the decoupled system we show that
our process preserves the spatial characteristics of the divergence and the
vorticities. We perform several numerical experiments and show the results on
different datasets.
",0,0,1
"Resource Cost Results for Entanglement Distillation and State Merging
  under Source Uncertainties","  We introduce one-way LOCC protocols for quantum state merging for compound
sources which have asymptotically optimal entanglement as well as classical
communication resource costs. For the arbitrarily varying quantum source (AVQS)
model we determine the one-way entanglement distillation capacity where we
utilize the robustification and elimination techniques well-known from
classical as well as quantum channel coding under assumption of arbitrarily
varying noise. Investigating quantum state merging for AVQS we demonstrate by
example that the usual robustification procedure leads to suboptimal resource
costs in this case.
",1,0,0
"User Cooperation for Enhanced Throughput Fairness in Wireless Powered
  Communication Networks","  This paper studies a novel user cooperation method in a wireless powered
cooperative communication network (WPCN) in which a pair of distributed
terminal users first harvest wireless energy broadcasted by one energy node
(EN) and then use the harvested energy to transmit information to a destination
node (DN). In particular the two cooperating users exchange their independent
information with each other so as to form a virtual antenna array and transmit
jointly to the DN. By allowing the users to share their harvested energy to
transmit each other's information the proposed method can effectively mitigate
the inherent user unfairness problem in WPCN where one user may suffer from
very low data rate due to poor energy harvesting performance and high data
transmission consumptions. Depending on the availability of channel state
information at the transmitters we consider the two users cooperating using
either coherent or non-coherent data transmissions. In both cases we derive
the maximum common throughput achieved by the cooperation schemes through
optimizing the time allocation on wireless energy transfer user message
exchange and joint information transmissions in a fixed-length time slot. We
also perform numerical analysis to study the impact of channel conditions on
the system performance. By comparing with some existing benchmark schemes our
results demonstrate the effectiveness of the proposed user cooperation in a
WPCN under different application scenarios.
",1,0,0
Learning to Compose over Tree Structures via POS Tags,"  Recursive Neural Network (RecNN) a type of models which compose words or
phrases recursively over syntactic tree structures has been proven to have
superior ability to obtain sentence representation for a variety of NLP tasks.
However RecNN is born with a thorny problem that a shared compositional
function for each node of trees can't capture the complex semantic
compositionality so that the expressive power of model is limited. In this
paper in order to address this problem we propose Tag-Guided
HyperRecNN/TreeLSTM (TG-HRecNN/TreeLSTM) which introduces hypernetwork into
RecNNs to take as inputs Part-of-Speech (POS) tags of word/phrase and generate
the semantic composition parameters dynamically. Experimental results on five
datasets for two typical NLP tasks show proposed models both obtain significant
improvement compared with RecNN and TreeLSTM consistently. Our TG-HTreeLSTM
outperforms all existing RecNN-based models and achieves or is competitive with
state-of-the-art on four sentence classification benchmarks. The effectiveness
of our models is also demonstrated by qualitative analysis.
",0,1,0
Development of POS tagger for English-Bengali Code-Mixed data,"  Code-mixed texts are widespread nowadays due to the advent of social media.
Since these texts combine two languages to formulate a sentence it gives rise
to various research problems related to Natural Language Processing. In this
paper we try to excavate one such problem namely Parts of Speech tagging of
code-mixed texts. We have built a system that can POS tag English-Bengali
code-mixed data where the Bengali words were written in Roman script. Our
approach initially involves the collection and cleaning of English-Bengali
code-mixed tweets. These tweets were used as a development dataset for building
our system. The proposed system is a modular approach that starts by tagging
individual tokens with their respective languages and then passes them to
different POS taggers designed for different languages (English and Bengali
in our case). Tags given by the two systems are later joined together and the
final result is then mapped to a universal POS tag set. Our system was checked
using 100 manually POS tagged code-mixed sentences and it returned an accuracy
of 75.29%
",0,1,0
Computational Code-Based Single-Server Private Information Retrieval,"  A new computational private information retrieval (PIR) scheme based on
random linear codes is presented. A matrix of messages from a McEliece scheme
is used to query the server with carefully chosen errors. The server responds
with the sum of the scalar multiple of the rows of the query matrix and the
files. The user recovers the desired file by erasure decoding the response.
Contrary to code-based cryptographic systems the scheme presented here enables
to use truly random codes not only codes disguised as such. Further we show
the relation to the so-called error subspace search problem and quotient error
search problem which we assume to be difficult and show that the scheme is
secure against attacks based on solving these problems.
",1,0,0
"Invariance Analysis of Saliency Models versus Human Gaze During Scene
  Free Viewing","  Most of current studies on human gaze and saliency modeling have used
high-quality stimuli. In real world however captured images undergo various
types of distortions during the whole acquisition transmission and displaying
chain. Some distortion types include motion blur lighting variations and
rotation. Despite few efforts influences of ubiquitous distortions on visual
attention and saliency models have not been systematically investigated. In
this paper we first create a large-scale database including eye movements of
10 observers over 1900 images degraded by 19 types of distortions. Second by
analyzing eye movements and saliency models we find that: a) observers look at
different locations over distorted versus original images and b) performances
of saliency models are drastically hindered over distorted images with the
maximum performance drop belonging to Rotation and Shearing distortions.
Finally we investigate the effectiveness of different distortions when serving
as data augmentation transformations. Experimental results verify that some
useful data augmentation transformations which preserve human gaze of reference
images can improve deep saliency models against distortions while some invalid
transformations which severely change human gaze will degrade the performance.
",0,0,1
Straggler Resilient Serverless Computing Based on Polar Codes,"  We propose a serverless computing mechanism for distributed computation based
on polar codes. Serverless computing is an emerging cloud based computation
model that lets users run their functions on the cloud without provisioning or
managing servers. Our proposed approach is a hybrid computing framework that
carries out computationally expensive tasks such as linear algebraic operations
involving large-scale data using serverless computing and does the rest of the
processing locally. We address the limitations and reliability issues of
serverless platforms such as straggling workers using coding theory drawing
ideas from recent literature on coded computation. The proposed mechanism uses
polar codes to ensure straggler-resilience in a computationally effective
manner. We provide extensive evidence showing polar codes outperform other
coding methods. We have designed a sequential decoder specifically for polar
codes in erasure channels with full-precision input and outputs. In addition
we have extended the proposed method to the matrix multiplication case where
both matrices being multiplied are coded. The proposed coded computation scheme
is implemented for AWS Lambda. Experiment results are presented where the
performance of the proposed coded computation technique is tested in
optimization via gradient descent. Finally we introduce the idea of partial
polarization which reduces the computational burden of encoding and decoding at
the expense of straggler-resilience.
",1,0,0
Extractive Summarization of EHR Discharge Notes,"  Patient summarization is essential for clinicians to provide coordinated care
and practice effective communication. Automated summarization has the potential
to save time standardize notes aid clinical decision making and reduce
medical errors. Here we provide an upper bound on extractive summarization of
discharge notes and develop an LSTM model to sequentially label topics of
history of present illness notes. We achieve an F1 score of 0.876 which
indicates that this model can be employed to create a dataset for evaluation of
extractive summarization methods.
",0,1,0
Supervised Learning with Projected Entangled Pair States,"  Tensor networks a model that originated from quantum physics has been
gradually generalized as efficient models in machine learning in recent years.
However in order to achieve exact contraction only tree-like tensor networks
such as the matrix product states and tree tensor networks have been
considered even for modeling two-dimensional data such as images. In this
work we construct supervised learning models for images using the projected
entangled pair states (PEPS) a two-dimensional tensor network having a similar
structure prior to natural images. Our approach first performs a feature map
which transforms the image data to a product state on a grid then contracts
the product state to a PEPS with trainable parameters to predict image labels.
The tensor elements of PEPS are trained by minimizing differences between
training labels and predicted labels. The proposed model is evaluated on image
classifications using the MNIST and the Fashion-MNIST datasets. We show that
our model is significantly superior to existing models using tree-like tensor
networks. Moreover using the same input features our method performs as well
as the multilayer perceptron classifier but with much fewer parameters and is
more stable. Our results shed light on potential applications of
two-dimensional tensor network models in machine learning.
",0,0,1
Some bounds on the size of codes,"  We present some upper bounds on the size of non-linear codes and their
restriction to systematic codes and linear codes. These bounds are independent
of other known theoretical bounds e.g. the Griesmer bound the Johnson bound
or the Plotkin bound and one of these is actually an improvement of a bound by
Litsyn and Laihonen. Our experiments show that in some cases (the majority of
cases for some q) our bounds provide the best value compared to all other
theoretical bounds.
",1,0,0
Improving the Performance of OTDOA based Positioning in NB-IoT Systems,"  In this paper we consider positioning with
observed-time-difference-of-arrival (OTDOA) for a device deployed in
long-term-evolution (LTE) based narrow-band Internet-of-things (NB-IoT)
systems. We propose an iterative expectation-maximization based successive
interference cancellation (EM-SIC) algorithm to jointly consider estimations of
residual frequency-offset (FO) fading-channel taps and time-of-arrival (ToA)
of the first arrival-path for each of the detected cells. In order to design a
low complexity ToA detector and also due to the limits of low-cost analog
circuits we assume an NB-IoT device working at a low-sampling rate such as
1.92 MHz or lower. The proposed EM-SIC algorithm comprises two stages to detect
ToA based on which OTDOA can be calculated. In a first stage after running
the EM-SIC block a predefined number of iterations a coarse ToA is estimated
for each of the detected cells. Then in a second stage to improve the ToA
resolution a low-pass filter is utilized to interpolate the correlations of
time-domain PRS signal evaluated at a low sampling-rate to a high sampling-rate
such as 30.72 MHz. To keep low-complexity only the correlations inside a small
search window centered at the coarse ToA estimates are upsampled. Then the
refined ToAs are estimated based on upsampled correlations. If at least three
cells are detected with OTDOA and the locations of detected cell sites the
position of the NB-IoT device can be estimated. We show through numerical
simulations that the proposed EM-SIC based ToA detector is robust against
impairments introduced by inter-cell interference fading-channel and residual
FO. Thus significant signal-to-noise (SNR) gains are obtained over traditional
ToA detectors that do not consider these impairments when positioning a device.
",1,0,0
Unique Information via Dependency Constraints,"  The partial information decomposition (PID) is perhaps the leading proposal
for resolving information shared between a set of sources and a target into
redundant synergistic and unique constituents. Unfortunately the PID
framework has been hindered by a lack of a generally agreed-upon multivariate
method of quantifying the constituents. Here we take a step toward rectifying
this by developing a decomposition based on a new method that quantifies unique
information. We first develop a broadly applicable method---the dependency
decomposition---that delineates how statistical dependencies influence the
structure of a joint distribution. The dependency decomposition then allows us
to define a measure of the information about a target that can be uniquely
attributed to a particular source as the least amount which the source-target
statistical dependency can influence the information shared between the sources
and the target. The result is the first measure that satisfies the core axioms
of the PID framework while not satisfying the Blackwell relation which depends
on a particular interpretation of how the variables are related. This makes a
key step forward to a practical PID.
",1,0,0
NUVA: A Naming Utterance Verifier for Aphasia Treatment,"  Anomia (word-finding difficulties) is the hallmark of aphasia an acquired
language disorder most commonly caused by stroke. Assessment of speech
performance using picture naming tasks is a key method for both diagnosis and
monitoring of responses to treatment interventions by people with aphasia
(PWA). Currently this assessment is conducted manually by speech and language
therapists (SLT). Surprisingly despite advancements in automatic speech
recognition (ASR) and artificial intelligence with technologies like deep
learning research on developing automated systems for this task has been
scarce. Here we present NUVA an utterance verification system incorporating a
deep learning element that classifies 'correct' versus' incorrect' naming
attempts from aphasic stroke patients. When tested on eight native
British-English speaking PWA the system's performance accuracy ranged between
83.6% to 93.6% with a 10-fold cross-validation mean of 89.5%. This performance
was not only significantly better than a baseline created for this study using
one of the leading commercially available ASRs (Google speech-to-text service)
but also comparable in some instances with two independent SLT ratings for the
same dataset.
",0,1,0
Audio-Visual Scene-Aware Dialog,"  We introduce the task of scene-aware dialog. Our goal is to generate a
complete and natural response to a question about a scene given video and
audio of the scene and the history of previous turns in the dialog. To answer
successfully agents must ground concepts from the question in the video while
leveraging contextual cues from the dialog history. To benchmark this task we
introduce the Audio Visual Scene-Aware Dialog (AVSD) Dataset. For each of more
than 11000 videos of human actions from the Charades dataset our dataset
contains a dialog about the video plus a final summary of the video by one of
the dialog participants. We train several baseline systems for this task and
evaluate the performance of the trained models using both qualitative and
quantitative metrics. Our results indicate that models must utilize all the
available inputs (video audio question and dialog history) to perform best
on this dataset.
",0,0,1
"Optimization of Unequal Error Protection Rateless Codes for Multimedia
  Multicasting","  Rateless codes have been shown to be able to provide greater flexibility and
efficiency than fixed-rate codes for multicast applications. In the following
we optimize rateless codes for unequal error protection (UEP) for multimedia
multicasting to a set of heterogeneous users. The proposed designs have the
objectives of providing either guaranteed or best-effort quality of service
(QoS). A randomly interleaved rateless encoder is proposed whereby users only
need to decode symbols up to their own QoS level. The proposed coder is
optimized based on measured transmission properties of standardized raptor
codes over wireless channels. It is shown that a guaranteed QoS problem
formulation can be transformed into a convex optimization problem yielding a
globally optimal solution. Numerical results demonstrate that the proposed
optimized random interleaved UEP rateless coder's performance compares
favorably with that of other recently proposed UEP rateless codes.
",1,0,0
Relaxed-Responsibility Hierarchical Discrete VAEs,"  Successfully training Variational Autoencoders (VAEs) with a hierarchy of
discrete latent variables remains an area of active research.
  Vector-Quantised VAEs are a powerful approach to discrete VAEs but naive
hierarchical extensions can be unstable when training. Leveraging insights from
classical methods of inference we introduce textitRelaxed-Responsibility
Vector-Quantisation a novel way to parameterise discrete latent variables a
refinement of relaxed Vector-Quantisation that gives better performance and
more stable training. This enables a novel approach to hierarchical discrete
variational autoencoders with numerous layers of latent variables (here up to
32) that we train end-to-end. Within hierarchical probabilistic deep generative
models with discrete latent variables trained end-to-end we achieve
state-of-the-art bits-per-dim results for various standard datasets. % Unlike
discrete VAEs with a single layer of latent variables we can produce samples
by ancestral sampling: it is not essential to train a second autoregressive
generative model over the learnt latent representations to then sample from and
then decode. % Moreover that latter approach in these deep hierarchical models
would require thousands of forward passes to generate a single sample. Further
we observe different layers of our model become associated with different
aspects of the data.
",0,0,1
"Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a
  Changing World","  In this paper we focus on online representation learning in non-stationary
environments which may require continuous adaptation of model architecture. We
propose a novel online dictionary-learning (sparse-coding) framework which
incorporates the addition and deletion of hidden units (dictionary elements)
and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of
the hippocampus known to be associated with improved cognitive function and
adaptation to new environments. In the online learning setting where new input
instances arrive sequentially in batches the neuronal-birth is implemented by
adding new units with random initial weights (random dictionary elements); the
number of new units is determined by the current performance (representation
error) of the dictionary higher error causing an increase in the birth rate.
Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity)
on the dictionary within the block-coordinate descent optimization at each
iteration of our online alternating minimization scheme which iterates between
the code and dictionary updates. Finally hidden unit connectivity adaptation
is facilitated by introducing sparsity in dictionary elements. Our empirical
evaluation on several real-life datasets (images and language) as well as on
synthetic data demonstrates that the proposed approach can considerably
outperform the state-of-art fixed-size (nonadaptive) online sparse coding of
Mairal et al. (2009) in the presence of nonstationary data. Moreover we
identify certain properties of the data (e.g. sparse inputs with nearly
non-overlapping supports) and of the model (e.g. dictionary sparsity)
associated with such improvements.
",0,0,1
Relation-Aware Global Attention for Person Re-identification,"  For person re-identification (re-id) attention mechanisms have become
attractive as they aim at strengthening discriminative features and suppressing
irrelevant ones which matches well the key of re-id i.e. discriminative
feature learning. Previous approaches typically learn attention using local
convolutions ignoring the mining of knowledge from global structure patterns.
Intuitively the affinities among spatial positions/nodes in the feature map
provide clustering-like information and are helpful for inferring semantics and
thus attention especially for person images where the feasible human poses are
constrained. In this work we propose an effective Relation-Aware Global
Attention (RGA) module which captures the global structural information for
better attention learning. Specifically for each feature position in order to
compactly grasp the structural information of global scope and local appearance
information we propose to stack the relations i.e. its pairwise
correlations/affinities with all the feature positions (e.g. in raster scan
order) and the feature itself together to learn the attention with a shallow
convolutional model. Extensive ablation studies demonstrate that our RGA can
significantly enhance the feature representation power and help achieve the
state-of-the-art performance on several popular benchmarks. The source code is
available at
https://github.com/microsoft/Relation-Aware-Global-Attention-Networks.
",0,0,1
"Intelligent Parameter Tuning in Optimization-based Iterative CT
  Reconstruction via Deep Reinforcement Learning","  A number of image-processing problems can be formulated as optimization
problems. The objective function typically contains several terms specifically
designed for different purposes. Parameters in front of these terms are used to
control the relative weights among them. It is of critical importance to tune
these parameters as quality of the solution depends on their values. Tuning
parameter is a relatively straightforward task for a human as one can
intelligently determine the direction of parameter adjustment based on the
solution quality. Yet manual parameter tuning is not only tedious in many
cases but becomes impractical when a number of parameters exist in a problem.
Aiming at solving this problem this paper proposes an approach that employs
deep reinforcement learning to train a system that can automatically adjust
parameters in a human-like manner. We demonstrate our idea in an example
problem of optimization-based iterative CT reconstruction with a pixel-wise
total-variation regularization term. We set up a parameter tuning policy
network (PTPN) which maps an CT image patch to an output that specifies the
direction and amplitude by which the parameter at the patch center is adjusted.
We train the PTPN via an end-to-end reinforcement learning procedure. We
demonstrate that under the guidance of the trained PTPN for parameter tuning at
each pixel reconstructed CT images attain quality similar or better than in
those reconstructed with manually tuned parameters.
",0,0,1
Continuous learning of face attribute synthesis,"  The generative adversarial network (GAN) exhibits great superiority in the
face attribute synthesis task. However existing methods have very limited
effects on the expansion of new attributes. To overcome the limitations of a
single network in new attribute synthesis a continuous learning method for
face attribute synthesis is proposed in this work. First the feature vector of
the input image is extracted and attribute direction regression is performed in
the feature space to obtain the axes of different attributes. The feature
vector is then linearly guided along the axis so that images with target
attributes can be synthesized by the decoder. Finally to make the network
capable of continuous learning the orthogonal direction modification module is
used to extend the newly-added attributes. Experimental results show that the
proposed method can endow a single network with the ability to learn attributes
continuously and as compared to those produced by the current
state-of-the-art methods the synthetic attributes have higher accuracy.
",0,0,1
"Efficient piecewise training of deep structured models for semantic
  segmentation","  Recent advances in semantic image segmentation have mostly been achieved by
training deep convolutional neural networks (CNNs). We show how to improve
semantic segmentation through the use of contextual information; specifically
we explore `patch-patch' context between image regions and `patch-background'
context. For learning from the patch-patch context we formulate Conditional
Random Fields (CRFs) with CNN-based pairwise potential functions to capture
semantic correlations between neighboring patches. Efficient piecewise training
of the proposed deep structured model is then applied to avoid repeated
expensive CRF inference for back propagation. For capturing the
patch-background context we show that a network design with traditional
multi-scale image input and sliding pyramid pooling is effective for improving
performance. Our experimental results set new state-of-the-art performance on a
number of popular semantic segmentation datasets including NYUDv2 PASCAL VOC
2012 PASCAL-Context and SIFT-flow. In particular we achieve an
intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012
dataset.
",0,0,1
ner and pos when nothing is capitalized,"  For those languages which use it capitalization is an important signal for
the fundamental NLP tasks of Named Entity Recognition (NER) and Part of Speech
(POS) tagging. In fact it is such a strong signal that model performance on
these tasks drops sharply in common lowercased scenarios such as noisy web
text or machine translation outputs. In this work we perform a systematic
analysis of solutions to this problem modifying only the casing of the train
or test data using lowercasing and truecasing methods. While prior work and
first impressions might suggest training a caseless model or using a truecaser
at test time we show that the most effective strategy is a concatenation of
cased and lowercased training data producing a single model with high
performance on both cased and uncased text. As shown in our experiments this
result holds across tasks and input representations. Finally we show that our
proposed solution gives an 8% F1 improvement in mention detection on noisy
out-of-domain Twitter data.
",0,1,0
LIBRE: The Multiple 3D LiDAR Dataset,"  In this work we present LIBRE: LiDAR Benchmarking and Reference a
first-of-its-kind dataset featuring 10 different LiDAR sensors covering a
range of manufacturers models and laser configurations. Data captured
independently from each sensor includes three different environments and
configurations: static targets where objects were placed at known distances
and measured from a fixed position within a controlled environment; adverse
weather where static obstacles were measured from a moving vehicle captured
in a weather chamber where LiDARs were exposed to different conditions (fog
rain strong light); and finally dynamic traffic where dynamic objects were
captured from a vehicle driven on public urban roads multiple times at
different times of the day and including supporting sensors such as cameras
infrared imaging and odometry devices. LIBRE will contribute to the research
community to (1) provide a means for a fair comparison of currently available
LiDARs and (2) facilitate the improvement of existing self-driving vehicles
and robotics-related software in terms of development and tuning of
LiDAR-based perception algorithms.
",0,0,1
"Time-Contrastive Learning Based Deep Bottleneck Features for
  Text-Dependent Speaker Verification","  There are a number of studies about extraction of bottleneck (BN) features
from deep neural networks (DNNs)trained to discriminate speakers pass-phrases
and triphone states for improving the performance of text-dependent speaker
verification (TD-SV). However a moderate success has been achieved. A recent
study [1] presented a time contrastive learning (TCL) concept to explore the
non-stationarity of brain signals for classification of brain states. Speech
signals have similar non-stationarity property and TCL further has the
advantage of having no need for labeled data. We therefore present a TCL based
BN feature extraction method. The method uniformly partitions each speech
utterance in a training dataset into a predefined number of multi-frame
segments. Each segment in an utterance corresponds to one class and class
labels are shared across utterances. DNNs are then trained to discriminate all
speech frames among the classes to exploit the temporal structure of speech. In
addition we propose a segment-based unsupervised clustering algorithm to
re-assign class labels to the segments. TD-SV experiments were conducted on the
RedDots challenge database. The TCL-DNNs were trained using speech data of
fixed pass-phrases that were excluded from the TD-SV evaluation set so the
learned features can be considered phrase-independent. We compare the
performance of the proposed TCL bottleneck (BN) feature with those of
short-time cepstral features and BN features extracted from DNNs discriminating
speakers pass-phrases speaker+pass-phrase as well as monophones whose labels
and boundaries are generated by three different automatic speech recognition
(ASR) systems. Experimental results show that the proposed TCL-BN outperforms
cepstral features and speaker+pass-phrase discriminant BN features and its
performance is on par with those of ASR derived BN features. Moreover....
",0,1,0
"Beamforming Design for Joint Localization and Data Transmission in
  Distributed Antenna System","  A distributed antenna system is studied whose goal is to provide data
communication and positioning functionalities to Mobile Stations (MSs). Each MS
receives data from a number of Base Stations (BSs) and uses the received
signal not only to extract the information but also to determine its location.
This is done based on Time of Arrival (TOA) or Time Difference of Arrival
(TDOA) measurements depending on the assumed synchronization conditions. The
problem of minimizing the overall power expenditure of the BSs under data
throughput and localization accuracy requirements is formulated with respect to
the beamforming vectors used at the BSs. The analysis covers both
frequency-flat and frequency-selective channels and accounts also for
robustness constraints in the presence of parameter uncertainty. The proposed
algorithmic solutions are based on rank-relaxation and Difference-of-Convex
(DC) programming.
",1,0,0
Minor Privacy Protection Through Real-time Video Processing at the Edge,"  The collection of a lot of personal information about individuals including
the minor members of a family by closed-circuit television (CCTV) cameras
creates a lot of privacy concerns. Particularly revealing children's
identifications or activities may compromise their well-being. In this paper
we investigate lightweight solutions that are affordable to edge surveillance
systems which is made feasible and accurate to identify minors such that
appropriate privacy-preserving measures can be applied accordingly. State of
the art deep learning architectures are modified and re-purposed in a cascaded
fashion to maximize the accuracy of our model. A pipeline extracts faces from
the input frames and classifies each one to be of an adult or a child. Over
20000 labeled sample points are used for classification. We explore the timing
and resources needed for such a model to be used in the Edge-Fog architecture
at the edge of the network where we can achieve near real-time performance on
the CPU. Quantitative experimental results show the superiority of our proposed
model with an accuracy of 92.1% in classification compared to some other face
recognition based child detection approaches.
",0,0,1
Relay vs User Cooperation in Time-Duplexed Multiaccess Networks,"  The performance of user-cooperation in a multi-access network is compared to
that of using a wireless relay. Using the total transmit and processing power
consumed at all nodes as a cost metric the outage probabilities achieved by
dynamic decode-and-forward (DDF) and amplify-and-forward (AF) are compared for
the two networks. A geometry-inclusive high signal-to-noise ratio (SNR) outage
analysis in conjunction with area-averaged numerical simulations shows that
user and relay cooperation achieve a maximum diversity of K and 2 respectively
for a K-user multiaccess network under both DDF and AF. However when
accounting for energy costs of processing and communication relay cooperation
can be more energy efficient than user cooperation i.e. relay cooperation
achieves coding (SNR) gains particularly in the low SNR regime that override
the diversity advantage of user cooperation.
",1,0,0
"On Robust Face Recognition via Sparse Encoding: the Good the Bad and
  the Ugly","  In the field of face recognition Sparse Representation (SR) has received
considerable attention during the past few years. Most of the relevant
literature focuses on holistic descriptors in closed-set identification
applications. The underlying assumption in SR-based methods is that each class
in the gallery has sufficient samples and the query lies on the subspace
spanned by the gallery of the same class. Unfortunately such assumption is
easily violated in the more challenging face verification scenario where an
algorithm is required to determine if two faces (where one or both have not
been seen before) belong to the same person. In this paper we first discuss
why previous attempts with SR might not be applicable to verification problems.
We then propose an alternative approach to face verification via SR.
Specifically we propose to use explicit SR encoding on local image patches
rather than the entire face. The obtained sparse signals are pooled via
averaging to form multiple region descriptors which are then concatenated to
form an overall face descriptor. Due to the deliberate loss spatial relations
within each region (caused by averaging) the resulting descriptor is robust to
misalignment & various image deformations. Within the proposed framework we
evaluate several SR encoding techniques: l1-minimisation Sparse Autoencoder
Neural Network (SANN) and an implicit probabilistic technique based on
Gaussian Mixture Models. Thorough experiments on AR FERET exYaleB BANCA and
ChokePoint datasets show that the proposed local SR approach obtains
considerably better and more robust performance than several previous
state-of-the-art holistic SR methods in both verification and closed-set
identification problems. The experiments also show that l1-minimisation based
encoding has a considerably higher computational than the other techniques but
leads to higher recognition rates.
",0,0,1
VideoClick: Video Object Segmentation with a Single Click,"  Annotating videos with object segmentation masks typically involves a two
stage procedure of drawing polygons per object instance for all the frames and
then linking them through time. While simple this is a very tedious time
consuming and expensive process making the creation of accurate annotations at
scale only possible for well-funded labs. What if we were able to segment an
object in the full video with only a single click? This will enable video
segmentation at scale with a very low budget opening the door to many
applications. Towards this goal in this paper we propose a bottom up approach
where given a single click for each object in a video we obtain the
segmentation masks of these objects in the full video. In particular we
construct a correlation volume that assigns each pixel in a target frame to
either one of the objects in the reference frame or the background. We then
refine this correlation volume via a recurrent attention module and decode the
final segmentation. To evaluate the performance we label the popular and
challenging Cityscapes dataset with video object segmentations. Results on this
new CityscapesVideo dataset show that our approach outperforms all the
baselines in this challenging setting.
",0,0,1
"Spatially Aware Dictionary Learning and Coding for Fossil Pollen
  Identification","  We propose a robust approach for performing automatic species-level
recognition of fossil pollen grains in microscopy images that exploits both
global shape and local texture characteristics in a patch-based matching
methodology. We introduce a novel criteria for selecting meaningful and
discriminative exemplar patches. We optimize this function during training
using a greedy submodular function optimization framework that gives a
near-optimal solution with bounded approximation error. We use these selected
exemplars as a dictionary basis and propose a spatially-aware sparse coding
method to match testing images for identification while maintaining global
shape correspondence. To accelerate the coding process for fast matching we
introduce a relaxed form that uses spatially-aware soft-thresholding during
coding. Finally we carry out an experimental study that demonstrates the
effectiveness and efficiency of our exemplar selection and classification
mechanisms achieving $86.13%$ accuracy on a difficult fine-grained species
classification task distinguishing three types of fossil spruce pollen.
",0,0,1
Self Super-Resolution for Magnetic Resonance Images using Deep Networks,"  High resolution magnetic resonance~(MR) imaging~(MRI) is desirable in many
clinical applications however there is a trade-off between resolution speed
of acquisition and noise. It is common for MR images to have worse
through-plane resolution~(slice thickness) than in-plane resolution. In these
MRI images high frequency information in the through-plane direction is not
acquired and cannot be resolved through interpolation. To address this issue
super-resolution methods have been developed to enhance spatial resolution. As
an ill-posed problem state-of-the-art super-resolution methods rely on the
presence of external/training atlases to learn the transform from low
resolution~(LR) images to high resolution~(HR) images. For several reasons
such HR atlas images are often not available for MRI sequences. This paper
presents a self super-resolution~(SSR) algorithm which does not use any
external atlas images yet can still resolve HR images only reliant on the
acquired LR image. We use a blurred version of the input image to create
training data for a state-of-the-art super-resolution deep network. The trained
network is applied to the original input image to estimate the HR image. Our
SSR result shows a significant improvement on through-plane resolution compared
to competing SSR methods.
",0,0,1
"Automatic Curation of Golf Highlights using Multimodal Excitement
  Features","  The production of sports highlight packages summarizing a game's most
exciting moments is an essential task for broadcast media. Yet it requires
labor-intensive video editing. We propose a novel approach for auto-curating
sports highlights and use it to create a real-world system for the editorial
aid of golf highlight reels. Our method fuses information from the players'
reactions (action recognition such as high-fives and fist pumps) spectators
(crowd cheering) and commentator (tone of the voice and word analysis) to
determine the most interesting moments of a game. We accurately identify the
start and end frames of key shot highlights with additional metadata such as
the player's name and the hole number allowing personalized content
summarization and retrieval. In addition we introduce new techniques for
learning our classifiers with reduced manual training data annotation by
exploiting the correlation of different modalities. Our work has been
demonstrated at a major golf tournament successfully extracting highlights
from live video streams over four consecutive days.
",0,0,1
Implicit Geometric Regularization for Learning Shapes,"  Representing shapes as level sets of neural networks has been recently proved
to be useful for different shape analysis and reconstruction tasks. So far
such representations were computed using either: (i) pre-computed implicit
shape representations; or (ii) loss functions explicitly defined over the
neural level sets. In this paper we offer a new paradigm for computing high
fidelity implicit neural representations directly from raw data (i.e. point
clouds with or without normal information). We observe that a rather simple
loss function encouraging the neural network to vanish on the input point
cloud and to have a unit norm gradient possesses an implicit geometric
regularization property that favors smooth and natural zero level set surfaces
avoiding bad zero-loss solutions. We provide a theoretical analysis of this
property for the linear case and show that in practice our method leads to
state of the art implicit neural representations with higher level-of-details
and fidelity compared to previous methods.
",0,0,1
"Computer Vision with Deep Learning for Plant Phenotyping in Agriculture:
  A Survey","  In light of growing challenges in agriculture with ever growing food demand
across the world efficient crop management techniques are necessary to
increase crop yield. Precision agriculture techniques allow the stakeholders to
make effective and customized crop management decisions based on data gathered
from monitoring crop environments. Plant phenotyping techniques play a major
role in accurate crop monitoring. Advancements in deep learning have made
previously difficult phenotyping tasks possible. This survey aims to introduce
the reader to the state of the art research in deep plant phenotyping.
",0,0,1
"Leveraging Virtual and Real Person for Unsupervised Person
  Re-identification","  Person re-identification (re-ID) is a challenging problem especially when no
labels are available for training. Although recent deep re-ID methods have
achieved great improvement it is still difficult to optimize deep re-ID model
without annotations in training data. To address this problem this study
introduces a novel approach for unsupervised person re-ID by leveraging virtual
and real data. Our approach includes two components: virtual person generation
and training of deep re-ID model. For virtual person generation we learn a
person generation model and a camera style transfer model using unlabeled real
data to generate virtual persons with different poses and camera styles. The
virtual data is formed as labeled training data enabling subsequently training
deep re-ID model in supervision. For training of deep re-ID model we divide it
into three steps: 1) pre-training a coarse re-ID model by using virtual data;
2) collaborative filtering based positive pair mining from the real data; and
3) fine-tuning of the coarse re-ID model by leveraging the mined positive pairs
and virtual data. The final re-ID model is achieved by iterating between step 2
and step 3 until convergence. Experimental results on two large-scale datasets
Market-1501 and DukeMTMC-reID demonstrate the effectiveness of our approach
and shows that the state of the art is achieved in unsupervised person re-ID.
",0,0,1
"Preliminary Report on the Structure of Croatian Linguistic Co-occurrence
  Networks","  In this article we investigate the structure of Croatian linguistic
co-occurrence networks. We examine the change of network structure properties
by systematically varying the co-occurrence window sizes the corpus sizes and
removing stopwords. In a co-occurrence window of size $n$ we establish a link
between the current word and $n-1$ subsequent words. The results point out that
the increase of the co-occurrence window size is followed by a decrease in
diameter average path shortening and expectedly condensing the average
clustering coefficient. The same can be noticed for the removal of the
stopwords. Finally since the size of texts is reflected in the network
properties our results suggest that the corpus influence can be reduced by
increasing the co-occurrence window size.
",0,1,0
Robust Learning Through Cross-Task Consistency,"  Visual perception entails solving a wide set of tasks e.g. object
detection depth estimation etc. The predictions made for multiple tasks from
the same image are not independent and therefore are expected to be
consistent. We propose a broadly applicable and fully computational method for
augmenting learning with Cross-Task Consistency. The proposed formulation is
based on inference-path invariance over a graph of arbitrary tasks. We observe
that learning with cross-task consistency leads to more accurate predictions
and better generalization to out-of-distribution inputs. This framework also
leads to an informative unsupervised quantity called Consistency Energy based
on measuring the intrinsic consistency of the system. Consistency Energy
correlates well with the supervised error (r=0.67) thus it can be employed as
an unsupervised confidence metric as well as for detection of
out-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on
multiple datasets including Taskonomy Replica CocoDoom and ApolloScape and
they benchmark cross-task consistency versus various baselines including
conventional multi-task learning cycle consistency and analytical
consistency.
",0,0,1
"Low-Complexity Channel Reconstruction Methods Based on SVD-ZF Precoding
  in Massive 3D-MIMO Systems","  In this paper we study the low-complexity channel reconstruction methods for
downlink precoding in massive multiple-Input multiple-Output (MIMO) systems.
When the user is allocated less streams than the number of its antennas the
base station (BS) or user usually utilizes the singular value decomposition
(SVD) to get the effective channels whose dimension is equal to the number of
streams. This process is called channel reconstruction and done in BS for time
division duplex (TDD) mode. However with the increasing of antennas in BS the
computation burden of SVD is getting incredible. Here we propose a series of
novel low-complexity channel reconstruction methods for downlink precoding in
3D spatial channel model. We consider different correlations between elevation
and azimuth antennas and divide the large dimensional matrix SVD into two
kinds of small-size matrix SVD. The simulation results show that the proposed
methods only produce less than 10% float computation than the traditional SVD
zero-forcing (SVD-ZF) precoding method while keeping nearly the same
performance of 1Gbps.
",1,0,0
"Uncertainty Estimation for End-To-End Learned Dense Stereo Matching via
  Probabilistic Deep Learning","  Motivated by the need to identify erroneous disparity assignments various
approaches for uncertainty and confidence estimation of dense stereo matching
have been presented in recent years. As in many other fields especially deep
learning based methods have shown convincing results. However most of these
methods only model the uncertainty contained in the data while ignoring the
uncertainty of the employed dense stereo matching procedure. Additionally
modelling the latter however is particularly beneficial if the domain of the
training data varies from that of the data to be processed. For this purpose
in the present work the idea of probabilistic deep learning is applied to the
task of dense stereo matching for the first time. Based on the well-known and
commonly employed GC-Net architecture a novel probabilistic neural network is
presented for the task of joint depth and uncertainty estimation from epipolar
rectified stereo image pairs. Instead of learning the network parameters
directly the proposed probabilistic neural network learns a probability
distribution from which parameters are sampled for every prediction. The
variations between multiple such predictions on the same image pair allow to
approximate the model uncertainty. The quality of the estimated depth and
uncertainty information is assessed in an extensive evaluation on three
different datasets.
",0,0,1
ALEX: Active Learning based Enhancement of a Model's Explainability,"  An active learning (AL) algorithm seeks to construct an effective classifier
with a minimal number of labeled examples in a bootstrapping manner. While
standard AL heuristics such as selecting those points for annotation for which
a classification model yields least confident predictions there has been no
empirical investigation to see if these heuristics lead to models that are more
interpretable to humans. In the era of data-driven learning this is an
important research direction to pursue. This paper describes our
work-in-progress towards developing an AL selection function that in addition
to model effectiveness also seeks to improve on the interpretability of a model
during the bootstrapping steps. Concretely speaking our proposed selection
function trains an `explainer' model in addition to the classifier model and
favours those instances where a different part of the data is used on an
average to explain the predicted class. Initial experiments exhibited
encouraging trends in showing that such a heuristic can lead to developing more
effective and more explainable end-to-end data-driven classifiers.
",0,0,1
Going Deeper in Spiking Neural Networks: VGG and Residual Architectures,"  Over the past few years Spiking Neural Networks (SNNs) have become popular
as a possible pathway to enable low-power event-driven neuromorphic hardware.
However their application in machine learning have largely been limited to
very shallow neural network architectures for simple problems. In this paper
we propose a novel algorithmic technique for generating an SNN with a deep
architecture and demonstrate its effectiveness on complex visual recognition
problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and
Residual network architectures with significantly better accuracy than the
state-of-the-art. Finally we present analysis of the sparse event-driven
computations to demonstrate reduced hardware overhead when operating in the
spiking domain.
",0,0,1
"Shadow Removal by a Lightness-Guided Network with Training on Unpaired
  Data","  Shadow removal can significantly improve the image visual quality and has
many applications in computer vision. Deep learning methods based on CNNs have
become the most effective approach for shadow removal by training on either
paired data where both the shadow and underlying shadow-free versions of an
image are known or unpaired data where shadow and shadow-free training images
are totally different with no correspondence. In practice CNN training on
unpaired data is more preferred given the easiness of training data collection.
In this paper we present a new Lightness-Guided Shadow Removal Network
(LG-ShadowNet) for shadow removal by training on unpaired data. In this method
we first train a CNN module to compensate for the lightness and then train a
second CNN module with the guidance of lightness information from the first CNN
module for final shadow removal. We also introduce a loss function to further
utilise the colour prior of existing data. Extensive experiments on widely used
ISTD adjusted ISTD and USR datasets demonstrate that the proposed method
outperforms the state-of-the-art methods with training on unpaired data.
",0,0,1
"A Combinatorial Methodology for Optimizing Non-Binary Graph-Based Codes:
  Theoretical Analysis and Applications in Data Storage","  Non-binary (NB) low-density parity-check (LDPC) codes are graph-based codes
that are increasingly being considered as a powerful error correction tool for
modern dense storage devices. The increasing levels of asymmetry incorporated
by the channels underlying modern dense storage systems exacerbates the error
floor problem. In a recent research the weight consistency matrix (WCM)
framework was introduced as an effective NB-LDPC code optimization methodology
that is suitable for modern Flash memory and magnetic recording (MR) systems.
In this paper we provide the in-depth theoretical analysis needed to
understand and properly apply the WCM framework. We focus on general absorbing
sets of type two (GASTs). In particular we introduce a novel tree
representation of a GAST called the unlabeled GAST tree using which we prove
that the WCM framework is optimal. Then we enumerate the WCMs. We demonstrate
the significance of the savings achieved by the WCM framework in the number of
matrices processed to remove a GAST. Moreover we provide a linear-algebraic
analysis of the null spaces of WCMs associated with a GAST. We derive the
minimum number of edge weight changes needed to remove a GAST via its WCMs
along with how to choose these changes. Additionally we propose a new set of
problematic objects namely the oscillating sets of type two (OSTs) which
contribute to the error floor of NB-LDPC codes with even column weights on
asymmetric channels and we show how to customize the WCM framework to remove
OSTs. We also extend the domain of the WCM framework applications by
demonstrating its benefits in optimizing column weight 5 codes codes used over
Flash channels with soft information and spatially-coupled codes. The
performance gains achieved via the WCM framework range between 1 and nearly 2.5
orders of magnitude in the error floor region over interesting channels.
",1,0,0
Towards real-world navigation with deep differentiable planners,"  We train embodied neural networks to plan and navigate unseen complex 3D
environments emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning we focus on differentiable planners such as Value
Iteration Networks (VIN) which are trained offline from safe expert
demonstrations. Although they work well in small simulations we address two
major limitations that hinder their deployment. First we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration which explicitly learns to model any
impossible actions. Secondly we extend the model to work with a limited
perspective camera under translation and rotation which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments succeeding in settings that are otherwise challenging
for this class of methods. As far as we know we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset
consisting of real images captured from a robot.
",0,0,1
"Hybrid Learning of Optical Flow and Next Frame Prediction to Boost
  Optical Flow in the Wild","  CNN-based optical flow estimation has attracted attention recently mainly
due to its impressively high frame rates. These networks perform well on
synthetic datasets but they are still far behind the classical methods in
real-world videos. This is because there is no ground truth optical flow for
training these networks on real data. In this paper we boost CNN-based optical
flow estimation in real scenes with the help of the freely available
self-supervised task of next-frame prediction. To this end we train the
network in a hybrid way providing it with a mixture of synthetic and real
videos. With the help of a sample-variant multi-tasking architecture the
network is trained on different tasks depending on the availability of
ground-truth. We also experiment with the prediction of ""next-flow"" instead of
estimation of the current flow which is intuitively closer to the task of
next-frame prediction and yields favorable results. We demonstrate the
improvement in optical flow estimation on the real-world KITTI benchmark.
Additionally we test the optical flow indirectly in an action classification
scenario. As a side product of this work we report significant improvements
over state-of-the-art in the task of next-frame prediction.
",0,0,1
Universal Lattice Codes for MIMO Channels,"  We propose a coding scheme that achieves the capacity of the compound MIMO
channel with algebraic lattices. Our lattice construction exploits the
multiplicative structure of number fields and their group of units to absorb
ill-conditioned channel realizations. To shape the constellation a discrete
Gaussian distribution over the lattice points is applied. These techniques
along with algebraic properties of the proposed lattices are then used to
construct a sub-optimal de-coupled coding schemes that achieves a gap to
compound capacity by decoding in a lattice that does not depend of the channel
realization. The gap is characterized in terms of algebraic invariants of the
codes and shown to be significantly smaller than previous schemes in the
literature. We also exhibit alternative algebraic constructions that achieve
the capacity of ergodic fading channels.
",1,0,0
"A level set representation method for N-dimensional convex shape and
  applications","  In this work we present a new efficient method for convex shape
representation which is regardless of the dimension of the concerned objects
using level-set approaches. Convexity prior is very useful for object
completion in computer vision. It is a very challenging task to design an
efficient method for high dimensional convex objects representation. In this
paper we prove that the convexity of the considered object is equivalent to
the convexity of the associated signed distance function. Then the second
order condition of convex functions is used to characterize the shape convexity
equivalently. We apply this new method to two applications: object segmentation
with convexity prior and convex hull problem (especially with outliers). For
both applications the involved problems can be written as a general
optimization problem with three constraints. Efficient algorithm based on
alternating direction method of multipliers is presented for the optimization
problem. Numerical experiments are conducted to verify the effectiveness and
efficiency of the proposed representation method and algorithm.
",0,0,1
"Precoded GFDM System to Combat Inter Carrier Interference : Performance
  Analysis","  The expected operating scenarios of 5G pose a great challenge to orthogonal
frequency division multiplexing (OFDM) which has poor out of band (OoB)
spectral properties stringent synchronization requirements and large symbol
duration. Generalized frequency division multiplexing (GFDM) which is the focus
of this work has been suggested in the literature as one of the possible
solutions to meet 5G requirements. In this work the analytical performance
evaluation of MMSE receiver for GFDM is presented. We also proposed precoding
techniques to enhance the performance of GFDM. A simplified expression of SINR
for MMSE receiver of GFDM is derived using special properties related to the
modulation matrix of GFDM which are described in this work. This SINR is used
to evaluate the BER performance. Precoding schemes are proposed to reduce
complexity of GFDM-MMSE receiver without compromising on the performance. Block
Inverse Discrete Fourier Transform (BIDFT) and Discrete Fourier Transform (DFT)
based precoding schemes are found to outperform GFDM-MMSE receiver due to
frequency diversity gain while having complexity similar to zero-forcing
receiver of GFDM. It is shown that both BIDFT and DFT-based precoding schemes
reduce peak to average power ratio (PAPR) significantly. Computational
complexity of different transmitters and receivers of precoded and uncoded GFDM
is also presented.
",1,0,0
Curvature: A signature for Action Recognition in Video Sequences,"  In this paper a novel signature of human action recognition namely the
curvature of a video sequence is introduced. In this way the distribution of
sequential data is modeled which enables few-shot learning. Instead of
depending on recognizing features within images our algorithm views actions as
sequences on the universal time scale across a whole sequence of images. The
video sequence viewed as a curve in pixel space is aligned by
reparameterization using the arclength of the curve in pixel space. Once such
curvatures are obtained statistical indexes are extracted and fed into a
learning-based classifier. Overall our method is simple but powerful.
Preliminary experimental results show that our method is effective and achieves
state-of-the-art performance in video-based human action recognition. Moreover
we see latent capacity in transferring this idea into other sequence-based
recognition applications such as speech recognition machine translation and
text generation.
",0,0,1
"Look into Person: Joint Body Parsing & Pose Estimation Network and A New
  Benchmark","  Human parsing and pose estimation have recently received considerable
interest due to their substantial application potentials. However the existing
datasets have limited numbers of images and annotations and lack a variety of
human appearances and coverage of challenging cases in unconstrained
environments. In this paper we introduce a new benchmark named ""Look into
Person (LIP)"" that provides a significant advancement in terms of scalability
diversity and difficulty which are crucial for future developments in
human-centric analysis. This comprehensive dataset contains over 50000
elaborately annotated images with 19 semantic part labels and 16 body joints
which are captured from a broad range of viewpoints occlusions and background
complexities. Using these rich annotations we perform detailed analyses of the
leading human parsing and pose estimation approaches thereby obtaining
insights into the successes and failures of these methods. To further explore
and take advantage of the semantic correlation of these two tasks we propose a
novel joint human parsing and pose estimation network to explore efficient
context modeling which can simultaneously predict parsing and pose with
extremely high quality. Furthermore we simplify the network to solve human
parsing by exploring a novel self-supervised structure-sensitive learning
approach which imposes human pose structures into the parsing results without
resorting to extra supervision. The dataset code and models are available at
http://www.sysu-hcp.net/lip/.
",0,0,1
"AutoTrack: Towards High-Performance Visual Tracking for UAV with
  Automatic Spatio-Temporal Regularization","  Most existing trackers based on discriminative correlation filters (DCF) try
to introduce predefined regularization term to improve the learning of target
objects e.g. by suppressing background learning or by restricting change rate
of correlation filters. However predefined parameters introduce much effort in
tuning them and they still fail to adapt to new situations that the designer
did not think of. In this work a novel approach is proposed to online
automatically and adaptively learn spatio-temporal regularization term.
Spatially local response map variation is introduced as spatial regularization
to make DCF focus on the learning of trust-worthy parts of the object and
global response map variation determines the updating rate of the filter.
Extensive experiments on four UAV benchmarks have proven the superiority of our
method compared to the state-of-the-art CPU- and GPU-based trackers with a
speed of ~60 frames per second running on a single CPU.
  Our tracker is additionally proposed to be applied in UAV localization.
Considerable tests in the indoor practical scenarios have proven the
effectiveness and versatility of our localization method. The code is available
at https://github.com/vision4robotics/AutoTrack.
",0,0,1
"Complexity in animal communication: Estimating the size of N-Gram
  structures","  In this paper new techniques that allow conditional entropy to estimate the
combinatorics of symbols are applied to animal communication studies to
estimate the communication's repertoire size. By using the conditional entropy
estimates at multiple orders the paper estimates the total repertoire sizes
for animal communication across bottlenose dolphins humpback whales and
several species of birds for N-grams length one to three. In addition to
discussing the impact of this method on studies of animal communication
complexity the reliability of these estimates is compared to other methods
through simulation. While entropy does undercount the total repertoire size due
to rare N-grams it gives a more accurate picture of the most frequently used
repertoire than just repertoire size alone.
",1,0,0
SRA: Fast Removal of General Multipath for ToF Sensors,"  A major issue with Time of Flight sensors is the presence of multipath
interference. We present Sparse Reflections Analysis (SRA) an algorithm for
removing this interference which has two main advantages. First it allows for
very general forms of multipath including interference with three or more
paths diffuse multipath resulting from Lambertian surfaces and combinations
thereof. SRA removes this general multipath with robust techniques based on
$L_1$ optimization. Second due to a novel dimension reduction we are able to
produce a very fast version of SRA which is able to run at frame rate.
Experimental results on both synthetic data with ground truth as well as real
images of challenging scenes validate the approach.
",0,0,1
"Delaying Interaction Layers in Transformer-based Encoders for Efficient
  Open Domain Question Answering","  Open Domain Question Answering (ODQA) on a large-scale corpus of documents
(e.g. Wikipedia) is a key challenge in computer science. Although
transformer-based language models such as Bert have shown on SQuAD the ability
to surpass humans for extracting answers in small passages of text they suffer
from their high complexity when faced to a much larger search space. The most
common way to tackle this problem is to add a preliminary Information Retrieval
step to heavily filter the corpus and only keep the relevant passages. In this
paper we propose a more direct and complementary solution which consists in
applying a generic change in the architecture of transformer-based models to
delay the attention between subparts of the input and allow a more efficient
management of computations. The resulting variants are competitive with the
original models on the extractive task and allow on the ODQA setting a
significant speedup and even a performance improvement in many cases.
",0,1,0
Information Theoretical Estimators Toolbox,"  We present ITE (information theoretical estimators) a free and open source
multi-platform Matlab/Octave toolbox that is capable of estimating many
different variants of entropy mutual information divergence association
measures cross quantities and kernels on distributions. Thanks to its highly
modular design ITE supports additionally (i) the combinations of the
estimation techniques (ii) the easy construction and embedding of novel
information theoretical estimators and (iii) their immediate application in
information theoretical optimization problems. ITE also includes a prototype
application in a central problem class of signal processing independent
subspace analysis and its extensions.
",1,0,0
"A queueing approach to the latency of decoupled UL/DL with flexible TDD
  and asymmetric services","  One of the main novelties in 5G is the flexible Time Division Duplex (TDD)
frame which allows adaptation to the latency requirements. However this
flexibility is not sufficient to support heterogeneous latency requirements in
which different traffic instances have different switching requirements between
Uplink (UL) and Downlink (DL). This is visible in a traffic mix of enhanced
mobile broadband (eMBB) and ultra-reliable low-latency communications (URLLC).
In this paper we address this problem through the use of a decoupled UL/DL
access where the UL and the DL of a device are not necessarily served by the
same base station. The latency gain over coupled access is quantified in the
form of queueing sojourn time in a Rayleigh channel as well as an upper bound
for critical traffic.
",1,0,0
Retrospective Reader for Machine Reading Comprehension,"  Machine reading comprehension (MRC) is an AI challenge that requires machine
to determine the correct answers to questions based on a given passage. MRC
systems must not only answer question when necessary but also distinguish when
no answer is available according to the given passage and then tactfully
abstain from answering. When unanswerable questions are involved in the MRC
task an essential verification module called verifier is especially required
in addition to the encoder though the latest practice on MRC modeling still
most benefits from adopting well pre-trained language models as the encoder
block by only focusing on the ""reading"". This paper devotes itself to exploring
better verifier design for the MRC task with unanswerable questions. Inspired
by how humans solve reading comprehension questions we proposed a
retrospective reader (Retro-Reader) that integrates two stages of reading and
verification strategies: 1) sketchy reading that briefly investigates the
overall interactions of passage and question and yield an initial judgment; 2)
intensive reading that verifies the answer and gives the final prediction. The
proposed reader is evaluated on two benchmark MRC challenge datasets SQuAD2.0
and NewsQA achieving new state-of-the-art results. Significance tests show
that our model is significantly better than the strong ELECTRA and ALBERT
baselines. A series of analysis is also conducted to interpret the
effectiveness of the proposed reader.
",0,1,0
"Towards moderate overparameterization: global convergence guarantees for
  training shallow neural networks","  Many modern neural network architectures are trained in an overparameterized
regime where the parameters of the model exceed the size of the training
dataset. Sufficiently overparameterized neural network architectures in
principle have the capacity to fit any set of labels including random noise.
However given the highly nonconvex nature of the training landscape it is not
clear what level and kind of overparameterization is required for first order
methods to converge to a global optima that perfectly interpolate any labels. A
number of recent theoretical works have shown that for very wide neural
networks where the number of hidden units is polynomially large in the size of
the training data gradient descent starting from a random initialization does
indeed converge to a global optima. However in practice much more moderate
levels of overparameterization seems to be sufficient and in many cases
overparameterized models seem to perfectly interpolate the training data as
soon as the number of parameters exceed the size of the training data by a
constant factor. Thus there is a huge gap between the existing theoretical
literature and practical experiments. In this paper we take a step towards
closing this gap. Focusing on shallow neural nets and smooth activations we
show that (stochastic) gradient descent when initialized at random converges at
a geometric rate to a nearby global optima as soon as the square-root of the
number of network parameters exceeds the size of the training data. Our results
also benefit from a fast convergence rate and continue to hold for
non-differentiable activations such as Rectified Linear Units (ReLUs).
",1,0,0
"Capacity scaling in a Non-coherent Wideband Massive SIMO Block Fading
  Channel","  The scaling of coherent and non-coherent channel capacity is studied in a
single-input multiple-output (SIMO) block Rayleigh fading channel as both the
bandwidth and the number of receiver antennas go to infinity jointly with the
transmit power fixed. The transmitter has no channel state information (CSI)
while the receiver may have genie-provided CSI (coherent receiver) or the
channel statistics only (non-coherent receiver). Our results show that if the
available bandwidth is smaller than a threshold bandwidth which is proportional
(up to leading order terms) to the square root of the number of antennas there
is no gap between the coherent capacity and the non-coherent capacity in terms
of capacity scaling behavior. On the other hand when the bandwidth is larger
than this threshold there is a capacity scaling gap. Since achievable rates
using pilot symbols for channel estimation are subject to the non-coherent
capacity bound this work reveals that pilot-assisted coherent receivers in
systems with a large number of receive antennas are unable to exploit excess
spectrum above a given threshold for capacity gain.
",1,0,0
Joint Image Captioning and Question Answering,"  Answering visual questions need acquire daily common knowledge and model the
semantic connection among different parts in images which is too difficult for
VQA systems to learn from images with the only supervision from answers.
Meanwhile image captioning systems with beam search strategy tend to generate
similar captions and fail to diversely describe images. To address the
aforementioned issues we present a system to have these two tasks compensate
with each other which is capable of jointly producing image captions and
answering visual questions. In particular we utilize question and image
features to generate question-related captions and use the generated captions
as additional features to provide new knowledge to the VQA system. For image
captioning our system attains more informative results in term of the relative
improvements on VQA tasks as well as competitive results using automated
metrics. Applying our system to the VQA tasks our results on VQA v2 dataset
achieve 65.8% using generated captions and 69.1% using annotated captions in
validation set and 68.4% in the test-standard set. Further an ensemble of 10
models results in 69.7% in the test-standard split.
",0,1,0
"Learning associations between clinical information and motion-based
  descriptors using a large scale MR-derived cardiac motion atlas","  The availability of large scale databases containing imaging and non-imaging
data such as the UK Biobank represents an opportunity to improve our
understanding of healthy and diseased bodily function. Cardiac motion atlases
provide a space of reference in which the motion fields of a cohort of subjects
can be directly compared. In this work a cardiac motion atlas is built from
cine MR data from the UK Biobank (~ 6000 subjects). Two automated quality
control strategies are proposed to reject subjects with insufficient image
quality. Based on the atlas three dimensionality reduction algorithms are
evaluated to learn data-driven cardiac motion descriptors and statistical
methods used to study the association between these descriptors and non-imaging
data. Results show a positive correlation between the atlas motion descriptors
and body fat percentage basal metabolic rate hypertension smoking status and
alcohol intake frequency. The proposed method outperforms the ability to
identify changes in cardiac function due to these known cardiovascular risk
factors compared to ejection fraction the most commonly used descriptor of
cardiac function. In conclusion this work represents a framework for further
investigation of the factors influencing cardiac health.
",0,0,1
"SEPP: Similarity Estimation of Predicted Probabilities for Defending and
  Detecting Adversarial Text","  There are two cases describing how a classifier processes input text namely
misclassification and correct classification. In terms of misclassified texts
a classifier handles the texts with both incorrect predictions and adversarial
texts which are generated to fool the classifier which is called a victim.
Both types are misunderstood by the victim but they can still be recognized by
other classifiers. This induces large gaps in predicted probabilities between
the victim and the other classifiers. In contrast text correctly classified by
the victim is often successfully predicted by the others and induces small
gaps. In this paper we propose an ensemble model based on similarity
estimation of predicted probabilities (SEPP) to exploit the large gaps in the
misclassified predictions in contrast to small gaps in the correct
classification. SEPP then corrects the incorrect predictions of the
misclassified texts. We demonstrate the resilience of SEPP in defending and
detecting adversarial texts through different types of victim classifiers
classification tasks and adversarial attacks.
",0,1,0
"Robust Technique for Representative Volume Element Identification in
  Noisy Microtomography Images of Porous Materials Based on Pores Morphology
  and Their Spatial Distribution","  Microtomography is a powerful method of materials investigation. It enables
to obtain physical properties of porous media non-destructively that is useful
in studies. One of the application ways is a calculation of porosity pore
sizes surface area and other parameters of metal-ceramic (cermet) membranes
which are widely spread in the filtration industry. The microtomography
approach is efficient because all of those parameters are calculated
simultaneously in contrast to the conventional techniques. Nevertheless the
calculations on Micro-CT reconstructed images appear to be time-consuming
consequently representative volume element should be chosen to speed them up.
This research sheds light on representative elementary volume identification
without consideration of any physical parameters such as porosity etc. Thus
the volume element could be found even in noised and grayscale images. The
proposed method is flexible and does not overestimate the volume size in the
case of anisotropic samples. The obtained volume element could be used for
computations of the domain's physical characteristics if the image is filtered
and binarized or for selections of optimal filtering parameters for denoising
procedure.
",0,0,1
Residual Moment Loss for Medical Image Segmentation,"  Location information is proven to benefit the deep learning models on
capturing the manifold structure of target objects and accordingly boosts the
accuracy of medical image segmentation. However most existing methods encode
the location information in an implicit way e.g. the distance transform maps
which describe the relative distance from each pixel to the contour boundary
for the network to learn. These implicit approaches do not fully exploit the
position information (i.e. absolute location) of targets. In this paper we
propose a novel loss function namely residual moment (RM) loss to explicitly
embed the location information of segmentation targets during the training of
deep learning networks. Particularly motivated by image moments the
segmentation prediction map and ground-truth map are weighted by coordinate
information. Then our RM loss encourages the networks to maintain the
consistency between the two weighted maps which promotes the segmentation
networks to easily locate the targets and extract manifold-structure-related
features. We validate the proposed RM loss by conducting extensive experiments
on two publicly available datasets i.e. 2D optic cup and disk segmentation
and 3D left atrial segmentation. The experimental results demonstrate the
effectiveness of our RM loss which significantly boosts the accuracy of
segmentation networks.
",0,0,1
"Image aesthetic evaluation using paralleled deep convolution neural
  network","  Image aesthetic evaluation has attracted much attention in recent years.
Image aesthetic evaluation methods heavily depend on the effective aesthetic
feature. Traditional meth-ods always extract hand-crafted features. However
these hand-crafted features are always designed to adapt particu-lar datasets
and extraction of them needs special design. Rather than extracting
hand-crafted features an automati-cally learn of aesthetic features based on
deep convolutional neural network (DCNN) is first adopt in this paper. As we
all know when the training dataset is given the DCNN architecture with high
complexity may meet the over-fitting problem. On the other side the DCNN
architecture with low complexity would not efficiently extract effective
features. For these reasons we further propose a paralleled convolutional
neural network (PDCNN) with multi-level structures to automatically adapt to
the training dataset. Experimental results show that our proposed PDCNN
architecture achieves better performance than other traditional methods.
",0,0,1
A Survey on Object Detection in Optical Remote Sensing Images,"  Object detection in optical remote sensing images being a fundamental but
challenging problem in the field of aerial and satellite image analysis plays
an important role for a wide range of applications and is receiving significant
attention in recent years. While enormous methods exist a deep review of the
literature concerning generic object detection is still lacking. This paper
aims to provide a review of the recent progress in this field. Different from
several previously published surveys that focus on a specific object class such
as building and road we concentrate on more generic object categories
including but are not limited to road building tree vehicle ship
airport urban-area. Covering about 270 publications we survey 1) template
matching-based object detection methods 2) knowledge-based object detection
methods 3) object-based image analysis (OBIA)-based object detection methods
4) machine learning-based object detection methods and 5) five publicly
available datasets and three standard evaluation metrics. We also discuss the
challenges of current studies and propose two promising research directions
namely deep learning-based feature representation and weakly supervised
learning-based geospatial object detection. It is our hope that this survey
will be beneficial for the researchers to have better understanding of this
research field.
",0,0,1
"All Fingers are not Equal: Intensity of References in Scientific
  Articles","  Research accomplishment is usually measured by considering all citations with
equal importance thus ignoring the wide variety of purposes an article is
being cited for. Here we posit that measuring the intensity of a reference is
crucial not only to perceive better understanding of research endeavor but
also to improve the quality of citation-based applications. To this end we
collect a rich annotated dataset with references labeled by the intensity and
propose a novel graph-based semi-supervised model GraLap to label the
intensity of references. Experiments with AAN datasets show a significant
improvement compared to the baselines to achieve the true labels of the
references (46% better correlation). Finally we provide four applications to
demonstrate how the knowledge of reference intensity leads to design better
real-world applications.
",0,1,0
"Spatial Attention-based Non-reference Perceptual Quality Prediction
  Network for Omnidirectional Images","  Due to the strong correlation between visual attention and perceptual
quality many methods attempt to use human saliency information for image
quality assessment. Although this mechanism can get good performance the
networks require human saliency labels which is not easily accessible for
omnidirectional images (ODI). To alleviate this issue we propose a spatial
attention-based perceptual quality prediction network for non-reference quality
assessment on ODIs (SAP-net). To drive our SAP-net we establish a large-scale
IQA dataset of ODIs (IQA-ODI) which is composed of subjective scores of 200
subjects on 1080 ODIs. In IQA-ODI there are 120 high quality ODIs as
reference and 960 ODIs with impairments in both JPEG compression and map
projection. Without any human saliency labels our network can adaptively
estimate human perceptual quality on impaired ODIs through a self-attention
manner which significantly promotes the prediction performance of quality
scores. Moreover our method greatly reduces the computational complexity in
quality assessment task on ODIs. Extensive experiments validate that our
network outperforms 9 state-of-the-art methods for quality assessment on ODIs.
The dataset and code have been available on url
https://github.com/yanglixiaoshen/SAP-Net.
",0,0,1
"Automatic Classification of Defective Photovoltaic Module Cells in
  Electroluminescence Images","  Electroluminescence (EL) imaging is a useful modality for the inspection of
photovoltaic (PV) modules. EL images provide high spatial resolution which
makes it possible to detect even finest defects on the surface of PV modules.
However the analysis of EL images is typically a manual process that is
expensive time-consuming and requires expert knowledge of many different
types of defects. In this work we investigate two approaches for automatic
detection of such defects in a single image of a PV cell. The approaches differ
in their hardware requirements which are dictated by their respective
application scenarios. The more hardware-efficient approach is based on
hand-crafted features that are classified in a Support Vector Machine (SVM). To
obtain a strong performance we investigate and compare various processing
variants. The more hardware-demanding approach uses an end-to-end deep
Convolutional Neural Network (CNN) that runs on a Graphics Processing Unit
(GPU). Both approaches are trained on 1968 cells extracted from high
resolution EL intensity images of mono- and polycrystalline PV modules. The CNN
is more accurate and reaches an average accuracy of 88.42%. The SVM achieves a
slightly lower average accuracy of 82.44% but can run on arbitrary hardware.
Both automated approaches make continuous highly accurate monitoring of PV
cells feasible.
",0,0,1
Users prefer Guetzli JPEG over same-sized libjpeg,"  We report on pairwise comparisons by human raters of JPEG images from libjpeg
and our new Guetzli encoder. Although both files are size-matched 75% of
ratings are in favor of Guetzli. This implies the Butteraugli psychovisual
image similarity metric which guides Guetzli is reasonably close to human
perception at high quality levels. We provide access to the raw ratings and
source images for further analysis and study.
",0,0,1
"Quantifying the Preferential Direction of the Model Gradient in
  Adversarial Training With Projected Gradient Descent","  Adversarial training especially projected gradient descent (PGD) has been a
successful approach for improving robustness against adversarial attacks. After
adversarial training gradients of models with respect to their inputs have a
preferential direction. However the direction of alignment is not
mathematically well established making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition that our metric
presents higher alignment values than a competing metric formulation and that
enforcing this alignment increases the robustness of models.
",0,0,1
Dependency Decomposition and a Reject Option for Explainable Models,"  Deploying machine learning models in safety-related do-mains (e.g. autonomous
driving medical diagnosis) demands for approaches that are explainable robust
against adversarial attacks and aware of the model uncertainty. Recent deep
learning models perform extremely well in various inference tasks but the
black-box nature of these approaches leads to a weakness regarding the three
requirements mentioned above. Recent advances offer methods to visualize
features describe attribution of the input (e.g.heatmaps) provide textual
explanations or reduce dimensionality. Howeverare explanations for
classification tasks dependent or are they independent of each other? For
in-stance is the shape of an object dependent on the color? What is the effect
of using the predicted class for generating explanations and vice versa? In the
context of explainable deep learning models we present the first analysis of
dependencies regarding the probability distribution over the desired image
classification outputs and the explaining variables (e.g. attributes texts
heatmaps). Therefore we perform an Explanation Dependency Decomposition (EDD).
We analyze the implications of the different dependencies and propose two ways
of generating the explanation. Finally we use the explanation to verify
(accept or reject) the prediction
",0,0,1
Executable Interval Temporal Logic Specifications,"  In this paper the reversibility of executable Interval Temporal Logic (ITL)
specifications is investigated. ITL allows for the reasoning about systems in
terms of behaviours which are represented as non-empty sequences of states. It
allows for the specification of systems at different levels of abstraction. At
a high level this specification is in terms of properties for instance safety
and liveness properties. At concrete level one can specify a system in terms of
programming constructs. One can execute these concrete specification i.e.
test and simulate the behaviour of the system. In this paper we will formalise
this notion of executability of ITL specifications. ITL also has a reflection
operator which allows for the reasoning about reversed behaviours. We will
investigate the reversibility of executable ITL specifications i.e. how one
can use this reflection operator to reverse the concrete behaviour of a
particular system.
",0,1,0
Online Open World Recognition,"  As we enter into the big data age and an avalanche of images have become
readily available recognition systems face the need to move from close lab
settings where the number of classes and training data are fixed to dynamic
scenarios where the number of categories to be recognized grows continuously
over time as well as new data providing useful information to update the
system. Recent attempts like the open world recognition framework tried to
inject dynamics into the system by detecting new unknown classes and adding
them incrementally while at the same time continuously updating the models for
the known classes. incrementally adding new classes and detecting instances
from unknown classes while at the same time continuously updating the models
for the known classes. In this paper we argue that to properly capture the
intrinsic dynamic of open world recognition it is necessary to add to these
aspects (a) the incremental learning of the underlying metric (b) the
incremental estimate of confidence thresholds for the unknown classes and (c)
the use of local learning to precisely describe the space of classes. We extend
three existing metric learning algorithms towards these goals by using online
metric learning. Experimentally we validate our approach on two large-scale
datasets in different learning scenarios. For all these scenarios our proposed
methods outperform their non-online counterparts. We conclude that local and
online learning is important to capture the full dynamics of open world
recognition.
",0,0,1
"The Longer the Better? The Interplay Between Review Length and Line of
  Argumentation in Online Consumer Reviews","  Review helpfulness serves as focal point in understanding customers' purchase
decision-making process on online retailer platforms. An overwhelming majority
of previous works find longer reviews to be more helpful than short reviews. In
this paper we propose that longer reviews should not be assumed to be
uniformly more helpful; instead we argue that the effect depends on the line
of argumentation in the review text. To test this idea we use a large dataset
of customer reviews from Amazon in combination with a state-of-the-art approach
from natural language processing that allows us to study argumentation lines at
sentence level. Our empirical analysis suggests that the frequency of
argumentation changes moderates the effect of review length on helpfulness.
Altogether we disprove the prevailing narrative that longer reviews are
uniformly perceived as more helpful. Our findings allow retailer platforms to
improve their customer feedback systems and to feature more useful product
reviews.
",0,1,0
"Fast and Provable Algorithms for Spectrally Sparse Signal Reconstruction
  via Low-Rank Hankel Matrix Completion","  A spectrally sparse signal of order $r$ is a mixture of $r$ damped or
undamped complex sinusoids. This paper investigates the problem of
reconstructing spectrally sparse signals from a random subset of $n$ regular
time domain samples which can be reformulated as a low rank Hankel matrix
completion problem. We introduce an iterative hard thresholding (IHT) algorithm
and a fast iterative hard thresholding (FIHT) algorithm for efficient
reconstruction of spectrally sparse signals via low rank Hankel matrix
completion. Theoretical recovery guarantees have been established for FIHT
showing that $O(r^2log^2(n))$ number of samples are sufficient for exact
recovery with high probability. Empirical performance comparisons establish
significant computational advantages for IHT and FIHT. In particular numerical
simulations on $3$D arrays demonstrate the capability of FIHT on handling large
and high-dimensional real data.
",1,0,0
Computer classification of linear codes,"  We present algorithms for classification of linear codes over finite fields
based on canonical augmentation and on lattice point enumeration. We apply
these algorithms to obtain classification results over fields with 2 3 and 4
elements. We validate a correct implementation of the algorithms with known
classification results from the literature which we partially extend to larger
ranges of parameters.
",1,0,0
"On the Secrecy Rate Region of a Fading Multiple-Antenna Gaussian
  Broadcast Channel with Confidential Messages and Partial CSIT","  In this paper we consider the secure transmission over the fast fading
multiple antenna Gaussian broadcast channels with confidential messages
(FMGBC-CM) where a multiple-antenna transmitter sends independent confidential
messages to two users with information theoretic secrecy and only the
statistics of the receivers' channel state information are known at the
transmitter. We first use the same marginal property of the FMGBC-CM to
classify the non-trivial cases i.e. those not degraded to the common wiretap
channels. We then derive the achievable rate region for the FMGBC-CM by solving
the channel input covariance matrices and the inflation factor. Due to the
complicated rate region formulae we resort to low SNR analysis to investigate
the characteristics of the channel. Finally the numerical examples show that
under the information-theoretic secrecy requirement both users can achieve
positive rates simultaneously.
",1,0,0
Dense Scale Network for Crowd Counting,"  Crowd counting has been widely studied by computer vision community in recent
years. Due to the large scale variation it remains to be a challenging task.
Previous methods adopt either multi-column CNN or single-column CNN with
multiple branches to deal with this problem. However restricted by the number
of columns or branches these methods can only capture a few different scales
and have limited capability. In this paper we propose a simple but effective
network called DSNet for crowd counting which can be easily trained in an
end-to-end fashion. The key component of our network is the dense dilated
convolution block in which each dilation layer is densely connected with the
others to preserve information from continuously varied scales. The dilation
rates in dilation layers are carefully selected to prevent the block from
gridding artifacts. To further enlarge the range of scales covered by the
network we cascade three blocks and link them with dense residual connections.
We also introduce a novel multi-scale density level consistency loss for
performance improvement. To evaluate our method we compare it with
state-of-the-art algorithms on four crowd counting datasets (ShanghaiTech
UCF-QNRF UCF_CC_50 and UCSD). Experimental results demonstrate that DSNet can
achieve the best performance and make significant improvements on all the four
datasets (30% on the UCF-QNRF and UCF_CC_50 and 20% on the others).
",0,0,1
"Learning Priors in High-frequency Domain for Inverse Imaging
  Reconstruction","  Ill-posed inverse problems in imaging remain an active research topic in
several decades with new approaches constantly emerging. Recognizing that the
popular dictionary learning and convolutional sparse coding are both
essentially modeling the high-frequency component of an image which convey
most of the semantic information such as texture details in this work we
propose a novel multi-profile high-frequency transform-guided denoising
autoencoder as prior (HF-DAEP). To achieve this goal we first extract a set of
multi-profile high-frequency components via a specific transformation and add
the artificial Gaussian noise to these high-frequency components as training
samples. Then as the high-frequency prior information is learned we
incorporate it into classical iterative reconstruction process by proximal
gradient descent technique. Preliminary results on highly under-sampled
magnetic resonance imaging and sparse-view computed tomography reconstruction
demonstrate that the proposed method can efficiently reconstruct feature
details and present advantages over state-of-the-arts.
",0,0,1
ListOps: A Diagnostic Dataset for Latent Tree Learning,"  Latent tree learning models learn to parse a sentence without syntactic
supervision and use that parse to build the sentence representation. Existing
work on such models has shown that while they perform well on tasks like
sentence classification they do not learn grammars that conform to any
plausible semantic or syntactic formalism (Williams et al. 2018a). Studying
the parsing ability of such models in natural language can be challenging due
to the inherent complexities of natural language like having several valid
parses for a single sentence. In this paper we introduce ListOps a toy dataset
created to study the parsing ability of latent tree models. ListOps sequences
are in the style of prefix arithmetic. The dataset is designed to have a single
correct parsing strategy that a system needs to learn to succeed at the task.
We show that the current leading latent tree models are unable to learn to
parse and succeed at ListOps. These models achieve accuracies worse than purely
sequential RNNs.
",0,1,0
Automatic Detection of Vague Words and Sentences in Privacy Policies,"  Website privacy policies represent the single most important source of
information for users to gauge how their personal data are collected used and
shared by companies. However privacy policies are often vague and people
struggle to understand the content. Their opaqueness poses a significant
challenge to both users and policy regulators. In this paper we seek to
identify vague content in privacy policies. We construct the first corpus of
human-annotated vague words and sentences and present empirical studies on
automatic vagueness detection. In particular we investigate context-aware and
context-agnostic models for predicting vague words and explore
auxiliary-classifier generative adversarial networks for characterizing
sentence vagueness. Our experimental results demonstrate the effectiveness of
proposed approaches. Finally we provide suggestions for resolving vagueness
and improving the usability of privacy policies.
",0,1,0
A Probabilistic Generative Grammar for Semantic Parsing,"  Domain-general semantic parsing is a long-standing goal in natural language
processing where the semantic parser is capable of robustly parsing sentences
from domains outside of which it was trained. Current approaches largely rely
on additional supervision from new domains in order to generalize to those
domains. We present a generative model of natural language utterances and
logical forms and demonstrate its application to semantic parsing. Our approach
relies on domain-independent supervision to generalize to new domains. We
derive and implement efficient algorithms for training parsing and sentence
generation. The work relies on a novel application of hierarchical Dirichlet
processes (HDPs) for structured prediction which we also present in this
manuscript.
  This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov
(2022) where the model plays a central role in a larger natural language
understanding system.
  This manuscript provides a new simplified and more complete presentation of
the work first introduced in Saparov Saraswat and Mitchell (2017). The
description and proofs of correctness of the training algorithm parsing
algorithm and sentence generation algorithm are much simplified in this new
presentation. We also describe the novel application of hierarchical Dirichlet
processes for structured prediction. In addition we extend the earlier work
with a new model of word morphology which utilizes the comprehensive
morphological data from Wiktionary.
",0,1,0
"Classifying the classifier: dissecting the weight space of neural
  networks","  This paper presents an empirical study on the weights of neural networks
where we interpret each model as a point in a high-dimensional space -- the
neural weight space. To explore the complex structure of this space we sample
from a diverse selection of training variations (dataset optimization
procedure architecture etc.) of neural network classifiers and train a large
number of models to represent the weight space. Then we use a machine learning
approach for analyzing and extracting information from this space. Most
centrally we train a number of novel deep meta-classifiers with the objective
of classifying different properties of the training setup by identifying their
footprints in the weight space. Thus the meta-classifiers probe for patterns
induced by hyper-parameters so that we can quantify how much where and when
these are encoded through the optimization process. This provides a novel and
complementary view for explainable AI and we show how meta-classifiers can
reveal a great deal of information about the training setup and optimization
by only considering a small subset of randomly selected consecutive weights. To
promote further research on the weight space we release the neural weight
space (NWS) dataset -- a collection of 320K weight snapshots from 16K
individually trained deep neural networks.
",0,0,1
Variational Bayesian Inference of Line Spectra,"  In this paper we address the fundamental problem of line spectral estimation
in a Bayesian framework. We target model order and parameter estimation via
variational inference in a probabilistic model in which the frequencies are
continuous-valued i.e. not restricted to a grid; and the coefficients are
governed by a Bernoulli-Gaussian prior model turning model order selection into
binary sequence detection. Unlike earlier works which retain only point
estimates of the frequencies we undertake a more complete Bayesian treatment
by estimating the posterior probability density functions (pdfs) of the
frequencies and computing expectations over them. Thus we additionally capture
and operate with the uncertainty of the frequency estimates. Aiming to maximize
the model evidence variational optimization provides analytic approximations
of the posterior pdfs and also gives estimates of the additional parameters. We
propose an accurate representation of the pdfs of the frequencies by mixtures
of von Mises pdfs which yields closed-form expectations. We define the
algorithm VALSE in which the estimates of the pdfs and parameters are
iteratively updated. VALSE is a gridless convergent method does not require
parameter tuning can easily include prior knowledge about the frequencies and
provides approximate posterior pdfs based on which the uncertainty in line
spectral estimation can be quantified. Simulation results show that accounting
for the uncertainty of frequency estimates rather than computing just point
estimates significantly improves the performance. The performance of VALSE is
superior to that of state-of-the-art methods and closely approaches the
Cram'er-Rao bound computed for the true model order.
",1,0,0
"Joint Optimal Number of RF chains and Power Allocation for Downlink
  Massive MIMO Systems","  This paper investigates the downlink of massive multiple-input
multiple-output (MIMO) systems that include a single cell Base Station (BS)
equipped with large number of antennas serving multiple users. As the number of
RF chains is getting large the system model considered in this paper assumes a
non negligible circuit power consumption. Hence the aim of this work is to
find the optimal balance between the power consumed by the RF chains and the
transmitted power. First assuming an equal power allocation among users the
optimal number of RF chains to be activated is analytically found. Then for a
given number of RF chains we derive analytically the optimal power allocation
among users. Based on these analysis we propose an iterative algorithm that
computes jointly the optimal number of RF chains and the optimal power
allocation vector. Simulations validate the analytical results and show the
high performance provided by the proposed algorithm.
",1,0,0
"Long-Term Human Motion Prediction by Modeling Motion Context and
  Enhancing Motion Dynamic","  Human motion prediction aims at generating future frames of human motion
based on an observed sequence of skeletons. Recent methods employ the latest
hidden states of a recurrent neural network (RNN) to encode the historical
skeletons which can only address short-term prediction. In this work we
propose a motion context modeling by summarizing the historical human motion
with respect to the current prediction. A modified highway unit (MHU) is
proposed for efficiently eliminating motionless joints and estimating next pose
given the motion context. Furthermore we enhance the motion dynamic by
minimizing the gram matrix loss for long-term motion prediction. Experimental
results show that the proposed model can promisingly forecast the human future
movements which yields superior performances over related state-of-the-art
approaches. Moreover specifying the motion context with the activity labels
enables our model to perform human motion transfer.
",0,0,1
"Noise Robust IOA/CAS Speech Separation and Recognition System For The
  Third 'CHIME' Challenge","  This paper presents the contribution to the third 'CHiME' speech separation
and recognition challenge including both front-end signal processing and
back-end speech recognition. In the front-end Multi-channel Wiener filter
(MWF) is designed to achieve background noise reduction. Different from
traditional MWF optimized parameter for the tradeoff between noise reduction
and target signal distortion is built according to the desired noise reduction
level. In the back-end several techniques are taken advantage to improve the
noisy Automatic Speech Recognition (ASR) performance including Deep Neural
Network (DNN) Convolutional Neural Network (CNN) and Long short-term memory
(LSTM) using medium vocabulary Lattice rescoring with a big vocabulary
language model finite state transducer and ROVER scheme. Experimental results
show the proposed system combining front-end and back-end is effective to
improve the ASR performance.
",0,1,0
"Real-Time Monocular Human Depth Estimation and Segmentation on Embedded
  Systems","  Estimating a scene's depth to achieve collision avoidance against moving
pedestrians is a crucial and fundamental problem in the robotic field. This
paper proposes a novel low complexity network architecture for fast and
accurate human depth estimation and segmentation in indoor environments aiming
to applications for resource-constrained platforms (including battery-powered
aerial micro-aerial and ground vehicles) with a monocular camera being the
primary perception module. Following the encoder-decoder structure the
proposed framework consists of two branches one for depth prediction and
another for semantic segmentation. Moreover network structure optimization is
employed to improve its forward inference speed. Exhaustive experiments on
three self-generated datasets prove our pipeline's capability to execute in
real-time achieving higher frame rates than contemporary state-of-the-art
frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT)
while maintaining comparable accuracy.
",0,0,1
Communication-Free Parallel Supervised Topic Models,"  Embarrassingly (communication-free) parallel Markov chain Monte Carlo (MCMC)
methods are commonly used in learning graphical models. However MCMC cannot be
directly applied in learning topic models because of the quasi-ergodicity
problem caused by multimodal distribution of topics. In this paper we develop
an embarrassingly parallel MCMC algorithm for sLDA. Our algorithm works by
switching the order of sampled topics combination and labeling variable
prediction in sLDA in which it overcomes the quasi-ergodicity problem because
high-dimension topics that follow a multimodal distribution are projected into
one-dimension document labels that follow a unimodal distribution. Our
empirical experiments confirm that the out-of-sample prediction performance
using our embarrassingly parallel algorithm is comparable to non-parallel sLDA
while the computation time is significantly reduced.
",0,1,0
"Not all Failure Modes are Created Equal: Training Deep Neural Networks
  for Explicable (Mis)Classification","  Deep Neural Networks are often brittle on image classification tasks and
known to misclassify inputs. While these misclassifications may be inevitable
all failure modes cannot be considered equal. Certain misclassifications (eg.
classifying the image of a dog to an airplane) can perplex humans and result in
the loss of human trust in the system. Even worse these errors (eg. a person
misclassified as a primate) can have odious societal impacts. Thus in this
work we aim to reduce inexplicable errors. To address this challenge we first
discuss methods to obtain the class-level semantics that capture the human's
expectation ($M^h$) regarding which classes are semantically close em vs.
ones that are far away. We show that for popular image benchmarks (like
CIFAR-10 CIFAR-100 ImageNet) class-level semantics can be readily obtained
by leveraging either human subject studies or publicly available human-curated
knowledge bases. Second we propose the use of Weighted Loss Functions (WLFs)
to penalize misclassifications by the weight of their inexplicability. Finally
we show that training (or fine-tuning) existing classifiers with the proposed
methods lead to Deep Neural Networks that have (1) comparable top-1 accuracy
(2) more explicable failure modes on both in-distribution and
out-of-distribution (OOD) test data and (3) incur significantly less cost in
the gathering of additional human labels compared to existing works.
",0,0,1
Dynamic Spectrum Management: A Complete Complexity Characterization,"  Consider a multi-user multi-carrier communication system where multiple users
share multiple discrete subcarriers. To achieve high spectrum efficiency the
users in the system must choose their transmit power dynamically in response to
fast channel fluctuations. Assuming perfect channel state information two
formulations for the spectrum management (power control) problem are considered
in this paper: the first is to minimize the total transmission power subject to
all users' transmission data rate constraints and the second is to maximize
the min-rate utility subject to individual power constraints at each user. It
is known in the literature that both formulations of the problem are polynomial
time solvable when the number of subcarriers is one and strongly NP-hard when
the number of subcarriers are greater than or equal to three. However the
complexity characterization of the problem when the number of subcarriers is
two has been missing for a long time. This paper answers this long-standing
open question: both formulations of the problem are strongly NP-hard when the
number of subcarriers is two.
",1,0,0
"Compositional Approaches for Representing Relations Between Words: A
  Comparative Study","  Identifying the relations that exist between words (or entities) is important
for various natural language processing tasks such as relational search
noun-modifier classification and analogy detection. A popular approach to
represent the relations between a pair of words is to extract the patterns in
which the words co-occur with from a corpus and assign each word-pair a vector
of pattern frequencies. Despite the simplicity of this approach it suffers
from data sparseness information scalability and linguistic creativity as the
model is unable to handle previously unseen word pairs in a corpus. In
contrast a compositional approach for representing relations between words
overcomes these issues by using the attributes of each individual word to
indirectly compose a representation for the common relations that hold between
the two words. This study aims to compare different operations for creating
relation representations from word-level representations. We investigate the
performance of the compositional methods by measuring the relational
similarities using several benchmark datasets for word analogy. Moreover we
evaluate the different relation representations in a knowledge base completion
task.
",0,1,0
"AutoPrompt: Eliciting Knowledge from Language Models with Automatically
  Generated Prompts","  The remarkable success of pretrained language models has motivated the study
of what kinds of knowledge these models learn during pretraining. Reformulating
tasks as fill-in-the-blanks problems (e.g. cloze tests) is a natural approach
for gauging such knowledge however its usage is limited by the manual effort
and guesswork required to write suitable prompts. To address this we develop
AutoPrompt an automated method to create prompts for a diverse set of tasks
based on a gradient-guided search. Using AutoPrompt we show that masked
language models (MLMs) have an inherent capability to perform sentiment
analysis and natural language inference without additional parameters or
finetuning sometimes achieving performance on par with recent state-of-the-art
supervised models. We also show that our prompts elicit more accurate factual
knowledge from MLMs than the manually created prompts on the LAMA benchmark
and that MLMs can be used as relation extractors more effectively than
supervised relation extraction models. These results demonstrate that
automatically generated prompts are a viable parameter-free alternative to
existing probing methods and as pretrained LMs become more sophisticated and
capable potentially a replacement for finetuning.
",0,1,0
A Better Variant of Self-Critical Sequence Training,"  In this work we present a simple yet better variant of Self-Critical
Sequence Training. We make a simple change in the choice of baseline function
in REINFORCE algorithm. The new baseline can bring better performance with no
extra cost compared to the greedy decoding baseline.
",0,0,1
Flexible Length Polar Codes through Graph Based Augmentation,"  The structure of polar codes inherently requires block lengths to be powers
of two. In this paper we investigate how different block lengths can be
realized by coupling of several short-length polar codes. For this we first
analyze ""code augmentation"" to better protect the semipolarized channels
improving the BER performance under belief propagation decoding. Several serial
and parallel augmentation schemes are discussed. A coding gain of $0.3$ dB at a
BER of $10^-5$ can be observed for the same total rate and length. Further
we extend this approach towards coupling of several ""sub-polar codes"" leading
to a reduced computational complexity and enabling the construction of flexible
length polar codes.
",1,0,0
Reduced-rank Analysis of the Total Least Squares,"  The reduced-rank method exploits the distortion-variance tradeoff to yield
superior solutions for classic problems in statistical signal processing such
as parameter estimation and filtering. The central idea is to reduce the
variance of the solution at the expense of introducing a little distortion. In
the context of parameter estimation this yields an estimator whose sum of
distortion plus variance is smaller than the variance of its undistorted
counterpart. The method intrinsically results in an ordering mechanism for the
singular vectors of the system matrix in the measurement model used for
estimating the parameter of interest. According to this ordering rule only a
few emphdominant singular vectors need to be selected to construct the
reduced-rank solution while the rest can be discarded. The reduced-rank
estimator is less sensitive to measurement errors. In this paper we attempt to
derive the reduced-rank estimator for the total least squares (TLS) problem
including the order selection rule. It will be shown that due to the inherent
structure of the problem it is not possible to exploit the distortion-variance
tradeoff in TLS formulations using existing techniques except in some special
cases. This result has not been reported previously and warrants a new
viewpoint to achieve rank reduction for the TLS estimation problem. The work is
motivated by the problems arising in practical applications such as channel
estimation in wireless communication systems and time synchronization in
wireless sensor networks.
",1,0,0
Cooperative Communication Using Network Coding,"  We consider a cognitive radio network scenario where a primary transmitter
and a secondary transmitter respectively communicate a message to their
respective primary receiver and secondary receiver over a packet-based wireless
link using a joint automatic-repeat-request (ARQ) error control scheme. The
secondary transmitter assists in the retransmission of the primary message
which improves the primary performance and is granted limited access to the
transmission resources. Conventional ARQ as well as two network-coding schemes
are investigated for application in the retransmission phase; namely the static
network-coding (SNC) scheme and the adaptive network-coding (ANC) scheme. For
each scheme we analyze the transmission process by investigating the
distribution of the number of transmission attempts and approximate it by
normal distributions. Considering both the cases of an adaptive frame size and
a truncated frame size we derive analytical results on packet throughput and
infer that the ANC scheme outperforms the SNC scheme.
",1,0,0
Mis-classified Vector Guided Softmax Loss for Face Recognition,"  Face recognition has witnessed significant progress due to the advances of
deep convolutional neural networks (CNNs) the central task of which is how to
improve the feature discrimination. To this end several margin-based
(textite.g. angular additive and additive angular margins) softmax loss
functions have been proposed to increase the feature margin between different
classes. However despite great achievements have been made they mainly suffer
from three issues: 1) Obviously they ignore the importance of informative
features mining for discriminative learning; 2) They encourage the feature
margin only from the ground truth class without realizing the discriminability
from other non-ground truth classes; 3) The feature margin between different
classes is set to be same and fixed which may not adapt the situations very
well. To cope with these issues this paper develops a novel loss function
which adaptively emphasizes the mis-classified feature vectors to guide the
discriminative feature learning. Thus we can address all the above issues and
achieve more discriminative face features. To the best of our knowledge this
is the first attempt to inherit the advantages of feature margin and feature
mining into a unified loss function. Experimental results on several benchmarks
have demonstrated the effectiveness of our method over state-of-the-art
alternatives.
",0,0,1
Wireless MapReduce Distributed Computing,"  Motivated by mobile edge computing and wireless data centers we study a
wireless distributed computing framework where the distributed nodes exchange
information over a wireless interference network. Our framework follows the
structure of MapReduce. This framework consists of Map Shuffle and Reduce
phases where Map and Reduce are computation phases and Shuffle is a data
transmission phase. In our setting we assume that the transmission is operated
over a wireless interference network. We demonstrate that by duplicating the
computation work at a cluster of distributed nodes in the Map phase one can
reduce the amount of transmission load required for the Shuffle phase. In this
work we characterize the fundamental tradeoff between computation load and
communication load under the assumption of one-shot linear schemes. The
proposed scheme is based on side information cancellation and zero-forcing and
we prove that it is optimal in terms of computation-communication tradeoff. The
proposed scheme outperforms the naive TDMA scheme with single node transmission
at a time as well as the coded TDMA scheme that allows coding across data in
terms of the computation-communication tradeoff.
",1,0,0
"Type Based Sign Modulation and its Application for ISI mitigation in
  Molecular Communication","  An important challenge in design of modulation schemes for molecular
communication is positivity of the transmission signal (only a positive
concentration of molecules can be released in the environment). This
restriction makes handling of the InterSymbol Interference (ISI) a challenge
for molecular communication. Previous works have proposed use of chemical
reactions to remove molecules from the environment and to effectively simulate
negative signals. However the differential equation describing a
diffusion-reaction process is non-linear. This precludes the possibility of
using Fourier transform tools. In this paper a solution for simulating
negative signals based on the diffusion-reaction channel model is proposed.
While the proposed solution does not exploit the full degrees of freedom
available for signaling in a diffusion-reaction process but its end-to-end
system is a linear channel and amenable to Fourier transform analysis. Based on
our solution a modulation scheme and a precoder are introduced and shown to
have a significant reduction in error probability compared to previous
modulation schemes such as CSK and MCSK. The effect of various imperfections
(such as quantization error) on the communication system performance are
studied.
",1,0,0
Phase Retrieval of Low-Rank Matrices by Anchored Regression,"  We study the low-rank phase retrieval problem where we try to recover a
$d_1times d_2$ low-rank matrix from a series of phaseless linear measurements.
This is a fourth-order inverse problem as we are trying to recover factors of
matrix that have been put through a quadratic nonlinearity after being
multiplied together.
  We propose a solution to this problem using the recently introduced technique
of anchored regression. This approach uses two different types of convex
relaxations: we replace the quadratic equality constraints for the phaseless
measurements by a search over a polytope and enforce the rank constraint
through nuclear norm regularization. The result is a convex program that works
in the space of $d_1 times d_2$ matrices.
  We analyze two specific scenarios. In the first the target matrix is
rank-$1$ and the observations are structured to correspond to a phaseless
blind deconvolution. In the second the target matrix has general rank and we
observe the magnitudes of the inner products against a series of independent
Gaussian random matrices. In each of these problems we show that the anchored
regression returns an accurate estimate from a near-optimal number of
measurements given that we have access to an anchor matrix of sufficient
quality. We also show how to create such an anchor in the phaseless blind
deconvolution problem again from an optimal number of measurements and
present a partial result in this direction for the general rank problem.
",1,0,0
HydraText: Multi-objective Optimization for Adversarial Textual Attack,"  The field of adversarial textual attack has significantly grown over the last
few years where the commonly considered objective is to craft adversarial
examples (AEs) that can successfully fool the target model. However the
imperceptibility of attacks which is also an essential objective for practical
attackers is often left out by previous studies. In consequence the crafted
AEs tend to have obvious structural and semantic differences from the original
human-written texts making them easily perceptible. In this paper we advocate
simultaneously considering both objectives of successful and imperceptible
attacks. Specifically we formulate the problem of crafting AEs as a
multi-objective set maximization problem and propose a novel evolutionary
algorithm (dubbed HydraText) to solve it. To the best of our knowledge
HydraText is currently the only approach that can be effectively applied to
both score-based and decision-based attack settings. Exhaustive experiments
involving 44237 instances demonstrate that HydraText consistently achieves
higher attack success rates and better attack imperceptibility than the
state-of-the-art textual attack approaches. A human evaluation study also shows
that the AEs crafted by HydraText are more indistinguishable from human-written
texts. Finally these AEs exhibit good transferability and can bring notable
robustness improvement to the target models by adversarial training.
",0,1,0
"Energy-Reliability Limits in Nanoscale Feedforward Neural Networks and
  Formulas","  Due to energy-efficiency requirements computational systems are now being
implemented using noisy nanoscale semiconductor devices whose reliability
depends on energy consumed. We study circuit-level energy-reliability limits
for deep feedforward neural networks (multilayer perceptrons) built using such
devices and en route also establish the same limits for formulas (boolean
tree-structured circuits). To obtain energy lower bounds we extend Pippenger's
mutual information propagation technique for characterizing the complexity of
noisy circuits since small circuit complexity need not imply low energy. Many
device technologies require all gates to have the same electrical operating
point; in circuits of such uniform gates we show that the minimum energy
required to achieve any non-trivial reliability scales superlinearly with the
number of inputs. Circuits implemented in emerging device technologies like
spin electronics can however have gates operate at different electrical
points; in circuits of such heterogeneous gates we show energy scaling can be
linear in the number of inputs. Building on our extended mutual information
propagation technique and using crucial insights from convex optimization
theory we develop an algorithm to compute energy lower bounds for any given
boolean tree under heterogeneous gates. This algorithm runs in linear time in
number of gates and is therefore practical for modern circuit design. As part
of our development we find a simple procedure for energy allocation across
circuit gates with different operating points and neural networks with
differently-operating layers.
",1,0,0
Rethinking Differentiable Search for Mixed-Precision Neural Networks,"  Low-precision networks with weights and activations quantized to low
bit-width are widely used to accelerate inference on edge devices. However
current solutions are uniform using identical bit-width for all filters. This
fails to account for the different sensitivities of different filters and is
suboptimal. Mixed-precision networks address this problem by tuning the
bit-width to individual filter requirements. In this work the problem of
optimal mixed-precision network search (MPS) is considered. To circumvent its
difficulties of discrete search space and combinatorial optimization a new
differentiable search architecture is proposed with several novel
contributions to advance the efficiency by leveraging the unique properties of
the MPS problem. The resulting Efficient differentiable MIxed-Precision network
Search (EdMIPS) method is effective at finding the optimal bit allocation for
multiple popular networks and can search a large model e.g. Inception-V3
directly on ImageNet without proxy task in a reasonable amount of time. The
learned mixed-precision networks significantly outperform their uniform
counterparts.
",0,0,1
On q-ary Bent and Plateaued Functions,"  We obtain the following results. For any prime $q$ the minimal Hamming
distance between distinct regular $q$-ary bent functions of $2n$ variables is
equal to $q^n$. The number of $q$-ary regular bent functions at the distance
$q^n$ from the quadratic bent function $Q_n=x_1x_2+dots+x_2n-1x_2n$ is
equal to $q^n(q^n-1+1)cdots(q+1)(q-1)$ for $q>2$. The Hamming distance
between distinct binary $s$-plateaued functions of $n$ variables is not less
than $2^fracs+n-22$ and the Hamming distance between distinctternary
$s$-plateaued functions of $n$ variables is not less than
$3^fracs+n-12$. These bounds are tight.
  For $q=3$ we prove an upper bound on nonlinearity of ternary functions in
terms of their correlation immunity. Moreover functions reaching this bound
are plateaued. For $q=2$ analogous result are well known but for large $q$ it
seems impossible. Constructions and some properties of $q$-ary plateaued
functions are discussed.
",1,0,0
Uniform Sampling over Episode Difficulty,"  Episodic training is a core ingredient of few-shot learning to train models
on tasks with limited labelled data. Despite its success episodic training
remains largely understudied prompting us to ask the question: what is the
best way to sample episodes? In this paper we first propose a method to
approximate episode sampling distributions based on their difficulty. Building
on this method we perform an extensive analysis and find that sampling
uniformly over episode difficulty outperforms other sampling schemes including
curriculum and easy-/hard-mining. As the proposed sampling method is algorithm
agnostic we can leverage these insights to improve few-shot learning
accuracies across many episodic training algorithms. We demonstrate the
efficacy of our method across popular few-shot learning datasets algorithms
network architectures and protocols.
",0,0,1
"CycAs: Self-supervised Cycle Association for Learning Re-identifiable
  Descriptions","  This paper proposes a self-supervised learning method for the person
re-identification (re-ID) problem where existing unsupervised methods usually
rely on pseudo labels such as those from video tracklets or clustering. A
potential drawback of using pseudo labels is that errors may accumulate and it
is challenging to estimate the number of pseudo IDs. We introduce a different
unsupervised method that allows us to learn pedestrian embeddings from raw
videos without resorting to pseudo labels. The goal is to construct a
self-supervised pretext task that matches the person re-ID objective. Inspired
by the emphdata association concept in multi-object tracking we propose the
textbfCycle textbfAssociation (textbfCycAs) task: after performing
data association between a pair of video frames forward and then backward a
pedestrian instance is supposed to be associated to itself. To fulfill this
goal the model must learn a meaningful representation that can well describe
correspondences between instances in frame pairs. We adapt the discrete
association process to a differentiable form such that end-to-end training
becomes feasible. Experiments are conducted in two aspects: We first compare
our method with existing unsupervised re-ID methods on seven benchmarks and
demonstrate CycAs' superiority. Then to further validate the practical value
of CycAs in real-world applications we perform training on self-collected
videos and report promising performance on standard test sets.
",0,0,1
"Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue
  Generation","  With increasing information from social media there are more and more videos
available. Therefore the ability to reason on a video is important and
deserves to be discussed. TheDialog System Technology Challenge (DSTC7)
(Yoshino et al. 2018) proposed an Audio Visual Scene-aware Dialog (AVSD) task
which contains five modalities including video dialogue history summary and
caption as a scene-aware environment. In this paper we propose the
entropy-enhanced dynamic memory network (DMN) to effectively model video
modality. The attention-based GRU in the proposed model can improve the model's
ability to comprehend and memorize sequential information. The entropy
mechanism can control the attention distribution higher so each to-be-answered
question can focus more specifically on a small set of video segments. After
the entropy-enhanced DMN secures the video context we apply an attention model
that in-corporates summary and caption to generate an accurate answer given the
question about the video. In the official evaluation our system can achieve
improved performance against the released baseline model for both subjective
and objective evaluation metrics.
",0,1,0
"Partially-Supervised Novel Object Captioning Leveraging Context from
  Paired Data","  In this paper we propose an approach to improve image captioning solution
for images with novel objects that do not have caption labels in the training
dataset. We refer to our approach as Partially-Supervised Novel Object
Captioning (PS-NOC). PS-NOC is agnostic to model architecture and primarily
focuses on the training approach that uses existing fully paired image-caption
data and the images with only the novel object detection labels (partially
paired data). We create synthetic paired captioning data for novel objects by
leveraging context from existing image-caption pairs. We then create
pseudo-label captions for partially paired images with novel objects and use
this additional data to fine-tune the captioning model. We also propose a
variant of SCST within PS-NOC called SCST-F1 that directly optimizes the
F1-score of novel objects. Using a popular captioning model (Up-Down) as
baseline PS-NOC sets new state-of-the-art results on held-out MS COCO
out-of-domain test split i.e. 85.9 F1-score and 103.8 CIDEr. This is an
improvement of 85.9 and 34.1 points respectively compared to baseline model
that does not use partially paired data during training. We also perform
detailed ablation studies to demonstrate the effectiveness of our approach.
",0,1,0
Augmented Shortcuts for Vision Transformers,"  Transformer models have achieved great progress on computer vision tasks
recently. The rapid development of vision transformers is mainly contributed by
their high representation ability for extracting informative features from
input images. However the mainstream transformer models are designed with deep
architectures and the feature diversity will be continuously reduced as the
depth increases i.e. feature collapse. In this paper we theoretically
analyze the feature collapse phenomenon and study the relationship between
shortcuts and feature diversity in these transformer models. Then we present
an augmented shortcut scheme which inserts additional paths with learnable
parameters in parallel on the original shortcuts. To save the computational
costs we further explore an efficient approach that uses the block-circulant
projection to implement augmented shortcuts. Extensive experiments conducted on
benchmark datasets demonstrate the effectiveness of the proposed method which
brings about 1% accuracy increase of the state-of-the-art visual transformers
without obviously increasing their parameters and FLOPs.
",0,0,1
"Constacyclic codes of length $p^sn$ over
  $mathbbF_p^m+umathbbF_p^m$","  Let $mathbbF_p^m$ be a finite field of cardinality $p^m$ and
$R=mathbbF_p^m[u]/langle u^2rangle=mathbbF_p^m+umathbbF_p^m$
$(u^2=0)$ where $p$ is a prime and $m$ is a positive integer. For any
$lambdain mathbbF_p^m^times$ an explicit representation for all
distinct $lambda$-constacyclic codes over $R$ of length $p^sn$ is given by a
canonical form decomposition for each code where $s$ and $n$ are positive
integers satisfying $rm gcd(pn)=1$. For any such code using its canonical
form decomposition the representation for the dual code of the code is
provided. Moreover representations for all distinct negacyclic codes and their
dual codes of length $p^sn$ over $R$ are obtained and self-duality for these
codes are determined. Finally all distinct self-dual negacyclic codes over
$mathbbF_5+umathbbF_5$ of length $2cdot 5^scdot 3^t$ are listed for any
positive integer $t$.
",1,0,0
Unsupervised Fusion Weight Learning in Multiple Classifier Systems,"  In this paper we present an unsupervised method to learn the weights with
which the scores of multiple classifiers must be combined in classifier fusion
settings. We also introduce a novel metric for ranking instances based on an
index which depends upon the rank of weighted scores of test points among the
weighted scores of training points. We show that the optimized index can be
used for computing measures such as average precision. Unlike most classifier
fusion methods where a single weight is learned to weigh all examples our
method learns instance-specific weights. The problem is formulated as learning
the weight which maximizes a clarity index; subsequently the index itself and
the learned weights both are used separately to rank all the test points. Our
method gives an unsupervised method of optimizing performance on actual test
data unlike the well known stacking-based methods where optimization is done
over a labeled training set. Moreover we show that our method is tolerant to
noisy classifiers and can be used for selecting N-best classifiers.
",0,0,1
BINAS: Bilinear Interpretable Neural Architecture Search,"  Practical use of neural networks often involves requirements on latency
energy and memory among others. A popular approach to find networks under such
requirements is through constrained Neural Architecture Search (NAS). However
previous methods use complicated predictors for the accuracy of the network.
Those predictors are hard to interpret and sensitive to many hyperparameters to
be tuned hence the resulting accuracy of the generated models is often
harmed. In this work we resolve this by introducing Bilinear Interpretable
Neural Architecture Search (BINAS) that is based on an accurate and simple
bilinear formulation of both an accuracy estimator and the expected resource
requirement together with a scalable search method with theoretical
guarantees. The simplicity of our proposed estimator together with the
intuitive way it is constructed bring interpretability through many insights
about the contribution of different design choices. For example we find that
in the examined search space adding depth and width is more effective at
deeper stages of the network and at the beginning of each resolution stage. Our
experiments show that BINAS generates comparable to or better architectures
than other state-of-the-art NAS methods within a reduced marginal search cost
while strictly satisfying the resource constraints.
",0,0,1
"Learnable Graph Matching: Incorporating Graph Partitioning with Deep
  Feature Learning for Multiple Object Tracking","  Data association across frames is at the core of Multiple Object Tracking
(MOT) task. This problem is usually solved by a traditional graph-based
optimization or directly learned via deep learning. Despite their popularity
we find some points worth studying in current paradigm: 1) Existing methods
mostly ignore the context information among tracklets and intra-frame
detections which makes the tracker hard to survive in challenging cases like
severe occlusion. 2) The end-to-end association methods solely rely on the data
fitting power of deep neural networks while they hardly utilize the advantage
of optimization-based assignment methods. 3) The graph-based optimization
methods mostly utilize a separate neural network to extract features which
brings the inconsistency between training and inference. Therefore in this
paper we propose a novel learnable graph matching method to address these
issues. Briefly speaking we model the relationships between tracklets and the
intra-frame detections as a general undirected graph. Then the association
problem turns into a general graph matching between tracklet graph and
detection graph. Furthermore to make the optimization end-to-end
differentiable we relax the original graph matching into continuous quadratic
programming and then incorporate the training of it into a deep graph network
with the help of the implicit function theorem. Lastly our method GMTracker
achieves state-of-the-art performance on several standard MOT datasets. Our
code will be available at https://github.com/jiaweihe1996/GMTracker .
",0,0,1
"Learning from Event Cameras with Sparse Spiking Convolutional Neural
  Networks","  Convolutional neural networks (CNNs) are now the de facto solution for
computer vision problems thanks to their impressive results and ease of
learning. These networks are composed of layers of connected units called
artificial neurons loosely modeling the neurons in a biological brain.
However their implementation on conventional hardware (CPU/GPU) results in
high power consumption making their integration on embedded systems difficult.
In a car for example embedded algorithms have very high constraints in term of
energy latency and accuracy. To design more efficient computer vision
algorithms we propose to follow an end-to-end biologically inspired approach
using event cameras and spiking neural networks (SNNs). Event cameras output
asynchronous and sparse events providing an incredibly efficient data source
but processing these events with synchronous and dense algorithms such as CNNs
does not yield any significant benefits. To address this limitation we use
spiking neural networks (SNNs) which are more biologically realistic neural
networks where units communicate using discrete spikes. Due to the nature of
their operations they are hardware friendly and energy-efficient but training
them still remains a challenge. Our method enables the training of sparse
spiking convolutional neural networks directly on event data using the popular
deep learning framework PyTorch. The performances in terms of accuracy
sparsity and training time on the popular DVS128 Gesture Dataset make it
possible to use this bio-inspired approach for the future embedding of
real-time applications on low-power neuromorphic hardware.
",0,0,1
Stall Pattern Avoidance in Polynomial Product Codes,"  Product codes are a concatenated error-correction scheme that has been often
considered for applications requiring very low bit-error rates which demand
that the error floor be decreased as much as possible. In this work we
consider product codes constructed from polynomial algebraic codes and propose
a novel low-complexity post-processing technique that is able to improve the
error-correction performance by orders of magnitude. We provide lower bounds
for the error rate achievable under post processing and present simulation
results indicating that these bounds are tight.
",1,0,0
A robust l_1 penalized DOA estimator,"  The SPS-LASSO has recently been introduced as a solution to the problem of
regularization parameter selection in the complex-valued LASSO problem. Still
the dependence on the grid size and the polynomial time of performing convex
optimization technique in each iteration in addition to the deficiencies in
the low noise regime confines its performance for Direction of Arrival (DOA)
estimation. This work presents methods to apply LASSO without grid size
limitation and with less complexity. As we show by simulations the proposed
methods loose a negligible performance compared to the Maximum Likelihood (ML)
estimator which needs a combinatorial search We also show by simulations that
compared to practical implementations of ML the proposed techniques are less
sensitive to the source power difference.
",1,0,0
Matching-based Spectrum Allocation in Cognitive Radio Networks,"  In this paper a novel spectrum association approach for cognitive radio
networks (CRNs) is proposed. Based on a measure of both inference and
confidence as well as on a measure of quality-of-service the association
between secondary users (SUs) in the network and frequency bands licensed to
primary users (PUs) is investigated. The problem is formulated as a matching
game between SUs and PUs. In this game SUs employ a soft-decision Bayesian
framework to detect PUs' signals and eventually rank them based on the
logarithm of the a posteriori ratio. A performance measure that captures both
the ranking metric and rate is further computed by the SUs. Using this
performance measure a PU evaluates its own utility function that it uses to
build its own association preferences. A distributed algorithm that allows both
SUs and PUs to interact and self-organize into a stable match is proposed.
Simulation results show that the proposed algorithm can improve the sum of SUs'
rates by up to 20 % and 60 % relative to the deferred acceptance algorithm and
random channel allocation approach respectively. The results also show an
improved convergence time.
",1,0,0
"New self-dual additive $mathbbF_4$-codes constructed from circulant
  graphs","  In order to construct quantum $[[n0d]]$ codes for $(nd)=(5615)$
$(5715)$ $(5816)$ $(6316)$ $(6717)$ $(7018)$ $(7118)$ $(7919)$
$(8320)$ $(8720)$ $(8921)$ $(9520)$ we construct self-dual additive
$mathbbF_4$-codes of length $n$ and minimum weight $d$ from circulant
graphs. The quantum codes with these parameters are constructed for the first
time.
",1,0,0
Novel LDPC Decoder via MLP Neural Networks,"  In this paper a new method for decoding Low Density Parity Check (LDPC)
codes based on Multi-Layer Perceptron (MLP) neural networks is proposed. Due
to the fact that in neural networks all procedures are processed in parallel
this method can be considered as a viable alternative to Message Passing
Algorithm (MPA) with high computational complexity. Our proposed algorithm
runs with soft criterion and concurrently does not use probabilistic quantities
to decide what the estimated codeword is. Although the neural decoder
performance is close to the error performance of Sum Product Algorithm (SPA)
it is comparatively less complex. Therefore the proposed decoder emerges as a
new infrastructure for decoding LDPC codes.
",1,0,0
"ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing
  System","  Given the increasingly serious air pollution problem the monitoring of air
quality index (AQI) in urban areas has drawn considerable attention. This paper
presents ImgSensingNet a vision guided aerial-ground sensing system for
fine-grained air quality monitoring and forecasting using the fusion of haze
images taken by the unmanned-aerial-vehicle (UAV) and the AQI data collected by
an on-ground three-dimensional (3D) wireless sensor network (WSN).
Specifically ImgSensingNet first leverages the computer vision technique to
tell the AQI scale in different regions from the taken haze images where
haze-relevant features and a deep convolutional neural network (CNN) are
designed for direct learning between haze images and corresponding AQI scale.
Based on the learnt AQI scale ImgSensingNet determines whether to wake up
on-ground wireless sensors for small-scale AQI monitoring and inference which
can greatly reduce the energy consumption of the system. An entropy-based model
is employed for accurate real-time AQI inference at unmeasured locations and
future air quality distribution forecasting. We implement and evaluate
ImgSensingNet on two university campuses since Feb. 2018 and has collected
17630 photos and 2.6 millions of AQI data samples. Experimental results
confirm that ImgSensingNet can achieve higher inference accuracy while greatly
reduce the energy consumption compared to state-of-the-art AQI monitoring
approaches.
",0,0,1
"The Cinderella Complex: Word Embeddings Reveal Gender Stereotypes in
  Movies and Books","  Our analysis of thousands of movies and books reveals how these cultural
products weave stereotypical gender roles into morality tales and perpetuate
gender inequality through storytelling. Using the word embedding techniques we
reveal the constructed emotional dependency of female characters on male
characters in stories.
",0,1,0
"Style is NOT a single variable: Case Studies for Cross-Style Language
  Understanding","  Every natural text is written in some style. Style is formed by a complex
combination of different stylistic factors including formality markers
emotions metaphors etc. One cannot form a complete understanding of a text
without considering these factors. The factors combine and co-vary in complex
ways to form styles. Studying the nature of the co-varying combinations sheds
light on stylistic language in general sometimes called cross-style language
understanding. This paper provides the benchmark corpus (xSLUE) that combines
existing datasets and collects a new one for sentence-level cross-style
language understanding and evaluation. The benchmark contains text in 15
different styles under the proposed four theoretical groupings: figurative
personal affective and interpersonal groups. For valid evaluation we collect
an additional diagnostic set by annotating all 15 styles on the same text.
Using xSLUE we propose three interesting cross-style applications in
classification correlation and generation. First our proposed cross-style
classifier trained with multiple styles together helps improve overall
classification performance against individually-trained style classifiers.
Second our study shows that some styles are highly dependent on each other in
human-written text. Finally we find that combinations of some contradictive
styles likely generate stylistically less appropriate text. We believe our
benchmark and case studies help explore interesting future directions for
cross-style research. The preprocessed datasets and code are publicly
available.
",0,1,0
"Bounds on the Rate of Linear Locally Repairable Codes over Small
  Alphabets","  Locally repairable codes (LRC) have recently been a subject of intense
research due to theoretical appeal and their application in distributed storage
systems. In an LRC any coordinate of a codeword can be recovered by accessing
only few other coordinates. For LRCs over small alphabet (such as binary) the
optimal rate-distance trade-off is unknown. In this paper we provide the
tightest known upper bound on the rate of linear LRCs of a given relative
distance an improvement over any previous result in particular
citecadambe2013upper.
",1,0,0
"Hybrid Consistency Training with Prototype Adaptation for Few-Shot
  Learning","  Few-Shot Learning (FSL) aims to improve a model's generalization capability
in low data regimes. Recent FSL works have made steady progress via metric
learning meta learning representation learning etc. However FSL remains
challenging due to the following longstanding difficulties. 1) The seen and
unseen classes are disjoint resulting in a distribution shift between training
and testing. 2) During testing labeled data of previously unseen classes is
sparse making it difficult to reliably extrapolate from labeled support
examples to unlabeled query examples. To tackle the first challenge we
introduce Hybrid Consistency Training to jointly leverage interpolation
consistency including interpolating hidden features that imposes linear
behavior locally and data augmentation consistency that learns robust
embeddings against sample variations. As for the second challenge we use
unlabeled examples to iteratively normalize features and adapt prototypes as
opposed to commonly used one-time update for more reliable prototype-based
transductive inference. We show that our method generates a 2% to 5%
improvement over the state-of-the-art methods with similar backbones on five
FSL datasets and more notably a 7% to 8% improvement for more challenging
cross-domain FSL.
",0,0,1
"Deterministic and Randomized Diffusion based Iterative Generalized Hard
  Thresholding (DiFIGHT) for Distributed Sparse Signal Recovery","  In this paper we propose a distributed iterated hard thresholding algorithm
termed DiFIGHT over a network that is built on the diffusion mechanism and also
propose a modification of the proposed algorithm termed MoDiFIGHT that has
low complexity in terms of communication in the network. We additionally
propose four different strategies termed RP RNP RGP$_r$ and RGNP$_r$ that
are used to randomly select a subset of nodes that are subsequently activated
to take part in the distributed algorithm so as to reduce the mean number of
communications during the run of the distributed algorithm. We present
theoretical estimates of the long run communication per unit time for these
different strategies when used by the two proposed algorithms. Also we
present analysis of the two proposed algorithms and provide provable bounds on
their recovery performance with or without using the random node selection
strategies. Finally we use numerical studies to show that both when the random
strategies are used as well as when they are not used the proposed algorithms
display performances far superior to distributed IHT algorithm using consensus
mechanism .
",1,0,0
"Learning Neural Models for Natural Language Processing in the Face of
  Distributional Shift","  The dominating NLP paradigm of training a strong neural predictor to perform
one task on a specific dataset has led to state-of-the-art performance in a
variety of applications (eg. sentiment classification span-prediction based
question answering or machine translation). However it builds upon the
assumption that the data distribution is stationary ie. that the data is
sampled from a fixed distribution both at training and test time. This way of
training is inconsistent with how we as humans are able to learn from and
operate within a constantly changing stream of information. Moreover it is
ill-adapted to real-world use cases where the data distribution is expected to
shift over the course of a model's lifetime.
  The first goal of this thesis is to characterize the different forms this
shift can take in the context of natural language processing and propose
benchmarks and evaluation metrics to measure its effect on current deep
learning architectures. We then proceed to take steps to mitigate the effect of
distributional shift on NLP models. To this end we develop methods based on
parametric reformulations of the distributionally robust optimization
framework. Empirically we demonstrate that these approaches yield more robust
models as demonstrated on a selection of realistic problems. In the third and
final part of this thesis we explore ways of efficiently adapting existing
models to new domains or tasks. Our contribution to this topic takes
inspiration from information geometry to derive a new gradient update rule
which alleviate catastrophic forgetting issues during adaptation.
",0,1,0
"How Useful is Region-based Classification of Remote Sensing Images in a
  Deep Learning Framework?","  In this paper we investigate the impact of segmentation algorithms as a
preprocessing step for classification of remote sensing images in a deep
learning framework. Especially we address the issue of segmenting the image
into regions to be classified using pre-trained deep neural networks as feature
extractors for an SVM-based classifier. An efficient segmentation as a
preprocessing step helps learning by adding a spatially-coherent structure to
the data. Therefore we compare algorithms producing superpixels with more
traditional remote sensing segmentation algorithms and measure the variation in
terms of classification accuracy. We establish that superpixel algorithms allow
for a better classification accuracy as a homogenous and compact segmentation
favors better generalization of the training samples.
",0,0,1
What do Models Learn from Question Answering Datasets?,"  While models have reached superhuman performance on popular question
answering (QA) datasets such as SQuAD they have yet to outperform humans on
the task of question answering itself. In this paper we investigate if models
are learning reading comprehension from QA datasets by evaluating BERT-based
models across five datasets. We evaluate models on their generalizability to
out-of-domain examples responses to missing or incorrect data and ability to
handle question variations. We find that no single dataset is robust to all of
our experiments and identify shortcomings in both datasets and evaluation
methods. Following our analysis we make recommendations for building future QA
datasets that better evaluate the task of question answering through reading
comprehension. We also release code to convert QA datasets to a shared format
for easier experimentation at
https://github.com/amazon-research/qa-dataset-converter.
",0,1,0
Near-Field Perturbation Effect on Constellation Error in Beam-Space MIMO,"  Beam-space MIMO has recently been proposed as a promising solution to enable
transmitting multiple data streams using a single RF chain and a single
pattern-reconfigurable antenna. Since in a beam-space MIMO system radiation
pattern of the transmit antenna is exploited as extra dimension for encoding
information near-field interaction of the transmit antenna with its
surrounding objects affects spatial multiplexing performance of the system.
Through numerical simulations in the previous work it has been concluded that
under BPSK signaling beam-space MIMO is not more vulnerable to near-field
coupling than its conventional counterpart. In this work we extend the study
to the case of higher-order modulation schemes where the presence of external
perturbation also affects the data constellation points transmitted by a
beam-space MIMO antenna. To this aim the error vector magnitude of the
transmitted signal is evaluated when placing a QPSK beam-space MIMO antenna in
close proximity to a hand model of the user. The obtained results emphasize the
importance of reconsidering the decoding approach for beam-space MIMO systems
in practical applications.
",1,0,0
TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval,"  We introduce TV show Retrieval (TVR) a new multimodal retrieval dataset. TVR
requires systems to understand both videos and their associated subtitle
(dialogue) texts making it more realistic. The dataset contains 109K queries
collected on 21.8K videos from 6 TV shows of diverse genres where each query
is associated with a tight temporal window. The queries are also labeled with
query types that indicate whether each of them is more related to video or
subtitle or both allowing for in-depth analysis of the dataset and the methods
that built on top of it. Strict qualification and post-annotation verification
tests are applied to ensure the quality of the collected data. Further we
present several baselines and a novel Cross-modal Moment Localization (XML )
network for multimodal moment retrieval tasks. The proposed XML model uses a
late fusion design with a novel Convolutional Start-End detector (ConvSE)
surpassing baselines by a large margin and with better efficiency providing a
strong starting point for future work. We have also collected additional
descriptions for each annotated moment in TVR to form a new multimodal
captioning dataset with 262K captions named TV show Caption (TVC). Both
datasets are publicly available. TVR: https://tvr.cs.unc.edu TVC:
https://tvr.cs.unc.edu/tvc.html.
",0,0,1
"Improving the Performance of English-Tamil Statistical Machine
  Translation System using Source-Side Pre-Processing","  Machine Translation is one of the major oldest and the most active research
area in Natural Language Processing. Currently Statistical Machine Translation
(SMT) dominates the Machine Translation research. Statistical Machine
Translation is an approach to Machine Translation which uses models to learn
translation patterns directly from data and generalize them to translate a new
unseen text. The SMT approach is largely language independent i.e. the models
can be applied to any language pair. Statistical Machine Translation (SMT)
attempts to generate translations using statistical methods based on bilingual
text corpora. Where such corpora are available excellent results can be
attained translating similar texts but such corpora are still not available
for many language pairs. Statistical Machine Translation systems in general
have difficulty in handling the morphology on the source or the target side
especially for morphologically rich languages. Errors in morphology or syntax
in the target language can have severe consequences on meaning of the sentence.
They change the grammatical function of words or the understanding of the
sentence through the incorrect tense information in verb. Baseline SMT also
known as Phrase Based Statistical Machine Translation (PBSMT) system does not
use any linguistic information and it only operates on surface word form.
Recent researches shown that adding linguistic information helps to improve the
accuracy of the translation with less amount of bilingual corpora. Adding
linguistic information can be done using the Factored Statistical Machine
Translation system through pre-processing steps. This paper investigates about
how English side pre-processing is used to improve the accuracy of
English-Tamil SMT system.
",0,1,0
"Stochastic Control of Event-Driven Feedback in Multi-Antenna
  Interference Channels","  Spatial interference avoidance is a simple and effective way of mitigating
interference in multi-antenna wireless networks. The deployment of this
technique requires channel-state information (CSI) feedback from each receiver
to all interferers resulting in substantial network overhead. To address this
issue this paper proposes the method of distributive control that
intelligently allocates CSI bits over multiple feedback links and adapts
feedback to channel dynamics. For symmetric channel distributions it is
optimal for each receiver to equally allocate the average sum-feedback rate for
different feedback links thereby decoupling their control. Using the criterion
of minimum sum-interference power the optimal feedback-control policy is shown
using stochastic-optimization theory to exhibit opportunism. Specifically a
specific feedback link is turned on only when the corresponding transmit-CSI
error is significant or interference-channel gain large and the optimal number
of feedback bits increases with this gain. For high mobility and considering
the sphere-cap-quantized-CSI model the optimal feedback-control policy is
shown to perform water-filling in time where the number of feedback bits
increases logarithmically with the corresponding interference-channel gain.
Furthermore we consider asymmetric channel distributions with heterogeneous
path losses and high mobility and prove the existence of a unique optimal
policy for jointly controlling multiple feedback links. Given the
sphere-cap-quantized-CSI model this policy is shown to perform water-filling
over feedback links. Finally simulation demonstrates that feedback-control
yields significant throughput gains compared with the conventional
differential-feedback method.
",1,0,0
"Robust cDNA microarray image segmentation and analysis technique based
  on Hough circle transform","  One of the most challenging tasks in microarray image analysis is spot
segmentation. A solution to this problem is to provide an algorithm than can be
used to find any spot within the microarray image. Circular Hough
Transformation (CHT) is a powerful feature extraction technique used in image
analysis computer vision and digital image processing. CHT algorithm is
applied on the cDNA microarray images to develop the accuracy and the
efficiency of the spots localization addressing and segmentation process. The
purpose of the applied technique is to find imperfect instances of spots within
a certain class of circles by applying a voting procedure on the cDNA
microarray images for spots localization addressing and characterizing the
pixels of each spot into foreground pixels and background simultaneously.
Intensive experiments on the University of North Carolina (UNC) microarray
database indicate that the proposed method is superior to the K-means method
and the Support vector machine (SVM). Keywords: Hough circle transformation
cDNA microarray image analysis cDNA microarray image segmentation spots
localization and addressing spots segmentation
",0,0,1
"Learning Uncertain Convolutional Features for Accurate Saliency
  Detection","  Deep convolutional neural networks (CNNs) have delivered superior performance
in many computer vision tasks. In this paper we propose a novel deep fully
convolutional network model for accurate salient object detection. The key
contribution of this work is to learn deep uncertain convolutional features
(UCF) which encourage the robustness and accuracy of saliency detection. We
achieve this via introducing a reformulated dropout (R-dropout) after specific
convolutional layers to construct an uncertain ensemble of internal feature
units. In addition we propose an effective hybrid upsampling method to reduce
the checkerboard artifacts of deconvolution operators in our decoder network.
The proposed methods can also be applied to other deep convolutional networks.
Compared with existing saliency detection methods the proposed UCF model is
able to incorporate uncertainties for more accurate object boundary inference.
Extensive experiments demonstrate that our proposed saliency model performs
favorably against state-of-the-art approaches. The uncertain feature learning
mechanism as well as the upsampling method can significantly improve
performance on other pixel-wise vision tasks.
",0,0,1
Compressive Sampling of Ensembles of Correlated Signals,"  We propose several sampling architectures for the efficient acquisition of an
ensemble of correlated signals. We show that without prior knowledge of the
correlation structure each of our architectures (under different sets of
assumptions) can acquire the ensemble at a sub-Nyquist rate. Prior to sampling
the analog signals are diversified using simple implementable components. The
diversification is achieved by injecting types of ""structured randomness"" into
the ensemble the result of which is subsampled. For reconstruction the
ensemble is modeled as a low-rank matrix that we have observed through an
(undetermined) set of linear equations. Our main results show that this matrix
can be recovered using standard convex programming techniques when the total
number of samples is on the order of the intrinsic degree of freedom of the
ensemble --- the more heavily correlated the ensemble the fewer samples are
needed.
  To motivate this study we discuss how such ensembles arise in the context of
array processing.
",1,0,0
"MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed
  Images","  Semantic segmentation of remotely sensed images plays an important role in
land resource management yield estimation and economic assessment. U-Net a
deep encoder-decoder architecture has been used frequently for image
segmentation with high accuracy. In this Letter we incorporate multi-scale
features generated by different layers of U-Net and design a multi-scale skip
connected and asymmetric-convolution-based U-Net (MACU-Net) for segmentation
using fine-resolution remotely sensed images. Our design has the following
advantages: (1) The multi-scale skip connections combine and realign semantic
features contained in both low-level and high-level feature maps; (2) the
asymmetric convolution block strengthens the feature representation and feature
extraction capability of a standard convolution layer. Experiments conducted on
two remotely sensed datasets captured by different satellite sensors
demonstrate that the proposed MACU-Net transcends the U-Net U-NetPPL U-Net
3+ amongst other benchmark approaches. Code is available at
https://github.com/lironui/MACU-Net.
",0,0,1
"How to Increase the Achievable Information Rate by Per-Channel
  Dispersion Compensation","  Deploying periodic inline chromatic dispersion compensation enables reducing
the complexity of the digital back propagation (DBP) algorithm. However
compared with nondispersion-managed (NDM) links dispersion-managed (DM) ones
suffer a stronger cross-phase modulation (XPM). Utilizing per-channel
dispersion-managed (CDM) links (e.g. using fiber Bragg grating) allows for a
complexity reduction of DBP while abating XPM compared to DM links. In this
paper we show for the first time that CDM links enable also a more effective
XPM compensation compared to NDM ones allowing a higher achievable information
rate (AIR). This is explained by resorting to the frequency-resolved
logarithmic perturbation model and showing that per-channel dispersion
compensation increases the frequency correlation of the distortions induced by
XPM over the channel bandwidth making them more similar to a conventional
phase noise. We compare the performance (in terms of the AIR) of a DM an NDM
and a CDM link considering two types of mismatched receivers: one neglects the
XPM phase distortion and the other compensates for it. With the former the CDM
link is inferior to the NDM one due to an increased in-band signal--noise
interaction. However with the latter a higher AIR is obtained with the CDM
link than with the NDM one owing to a higher XPM frequency correlation. The DM
link has the lowest AIR for both receivers because of a stronger XPM.
",1,0,0
"A framework for (under)specifying dependency syntax without overloading
  annotators","  We introduce a framework for lightweight dependency syntax annotation. Our
formalism builds upon the typical representation for unlabeled dependencies
permitting a simple notation and annotation workflow. Moreover the formalism
encourages annotators to underspecify parts of the syntax if doing so would
streamline the annotation process. We demonstrate the efficacy of this
annotation on three languages and develop algorithms to evaluate and compare
underspecified annotations.
",0,1,0
Visual Semantic Information Pursuit: A Survey,"  Visual semantic information comprises two important parts: the meaning of
each visual semantic unit and the coherent visual semantic relation conveyed by
these visual semantic units. Essentially the former one is a visual perception
task while the latter one corresponds to visual context reasoning. Remarkable
advances in visual perception have been achieved due to the success of deep
learning. In contrast visual semantic information pursuit a visual scene
semantic interpretation task combining visual perception and visual context
reasoning is still in its early stage. It is the core task of many different
computer vision applications such as object detection visual semantic
segmentation visual relationship detection or scene graph generation. Since it
helps to enhance the accuracy and the consistency of the resulting
interpretation visual context reasoning is often incorporated with visual
perception in current deep end-to-end visual semantic information pursuit
methods. However a comprehensive review for this exciting area is still
lacking. In this survey we present a unified theoretical paradigm for all
these methods followed by an overview of the major developments and the future
trends in each potential direction. The common benchmark datasets the
evaluation metrics and the comparisons of the corresponding methods are also
introduced.
",0,0,1
Traditional Method Inspired Deep Neural Network for Edge Detection,"  Recently Deep-Neural-Network (DNN) based edge prediction is progressing
fast. Although the DNN based schemes outperform the traditional edge detectors
they have much higher computational complexity. It could be that the DNN based
edge detectors often adopt the neural net structures designed for high-level
computer vision tasks such as image segmentation and object recognition. Edge
detection is a rather local and simple job the over-complicated architecture
and massive parameters may be unnecessary. Therefore we propose a traditional
method inspired framework to produce good edges with minimal complexity. We
simplify the network architecture to include Feature Extractor Enrichment and
Summarizer which roughly correspond to gradient low pass filter and pixel
connection in the traditional edge detection schemes. The proposed structure
can effectively reduce the complexity and retain the edge prediction quality.
Our TIN2 (Traditional Inspired Network) model has an accuracy higher than the
recent BDCN2 (Bi-Directional Cascade Network) but with a smaller model.
",0,0,1
"A robust and adaptable method for face detection based on Color
  Probabilistic Estimation Technique","  Human face perception is currently an active research area in the computer
vision community. Skin detection is one of the most important and primary
stages for this purpose. So far many approaches are proposed to done this
case. Near all of these methods have tried to find best match intensity
distribution with skin pixels based on popular color spaces such as RGB HSI or
YCBCR. Results show that these methods cannot provide an accurate approach for
every kind of skin. In this paper an approach is proposed to solve this
problem using a color probabilistic estimation technique. This approach is
including two stages. In the first one the skin intensity distribution is
estimated using some train photos of pure skin and at the second stage the
skin pixels are detected using Gaussian model and optimal threshold tuning.
Then from the skin region facial features have been extracted to get the face
from the skin region. In the results section the proposed approach is applied
on FEI database and the accuracy rate reached 99.25%. The proposed approach can
be used for all kinds of skin using train stage which is the main advantage
among the other advantages such as Low noise sensitivity and low computational
complexity.
",0,0,1
"Learning non-rigid surface reconstruction from spatio-temporal image
  patches","  We present a method to reconstruct a dense spatio-temporal depth map of a
non-rigidly deformable object directly from a video sequence. The estimation of
depth is performed locally on spatio-temporal patches of the video and then
the full depth video of the entire shape is recovered by combining them
together. Since the geometric complexity of a local spatio-temporal patch of a
deforming non-rigid object is often simple enough to be faithfully represented
with a parametric model we artificially generate a database of small deforming
rectangular meshes rendered with different material properties and light
conditions along with their corresponding depth videos and use such data to
train a convolutional neural network. We tested our method on both synthetic
and Kinect data and experimentally observed that the reconstruction error is
significantly lower than the one obtained using other approaches like
conventional non-rigid structure from motion.
",0,0,1
Information Recovery from Pairwise Measurements,"  A variety of information processing tasks in practice involve recovering $n$
objects from single-shot graph-based measurements particularly those taken
over the edges of some measurement graph $mathcalG$. This paper concerns the
situation where each object takes value over a group of $M$ different values
and where one is interested to recover all these values based on observations
of certain pairwise relations over $mathcalG$. The imperfection of
measurements presents two major challenges for information recovery: 1)
$textitinaccuracy$: a (dominant) portion $1-p$ of measurements are
corrupted; 2) $textitincompleteness$: a significant fraction of pairs are
unobservable i.e. $mathcalG$ can be highly sparse.
  Under a natural random outlier model we characterize the $textitminimax
recovery rate$ that is the critical threshold of non-corruption rate $p$
below which exact information recovery is infeasible. This accommodates a very
general class of pairwise relations. For various homogeneous random graph
models (e.g. Erdos Renyi random graphs random geometric graphs small world
graphs) the minimax recovery rate depends almost exclusively on the edge
sparsity of the measurement graph $mathcalG$ irrespective of other graphical
metrics. This fundamental limit decays with the group size $M$ at a square root
rate before entering a connectivity-limited regime. Under the Erdos Renyi
random graph a tractable combinatorial algorithm is proposed to approach the
limit for large $M$ ($M=n^Omega(1)$) while order-optimal recovery is
enabled by semidefinite programs in the small $M$ regime.
  The extended (and most updated) version of this work can be found at
(http://arxiv.org/abs/1504.01369).
",1,0,0
"A Comprehensive Study of Class Incremental Learning Algorithms for
  Visual Tasks","  The ability of artificial agents to increment their capabilities when
confronted with new data is an open challenge in artificial intelligence. The
main challenge faced in such cases is catastrophic forgetting i.e. the
tendency of neural networks to underfit past data when new ones are ingested. A
first group of approaches tackles forgetting by increasing deep model capacity
to accommodate new knowledge. A second type of approaches fix the deep model
size and introduce a mechanism whose objective is to ensure a good compromise
between stability and plasticity of the model. While the first type of
algorithms were compared thoroughly this is not the case for methods which
exploit a fixed size model. Here we focus on the latter place them in a
common conceptual and experimental framework and propose the following
contributions: (1) define six desirable properties of incremental learning
algorithms and analyze them according to these properties (2) introduce a
unified formalization of the class-incremental learning problem (3) propose a
common evaluation framework which is more thorough than existing ones in terms
of number of datasets size of datasets size of bounded memory and number of
incremental states (4) investigate the usefulness of herding for past
exemplars selection (5) provide experimental evidence that it is possible to
obtain competitive performance without the use of knowledge distillation to
tackle catastrophic forgetting and (6) facilitate reproducibility by
integrating all tested methods in a common open-source repository. The main
experimental finding is that none of the existing algorithms achieves the best
results in all evaluated settings. Important differences arise notably if a
bounded memory of past classes is allowed or not.
",0,0,1
"Iris and periocular recognition in arabian race horses using deep
  convolutional neural networks","  This paper presents a study devoted to recognizing horses by means of their
iris and periocular features using deep convolutional neural networks (DCNNs).
Identification of race horses is crucial for animal identity confirmation prior
to racing. As this is usually done shortly before a race fast and reliable
methods that are friendly and inflict no harm upon animals are important. Iris
recognition has been shown to work with horse irides provided that algorithms
deployed for such task are fine-tuned for horse irides and input data is of
very high quality. In our work we examine a possibility of utilizing deep
convolutional neural networks for a fusion of both iris and periocular region
features. With such methodology ocular biometrics in horses could perform well
without employing complicated algorithms that require a lot of fine-tuning and
prior knowledge of the input image while at the same time being rotation
translation and to some extent also image quality invariant. We were able to
achieve promising results with EER=9.5% using two network architectures with
score-level fusion.
",0,0,1
"Working with scale: 2nd place solution to Product Detection in Densely
  Packed Scenes [Technical Report]","  This report describes a 2nd place solution of the detection challenge which
is held within CVPR 2020 Retail-Vision workshop. Instead of going further
considering previous results this work mainly aims to verify previously
observed takeaways by re-experimenting. The reliability and reproducibility of
the results are reached by incorporating a popular object detection toolbox -
MMDetection. In this report I firstly represent the results received for
Faster-RCNN and RetinaNet models which were taken for comparison in the
original work. Then I describe the experiment results with more advanced
models. The final section reviews two simple tricks for Faster-RCNN model that
were used for my final submission: changing default anchor scale parameter and
train-time image tiling. The source code is available at
https://github.com/tyomj/product_detection.
",0,0,1
"Real-Time Segmentation of Non-Rigid Surgical Tools based on Deep
  Learning and Tracking","  Real-time tool segmentation is an essential component in computer-assisted
surgical systems. We propose a novel real-time automatic method based on Fully
Convolutional Networks (FCN) and optical flow tracking. Our method exploits the
ability of deep neural networks to produce accurate segmentations of highly
deformable parts along with the high speed of optical flow. Furthermore the
pre-trained FCN can be fine-tuned on a small amount of medical images without
the need to hand-craft features. We validated our method using existing and new
benchmark datasets covering both ex vivo and in vivo real clinical cases where
different surgical instruments are employed. Two versions of the method are
presented non-real-time and real-time. The former using only deep learning
achieves a balanced accuracy of 89.6% on a real clinical dataset outperforming
the (non-real-time) state of the art by 3.8% points. The latter a combination
of deep learning with optical flow tracking yields an average balanced
accuracy of 78.2% across all the validated datasets.
",0,0,1
CT Image Harmonization for Enhancing Radiomics Studies,"  While remarkable advances have been made in Computed Tomography (CT)
capturing CT images with non-standardized protocols causes low reproducibility
regarding radiomic features forming a barrier on CT image analysis in a large
scale. RadiomicGAN is developed to effectively mitigate the discrepancy caused
by using non-standard reconstruction kernels. RadiomicGAN consists of hybrid
neural blocks including both pre-trained and trainable layers adopted to learn
radiomic feature distributions efficiently. A novel training approach called
Dynamic Window-based Training has been developed to smoothly transform the
pre-trained model to the medical imaging domain. Model performance evaluated
using 1401 radiomic features show that RadiomicGAN clearly outperforms the
state-of-art image standardization models.
",0,0,1
"Collaborative Video Object Segmentation by Foreground-Background
  Integration","  This paper investigates the principles of embedding learning to tackle the
challenging semi-supervised video object segmentation. Different from previous
practices that only explore the embedding learning using pixels from foreground
object (s) we consider background should be equally treated and thus propose
Collaborative video object segmentation by Foreground-Background Integration
(CFBI) approach. Our CFBI implicitly imposes the feature embedding from the
target foreground object and its corresponding background to be contrastive
promoting the segmentation results accordingly. With the feature embedding from
both foreground and background our CFBI performs the matching process between
the reference and the predicted sequence from both pixel and instance levels
making the CFBI be robust to various object scales. We conduct extensive
experiments on three popular benchmarks i.e. DAVIS 2016 DAVIS 2017 and
YouTube-VOS. Our CFBI achieves the performance (J$F) of 89.4% 81.9% and
81.4% respectively outperforming all the other state-of-the-art methods.
Code: https://github.com/z-x-yang/CFBI.
",0,0,1
"LDC-VAE: A Latent Distribution Consistency Approach to Variational
  AutoEncoders","  Variational autoencoders (VAEs) as an important aspect of generative models
have received a lot of research interests and reached many successful
applications. However it is always a challenge to achieve the consistency
between the learned latent distribution and the prior latent distribution when
optimizing the evidence lower bound (ELBO) and finally leads to an
unsatisfactory performance in data generation. In this paper we propose a
latent distribution consistency approach to avoid such substantial
inconsistency between the posterior and prior latent distributions in ELBO
optimizing. We name our method as latent distribution consistency VAE
(LDC-VAE). We achieve this purpose by assuming the real posterior distribution
in latent space as a Gibbs form and approximating it by using our encoder.
However there is no analytical solution for such Gibbs posterior in
approximation and traditional approximation ways are time consuming such as
using the iterative sampling-based MCMC. To address this problem we use the
Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior.
Meanwhile we use the SVGD to train a sampler net which can obtain efficient
samples from the Gibbs posterior. Comparative studies on the popular image
generation datasets show that our method has achieved comparable or even better
performance than several powerful improvements of VAEs.
",0,0,1
Tag-Weighted Topic Model For Large-scale Semi-Structured Documents,"  To date there have been massive Semi-Structured Documents (SSDs) during the
evolution of the Internet. These SSDs contain both unstructured features (e.g.
plain text) and metadata (e.g. tags). Most previous works focused on modeling
the unstructured text and recently some other methods have been proposed to
model the unstructured text with specific tags. To build a general model for
SSDs remains an important problem in terms of both model fitness and
efficiency. We propose a novel method to model the SSDs by a so-called
Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the
tags and words information not only to learn the document-topic and topic-word
distributions but also to infer the tag-topic distributions for text mining
tasks. We present an efficient variational inference method with an EM
algorithm for estimating the model parameters. Meanwhile we propose three
large-scale solutions for our model under the MapReduce distributed computing
platform for modeling large-scale SSDs. The experimental results show the
effectiveness efficiency and the robustness by comparing our model with the
state-of-the-art methods in document modeling tags prediction and text
classification. We also show the performance of the three distributed solutions
in terms of time and accuracy on document modeling.
",0,1,0
"Inexact Alternating Optimization for Phase Retrieval In the Presence of
  Outliers","  Phase retrieval has been mainly considered in the presence of Gaussian noise.
However the performance of the algorithms proposed under the Gaussian noise
model severely degrades when grossly corrupted data i.e. outliers exist.
This paper investigates techniques for phase retrieval in the presence of
heavy-tailed noise -- which is considered a better model for situations where
outliers exist. An $ell_p$-norm ($0<p<2$) based estimator is proposed for
fending against such noise and two-block inexact alternating optimization is
proposed as the algorithmic framework to tackle the resulting optimization
problem. Two specific algorithms are devised by exploring different local
approximations within this framework. Interestingly the core conditional
minimization steps can be interpreted as iteratively reweighted least squares
and gradient descent. Convergence properties of the algorithms are discussed
and the Cram'er-Rao bound (CRB) is derived. Simulations demonstrate that the
proposed algorithms approach the CRB and outperform state-of-the-art algorithms
in heavy-tailed noise.
",1,0,0
False Discovery Rate Control via Debiased Lasso,"  We consider the problem of variable selection in high-dimensional statistical
models where the goal is to report a set of variables out of many predictors
$X_1 dotsc X_p$ that are relevant to a response of interest. For linear
high-dimensional model where the number of parameters exceeds the number of
samples $(p>n)$ we propose a procedure for variables selection and prove that
it controls the ""directional"" false discovery rate (FDR) below a pre-assigned
significance level $qin [01]$. We further analyze the statistical power of
our framework and show that for designs with subgaussian rows and a common
precision matrix $OmegainmathbbR^ptimes p$ if the minimum nonzero
parameter $theta_min$ satisfies $$sqrtn theta_min - sigma
sqrt2(max_iin [p]Omega_ii)logleft(frac2pqs_0right) to
infty$$ then this procedure achieves asymptotic power one. Our framework is
built upon the debiasing approach and assumes the standard condition $s_0 =
o(sqrtn/(log p)^2)$ where $s_0$ indicates the number of true positives
among the $p$ features. Notably this framework achieves exact directional FDR
control without any assumption on the amplitude of unknown regression
parameters and does not require any knowledge of the distribution of
covariates or the noise level. We test our method in synthetic and real data
experiments to assess its performance and to corroborate our theoretical
results.
",1,0,0
Refined Upper Bounds on Stopping Redundancy of Binary Linear Codes,"  The $l$-th stopping redundancy $rho_l(mathcal C)$ of the binary $[n k d]$
code $mathcal C$ $1 le l le d$ is defined as the minimum number of rows in
the parity-check matrix of $mathcal C$ such that the smallest stopping set is
of size at least $l$. The stopping redundancy $rho(mathcal C)$ is defined as
$rho_d(mathcal C)$. In this work we improve on the probabilistic analysis of
stopping redundancy proposed by Han Siegel and Vardy which yields the best
bounds known today. In our approach we judiciously select the first few rows
in the parity-check matrix and then continue with the probabilistic method. By
using similar techniques we improve also on the best known bounds on
$rho_l(mathcal C)$ for $1 le l le d$. Our approach is compared to the
existing methods by numerical computations.
",1,0,0
"MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual
  Network","  In this paper we present MultiPoseNet a novel bottom-up multi-person pose
estimation architecture that combines a multi-task model with a novel
assignment method. MultiPoseNet can jointly handle person detection keypoint
detection person segmentation and pose estimation problems. The novel
assignment method is implemented by the Pose Residual Network (PRN) which
receives keypoint and person detections and produces accurate poses by
assigning keypoints to person instances. On the COCO keypoints dataset our
pose estimation method outperforms all previous bottom-up methods both in
accuracy (+4-point mAP over previous best result) and speed; it also performs
on par with the best top-down methods while being at least 4x faster. Our
method is the fastest real time system with 23 frames/sec. Source code is
available at: https://github.com/mkocabas/pose-residual-network
",0,0,1
Decoding High-Order Interleaved Rank-Metric Codes,"  This paper presents an algorithm for decoding homogeneous interleaved codes
of high interleaving order in the rank metric. The new decoder is an adaption
of the Hamming-metric decoder by Metzner and Kapturowski (1990) and guarantees
to correct all rank errors of weight up to $d-2$ whose rank over the large base
field of the code equals the number of errors where $d$ is the minimum rank
distance of the underlying code. In contrast to previously-known decoding
algorithms the new decoder works for any rank-metric code not only Gabidulin
codes. It is purely based on linear-algebraic computations and has an explicit
and easy-to-handle success condition. Furthermore a lower bound on the
decoding success probability for random errors of a given weight is derived.
The relation of the new algorithm to existing interleaved decoders in the
special case of Gabidulin codes is given.
",1,0,0
Deep Generation of Coq Lemma Names Using Elaborated Terms,"  Coding conventions for naming spacing and other essentially stylistic
properties are necessary for developers to effectively understand review and
modify source code in large software projects. Consistent conventions in
verification projects based on proof assistants such as Coq increase in
importance as projects grow in size and scope. While conventions can be
documented and enforced manually at high cost emerging approaches
automatically learn and suggest idiomatic names in Java-like languages by
applying statistical language models on large code corpora. However due to its
powerful language extension facilities and fusion of type checking and
computation Coq is a challenging target for automated learning techniques. We
present novel generation models for learning and suggesting lemma names for Coq
projects. Our models based on multi-input neural networks are the first to
leverage syntactic and semantic information from Coq's lexer (tokens in lemma
statements) parser (syntax trees) and kernel (elaborated terms) for naming;
the key insight is that learning from elaborated terms can substantially boost
model performance. We implemented our models in a toolchain dubbed Roosterize
and applied it on a large corpus of code derived from the Mathematical
Components family of projects known for its stringent coding conventions. Our
results show that Roosterize substantially outperforms baselines for suggesting
lemma names highlighting the importance of using multi-input models and
elaborated terms.
",0,1,0
Fine-grained Interpretation and Causation Analysis in Deep NLP Models,"  This paper is a write-up for the tutorial on ""Fine-grained Interpretation and
Causation Analysis in Deep NLP Models"" that we are presenting at NAACL 2021. We
present and discuss the research work on interpreting fine-grained components
of a model from two perspectives i) fine-grained interpretation ii) causation
analysis. The former introduces methods to analyze individual neurons and a
group of neurons with respect to a language property or a task. The latter
studies the role of neurons and input features in explaining decisions made by
the model. We also discuss application of neuron analysis such as network
manipulation and domain adaptation. Moreover we present two toolkits namely
NeuroX and Captum that support functionalities discussed in this tutorial.
",0,1,0
DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection,"  Every 20 seconds a limb is amputated somewhere in the world due to diabetes.
This is a global health problem that requires a global solution. The MICCAI
challenge discussed in this paper which concerns the automated detection of
diabetic foot ulcers using machine learning techniques will accelerate the
development of innovative healthcare technology to address this unmet medical
need. In an effort to improve patient care and reduce the strain on healthcare
systems recent research has focused on the creation of cloud-based detection
algorithms. These can be consumed as a service by a mobile app that patients
(or a carer partner or family member) could use themselves at home to monitor
their condition and to detect the appearance of a diabetic foot ulcer (DFU).
Collaborative work between Manchester Metropolitan University Lancashire
Teaching Hospital and the Manchester University NHS Foundation Trust has
created a repository of 4000 DFU images for the purpose of supporting research
toward more advanced methods of DFU detection. Based on a joint effort
involving the lead scientists of the UK US India and New Zealand this
challenge will solicit original work and promote interactions between
researchers and interdisciplinary collaborations. This paper presents a dataset
description and analysis assessment methods benchmark algorithms and initial
evaluation results. It facilitates the challenge by providing useful insights
into state-of-the-art and ongoing research. This grand challenge takes on even
greater urgency in a peri and post-pandemic period where stresses on resource
utilization will increase the need for technology that allows people to remain
active healthy and intact in their home.
",0,0,1
"An Analytical Model of Information Dissemination for a Gossip-based
  Protocol","  We develop an analytical model of information dissemination for a gossiping
protocol that combines both pull and push approaches. With this model we
analyse how fast an item is replicated through a network and how fast the item
spreads in the network and how fast the item covers the network. We also
determine the optimal size of the exchange buffer to obtain fast replication.
Our results are confirmed by large-scale simulation experiments.
",1,0,0
"Hybrid Beamforming with Spatial Modulation in Multi-user Massive MIMO
  mmWave Networks","  The cost of radio frequency (RF) chains is the biggest drawback of massive
MIMO millimeter wave networks. By employing spatial modulation (SM) it is
possible to implement lower number of RF chains than transmit antennas but
still achieve high spectral efficiency. In this work we propose a system model
of the SM scheme together with hybrid beamforming at the transmitter and
digital combining at the receiver. In the proposed model spatially-modulated
bits are mapped onto indices of antenna arrays. It is shown that the proposed
model achieves approximately 5dB gain over classical multi-user SM scheme with
only 8 transmit antennas at each antenna array. This gain can be improved
further by increasing the number of transmit antennas at each array without
increasing the number of RF chains.
",1,0,0
Role of temporal inference in the recognition of textual inference,"  This project is a part of nature language processing and its aims to develop
a system of recognition inference text-appointed TIMINF. This type of system
can detect given two portions of text if a text is semantically deducted from
the other. We focused on making the inference time in this type of system. For
that we have built and analyzed a body built from questions collected through
the web. This study has enabled us to classify different types of times
inferences and for designing the architecture of TIMINF which seeks to
integrate a module inference time in a detection system inference text. We also
assess the performance of sorties TIMINF system on a test corpus with the same
strategy adopted in the challenge RTE.
",0,1,0
"Rectifier Neural Network with a Dual-Pathway Architecture for Image
  Denoising","  Recently deep neural networks based on tanh activation function have shown
their impressive power in image denoising. In this letter we try to use
rectifier function instead of tanh and propose a dual-pathway rectifier neural
network by combining two rectifier neurons with reversed input and output
weights in the same hidden layer. We drive the equivalent activation function
and compare it to some typical activation functions for image denoising under
the same network architecture. The experimental results show that our model
achieves superior performances faster especially when the noise is small.
",0,0,1
"Rethinking Common Assumptions to Mitigate Racial Bias in Face
  Recognition Datasets","  Many existing works have made great strides towards reducing racial bias in
face recognition. However most of these methods attempt to rectify bias that
manifests in models during training instead of directly addressing a major
source of the bias the dataset itself. Exceptions to this are
BUPT-Balancedface/RFW and Fairface but these works assume that primarily
training on a single race or not racially balancing the dataset are inherently
disadvantageous. We demonstrate that these assumptions are not necessarily
valid. In our experiments training on only African faces induced less bias
than training on a balanced distribution of faces and distributions skewed to
include more African faces produced more equitable models. We additionally
notice that adding more images of existing identities to a dataset in place of
adding new identities can lead to accuracy boosts across racial categories. Our
code is available at
https://github.com/j-alex-hanson/rethinking-race-face-datasets.
",0,0,1
"Learning Rope Manipulation Policies Using Dense Object Descriptors
  Trained on Synthetic Depth Data","  Robotic manipulation of deformable 1D objects such as ropes cables and
hoses is challenging due to the lack of high-fidelity analytic models and large
configuration spaces. Furthermore learning end-to-end manipulation policies
directly from images and physical interaction requires significant time on a
robot and can fail to generalize across tasks. We address these challenges
using interpretable deep visual representations for rope extending recent work
on dense object descriptors for robot manipulation. This facilitates the design
of interpretable and transferable geometric policies built on top of the
learned representations decoupling visual reasoning and control. We present an
approach that learns point-pair correspondences between initial and goal rope
configurations which implicitly encodes geometric structure entirely in
simulation from synthetic depth images. We demonstrate that the learned
representation -- dense depth object descriptors (DDODs) -- can be used to
manipulate a real rope into a variety of different arrangements either by
learning from demonstrations or using interpretable geometric policies. In 50
trials of a knot-tying task with the ABB YuMi Robot the system achieves a 66%
knot-tying success rate from previously unseen configurations. See
https://tinyurl.com/rope-learning for supplementary material and videos.
",0,0,1
Unequal Error Protection in Coded Slotted ALOHA,"  We analyze the performance of coded slotted ALOHA systems for a scenario
where users have different error protection requirements and correspondingly
can be divided into user classes. The main goal is to design the system so that
the requirements for each class are satisfied. To that end we derive
analytical error floor approximations of the packet loss rate for each class in
the finite frame length regime as well as the density evolution in the
asymptotic case. Based on this analysis we propose a heuristic approach for
the optimization of the degree distributions to provide the required unequal
error protection. In addition we analyze the decoding delay for users in
different classes and show that better protected users experience a smaller
average decoding delay.
",1,0,0
"Air-Writing Translater: A Novel Unsupervised Domain Adaptation Method
  for Inertia-Trajectory Translation of In-air Handwriting","  As a new way of human-computer interaction inertial sensor based in-air
handwriting can provide a natural and unconstrained interaction to express more
complex and richer information in 3D space. However most of the existing
in-air handwriting work is mainly focused on handwritten character recognition
which makes these work suffer from poor readability of inertial signal and lack
of labeled samples. To address these two problems we use unsupervised domain
adaptation method to reconstruct the trajectory of inertial signal and generate
inertial samples using online handwritten trajectories. In this paper we
propose an AirWriting Translater model to learn the bi-directional translation
between trajectory domain and inertial domain in the absence of paired inertial
and trajectory samples. Through semantic-level adversarial training and latent
classification loss the proposed model learns to extract domain-invariant
content between inertial signal and trajectory while preserving semantic
consistency during the translation across the two domains. We carefully design
the architecture so that the proposed framework can accept inputs of arbitrary
length and translate between different sampling rates. We also conduct
experiments on two public datasets: 6DMG (in-air handwriting dataset) and CT
(handwritten trajectory dataset) the results on the two datasets demonstrate
that the proposed network successes in both Inertia-to Trajectory and
Trajectory-to-Inertia translation tasks.
",0,0,1
More Classes of Complete Permutation Polynomials over $F_q$,"  In this paper by using a powerful criterion for permutation polynomials
given by Zieve we give several classes of complete permutation monomials over
$F_q^r$. In addition we present a class of complete permutation
multinomials which is a generalization of recent work.
",1,0,0
"Multi-reference Tacotron by Intercross Training for Style
  DisentanglingTransfer and Control in Speech Synthesis","  Speech style control and transfer techniques aim to enrich the diversity and
expressiveness of synthesized speech. Existing approaches model all speech
styles into one representation lacking the ability to control a specific
speech feature independently. To address this issue we introduce a novel
multi-reference structure to Tacotron and propose intercross training approach
which together ensure that each sub-encoder of the multi-reference encoder
independently disentangles and controls a specific style. Experimental results
show that our model is able to control and transfer desired speech styles
individually.
",0,1,0
"MoCo-CXR: MoCo Pretraining Improves Representation and Transferability
  of Chest X-ray Models","  Contrastive learning is a form of self-supervision that can leverage
unlabeled data to produce pretrained models. While contrastive learning has
demonstrated promising results on natural image classification tasks its
application to medical imaging tasks like chest X-ray interpretation has been
limited. In this work we propose MoCo-CXR which is an adaptation of the
contrastive learning method Momentum Contrast (MoCo) to produce models with
better representations and initializations for the detection of pathologies in
chest X-rays. In detecting pleural effusion we find that linear models trained
on MoCo-CXR-pretrained representations outperform those without
MoCo-CXR-pretrained representations indicating that MoCo-CXR-pretrained
representations are of higher-quality. End-to-end fine-tuning experiments
reveal that a model initialized via MoCo-CXR-pretraining outperforms its
non-MoCo-CXR-pretrained counterpart. We find that MoCo-CXR-pretraining provides
the most benefit with limited labeled training data. Finally we demonstrate
similar results on a target Tuberculosis dataset unseen during pretraining
indicating that MoCo-CXR-pretraining endows models with representations and
transferability that can be applied across chest X-ray datasets and tasks.
",0,0,1
"Go Beyond Plain Fine-tuning: Improving Pretrained Models for Social
  Commonsense","  Pretrained language models have demonstrated outstanding performance in many
NLP tasks recently. However their social intelligence which requires
commonsense reasoning about the current situation and mental states of others
is still developing. Towards improving language models' social intelligence we
focus on the Social IQA dataset a task requiring social and emotional
commonsense reasoning. Building on top of the pretrained RoBERTa and GPT2
models we propose several architecture variations and extensions as well as
leveraging external commonsense corpora to optimize the model for Social IQA.
Our proposed system achieves competitive results as those top-ranking models on
the leaderboard. This work demonstrates the strengths of pretrained language
models and provides viable ways to improve their performance for a particular
task.
",0,1,0
Transfer learning for music classification and regression tasks,"  In this paper we present a transfer learning approach for music
classification and regression tasks. We propose to use a pre-trained convnet
feature a concatenated feature vector using the activations of feature maps of
multiple layers in a trained convolutional network. We show how this convnet
feature can serve as general-purpose music representation. In the experiments
a convnet is trained for music tagging and then transferred to other
music-related classification and regression tasks. The convnet feature
outperforms the baseline MFCC feature in all the considered tasks and several
previous approaches that are aggregating MFCCs as well as low- and high-level
music features.
",0,0,1
"Beyond Topics: Discovering Latent Healthcare Objectives from Event
  Sequences","  A meaningful understanding of clinical protocols and patient pathways helps
improve healthcare outcomes. Electronic health records (EHR) reflect real-world
treatment behaviours that are used to enhance healthcare management but present
challenges; protocols and pathways are often loosely defined and with elements
frequently not recorded in EHRs complicating the enhancement. To solve this
challenge healthcare objectives associated with healthcare management
activities can be indirectly observed in EHRs as latent topics. Topic models
such as Latent Dirichlet Allocation (LDA) are used to identify latent patterns
in EHR data. However they do not examine the ordered nature of EHR sequences
nor do they appraise individual events in isolation. Our novel approach the
Categorical Sequence Encoder (CaSE) addresses these shortcomings. The
sequential nature of EHRs is captured by CaSE's event-level representations
revealing latent healthcare objectives. In synthetic EHR sequences CaSE
outperforms LDA by up to 37% at identifying healthcare objectives. In the
real-world MIMIC-III dataset CaSE identifies meaningful representations that
could critically enhance protocol and pathway development.
",0,1,0
"A Comprehensive Performance Evaluation for 3D Transformation Estimation
  Techniques","  3D local feature extraction and matching is the basis for solving many tasks
in the area of computer vision such as 3D registration modeling recognition
and retrieval. However this process commonly draws into false correspondences
due to noise limited features occlusion incomplete surface and etc. In order
to estimate accurate transformation based on these corrupted correspondences
numerous transformation estimation techniques have been proposed. However the
merits demerits and appropriate application for these methods are unclear
owing to that no comprehensive evaluation for the performance of these methods
has been conducted. This paper evaluates eleven state-of-the-art transformation
estimation proposals on both descriptor based and synthetic correspondences. On
descriptor based correspondences several evaluation items (including the
performance on different datasets robustness to different overlap ratios and
the performance of these technique combined with Iterative Closest Point (ICP)
different local features and LRF/A techniques) of these methods are tested on
four popular datasets acquired with different devices. On synthetic
correspondences the robustness of these methods to varying percentages of
correct correspondences (PCC) is evaluated. In addition we also evaluate the
efficiencies of these methods. Finally the merits demerits and application
guidance of these tested transformation estimation methods are summarized.
",0,0,1
Evaluating Bias In Dutch Word Embeddings,"  Recent research in Natural Language Processing has revealed that word
embeddings can encode social biases present in the training data which can
affect minorities in real world applications. This paper explores the gender
bias implicit in Dutch embeddings while investigating whether English language
based approaches can also be used in Dutch. We implement the Word Embeddings
Association Test (WEAT) Clustering and Sentence Embeddings Association Test
(SEAT) methods to quantify the gender bias in Dutch word embeddings then we
proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods
and finally we evaluate the performance of the debiased embeddings in
downstream tasks. The results suggest that among others gender bias is
present in traditional and contextualized Dutch word embeddings. We highlight
how techniques used to measure and reduce bias created for English can be used
in Dutch embeddings by adequately translating the data and taking into account
the unique characteristics of the language. Furthermore we analyze the effect
of the debiasing techniques on downstream tasks which show a negligible impact
on traditional embeddings and a 2% decrease in performance in contextualized
embeddings. Finally we release the translated Dutch datasets to the public
along with the traditional embeddings with mitigated bias.
",0,1,0
Radius Adaptive Convolutional Neural Network,"  Convolutional neural network (CNN) is widely used in computer vision
applications. In the networks that deal with images CNNs are the most
time-consuming layer of the networks. Usually the solution to address the
computation cost is to decrease the number of trainable parameters. This
solution usually comes with the cost of dropping the accuracy. Another problem
with this technique is that usually the cost of memory access is not taken into
account which results in insignificant speedup gain. The number of operations
and memory access in a standard convolution layer is independent of the input
content which makes it limited for certain accelerations. We propose a simple
modification to a standard convolution to bridge this gap. We propose an
adaptive convolution that adopts different kernel sizes (or radii) based on the
content. The network can learn and select the proper radius based on the input
content in a soft decision manner. Our proposed radius-adaptive convolutional
neural network (RACNN) has a similar number of weights to a standard one yet
results show it can reach higher speeds. The code has been made available at:
https://github.com/meisamrf/racnn.
",0,0,1
Adaptive PCA for Time-Varying Data,"  In this paper we present an online adaptive PCA algorithm that is able to
compute the full dimensional eigenspace per new time-step of sequential data.
The algorithm is based on a one-step update rule that considers all second
order correlations between previous samples and the new time-step. Our
algorithm has O(n) complexity per new time-step in its deterministic mode and
O(1) complexity per new time-step in its stochastic mode. We test our algorithm
on a number of time-varying datasets of different physical phenomena. Explained
variance curves indicate that our technique provides an excellent approximation
to the original eigenspace computed using standard PCA in batch mode. In
addition our experiments show that the stochastic mode despite its much lower
computational complexity converges to the same eigenspace computed using the
deterministic mode.
",0,0,1
"Wavelet Domain Style Transfer for an Effective Perception-distortion
  Tradeoff in Single Image Super-Resolution","  In single image super-resolution (SISR) given a low-resolution (LR) image
one wishes to find a high-resolution (HR) version of it which is both accurate
and photo-realistic. Recently it has been shown that there exists a
fundamental tradeoff between low distortion and high perceptual quality and
the generative adversarial network (GAN) is demonstrated to approach the
perception-distortion (PD) bound effectively. In this paper we propose a novel
method based on wavelet domain style transfer (WDST) which achieves a better
PD tradeoff than the GAN based methods. Specifically we propose to use 2D
stationary wavelet transform (SWT) to decompose one image into low-frequency
and high-frequency sub-bands. For the low-frequency sub-band we improve its
objective quality through an enhancement network. For the high-frequency
sub-band we propose to use WDST to effectively improve its perceptual quality.
By feat of the perfect reconstruction property of wavelets these sub-bands can
be re-combined to obtain an image which has simultaneously high objective and
perceptual quality. The numerical results on various datasets show that our
method achieves the best trade-off between the distortion and perceptual
quality among the existing state-of-the-art SISR methods.
",0,0,1
"Capacity of Binary State Symmetric Channel with and without Feedback and
  Transmission Cost","  We consider a unit memory channel called Binary State Symmetric Channel
(BSSC) in which the channel state is the modulo2 addition of the current
channel input and the previous channel output. We derive closed form
expressions for the capacity and corresponding channel input distribution of
this BSSC with and without feedback and transmission cost. We also show that
the capacity of the BSSC is not increased by feedback and it is achieved by a
first order symmetric Markov process.
",1,0,0
Modeling Topical Relevance for Multi-Turn Dialogue Generation,"  Topic drift is a common phenomenon in multi-turn dialogue. Therefore an
ideal dialogue generation models should be able to capture the topic
information of each context detect the relevant context and produce
appropriate responses accordingly. However existing models usually use word or
sentence level similarities to detect the relevant contexts which fail to well
capture the topical level relevance. In this paper we propose a new model
named STAR-BTM to tackle this problem. Firstly the Biterm Topic Model is
pre-trained on the whole training dataset. Then the topic level attention
weights are computed based on the topic representation of each context.
Finally the attention weights and the topic distribution are utilized in the
decoding process to generate the corresponding responses. Experimental results
on both Chinese customer services data and English Ubuntu dialogue data show
that STAR-BTM significantly outperforms several state-of-the-art methods in
terms of both metric-based and human evaluations.
",0,1,0
"Making Higher Order MOT Scalable: An Efficient Approximate Solver for
  Lifted Disjoint Paths","  We present an efficient approximate message passing solver for the lifted
disjoint paths problem (LDP) a natural but NP-hard model for multiple object
tracking (MOT). Our tracker scales to very large instances that come from long
and crowded MOT sequences. Our approximate solver enables us to process the
MOT15/16/17 benchmarks without sacrificing solution quality and allows for
solving MOT20 which has been out of reach up to now for LDP solvers due to its
size and complexity. On all these four standard MOT benchmarks we achieve
performance comparable or better than current state-of-the-art methods
including a tracker based on an optimal LDP solver.
",0,0,1
Word-Level Style Control for Expressive Non-attentive Speech Synthesis,"  This paper presents an expressive speech synthesis architecture for modeling
and controlling the speaking style at a word level. It attempts to learn
word-level stylistic and prosodic representations of the speech data with the
aid of two encoders. The first one models style by finding a combination of
style tokens for each word given the acoustic features and the second outputs
a word-level sequence conditioned only on the phonetic information in order to
disentangle it from the style information. The two encoder outputs are aligned
and concatenated with the phoneme encoder outputs and then decoded with a
Non-Attentive Tacotron model. An extra prior encoder is used to predict the
style tokens autoregressively in order for the model to be able to run without
a reference utterance. We find that the resulting model gives both word-level
and global control over the style as well as prosody transfer capabilities.
",0,1,0
Challenge of Multi-Camera Tracking,"  Multi-camera tracking is quite different from single camera tracking and it
faces new technology and system architecture challenges. By analyzing the
corresponding characteristics and disadvantages of the existing algorithms
problems in multi-camera tracking are summarized and some new directions for
future work are also generalized.
",0,0,1
Enhancing Photorealism Enhancement,"  We present an approach to enhancing the realism of synthetic images. The
images are enhanced by a convolutional network that leverages intermediate
representations produced by conventional rendering pipelines. The network is
trained via a novel adversarial objective which provides strong supervision at
multiple perceptual levels. We analyze scene layout distributions in commonly
used datasets and find that they differ in important ways. We hypothesize that
this is one of the causes of strong artifacts that can be observed in the
results of many prior methods. To address this we propose a new strategy for
sampling image patches during training. We also introduce multiple
architectural improvements in the deep network modules used for photorealism
enhancement. We confirm the benefits of our contributions in controlled
experiments and report substantial gains in stability and realism in comparison
to recent image-to-image translation methods and a variety of other baselines.
",0,0,1
Reference Resolution within the Framework of Cognitive Grammar,"  Following the principles of Cognitive Grammar we concentrate on a model for
reference resolution that attempts to overcome the difficulties previous
approaches based on the fundamental assumption that all reference (independent
on the type of the referring expression) is accomplished via access to and
restructuring of domains of reference rather than by direct linkage to the
entities themselves. The model accounts for entities not explicitly mentioned
but understood in a discourse and enables exploitation of discursive and
perceptual context to limit the set of potential referents for a given
referring expression. As the most important feature we note that a single
mechanism is required to handle what are typically treated as diverse
phenomena. Our approach then provides a fresh perspective on the relations
between Cognitive Grammar and the problem of reference.
",0,1,0
"SberQuAD -- Russian Reading Comprehension Dataset: Description and
  Analysis","  SberQuAD -- a large scale analog of Stanford SQuAD in the Russian language -
is a valuable resource that has not been properly presented to the scientific
community. We fill this gap by providing a description a thorough analysis
and baseline experimental results.
",0,1,0
Deep Fragment Embeddings for Bidirectional Image Sentence Mapping,"  We introduce a model for bidirectional retrieval of images and sentences
through a multi-modal embedding of visual and natural language data. Unlike
previous models that directly map images or sentences into a common embedding
space our model works on a finer level and embeds fragments of images
(objects) and fragments of sentences (typed dependency tree relations) into a
common space. In addition to a ranking objective seen in previous work this
allows us to add a new fragment alignment objective that learns to directly
associate these fragments across modalities. Extensive experimental evaluation
shows that reasoning on both the global level of images and sentences and the
finer level of their respective fragments significantly improves performance on
image-sentence retrieval tasks. Additionally our model provides interpretable
predictions since the inferred inter-modal fragment alignment is explicit.
",0,1,0
"Understanding and Improving Convolutional Neural Networks via
  Concatenated Rectified Linear Units","  Recently convolutional neural networks (CNNs) have been used as a powerful
tool to solve many problems of machine learning and computer vision. In this
paper we aim to provide insight on the property of convolutional neural
networks as well as a generic method to improve the performance of many CNN
architectures. Specifically we first examine existing CNN models and observe
an intriguing property that the filters in the lower layers form pairs (i.e.
filters with opposite phase). Inspired by our observation we propose a novel
simple yet effective activation scheme called concatenated ReLU (CRelu) and
theoretically analyze its reconstruction property in CNNs. We integrate CRelu
into several state-of-the-art CNN architectures and demonstrate improvement in
their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer
trainable parameters. Our results suggest that better understanding of the
properties of CNNs can lead to significant performance improvement with a
simple modification.
",0,0,1
Deep Identity-aware Transfer of Facial Attributes,"  This paper presents a Deep convolutional network model for Identity-Aware
Transfer (DIAT) of facial attributes. Given the source input image and the
reference attribute DIAT aims to generate a facial image that owns the
reference attribute as well as keeps the same or similar identity to the input
image. In general our model consists of a mask network and an attribute
transform network which work in synergy to generate a photo-realistic facial
image with the reference attribute. Considering that the reference attribute
may be only related to some parts of the image the mask network is introduced
to avoid the incorrect editing on attribute irrelevant region. Then the
estimated mask is adopted to combine the input and transformed image for
producing the transfer result. For joint training of transform network and mask
network we incorporate the adversarial attribute loss identity-aware adaptive
perceptual loss and VGG-FACE based identity loss. Furthermore a denoising
network is presented to serve for perceptual regularization to suppress the
artifacts in transfer result while an attribute ratio regularization is
introduced to constrain the size of attribute relevant region. Our DIAT can
provide a unified solution for several representative facial attribute transfer
tasks e.g. expression transfer accessory removal age progression and
gender transfer and can be extended for other face enhancement tasks such as
face hallucination. The experimental results validate the effectiveness of the
proposed method. Even for the identity-related attribute (e.g. gender) our
DIAT can obtain visually impressive results by changing the attribute while
retaining most identity-aware features.
",0,0,1
"Deep Learning Guided Building Reconstruction from Satellite
  Imagery-derived Point Clouds","  3D urban reconstruction of buildings from remotely sensed imagery has drawn
significant attention during the past two decades. While aerial imagery and
LiDAR provide higher resolution satellite imagery is cheaper and more
efficient to acquire for large scale need. However the high orbital altitude
of satellite observation brings intrinsic challenges like unpredictable
atmospheric effect multi view angles significant radiometric differences due
to the necessary multiple views diverse land covers and urban structures in a
scene small base-height ratio or narrow field of view all of which may
degrade 3D reconstruction quality. To address these major challenges we
present a reliable and effective approach for building model reconstruction
from the point clouds generated from multi-view satellite images. We utilize
multiple types of primitive shapes to fit the input point cloud. Specifically
a deep-learning approach is adopted to distinguish the shape of building roofs
in complex and yet noisy scenes. For points that belong to the same roof shape
a multi-cue hierarchical RANSAC approach is proposed for efficient and
reliable segmenting and reconstructing the building point cloud. Experimental
results over four selected urban areas (0.34 to 2.04 sq km in size) demonstrate
the proposed method can generate detailed roof structures under noisy data
environments. The average successful rate for building shape recognition is
83.0% while the overall completeness and correctness are over 70% with
reference to ground truth created from airborne lidar. As the first effort to
address the public need of large scale city model generation the development
is deployed as open source software.
",0,0,1
"Handwriting Trajectory Recovery using End-to-End Deep Encoder-Decoder
  Network","  In this paper we introduce a novel technique to recover the pen trajectory
of offline characters which is a crucial step for handwritten character
recognition. Generally online acquisition approach has more advantage than its
offline counterpart as the online technique keeps track of the pen movement.
Hence pen tip trajectory retrieval from offline text can bridge the gap
between online and offline methods. Our proposed framework employs sequence to
sequence model which consists of an encoder-decoder LSTM module. Our encoder
module consists of Convolutional LSTM network which takes an offline character
image as the input and encodes the feature sequence to a hidden representation.
The output of the encoder is fed to a decoder LSTM and we get the successive
coordinate points from every time step of the decoder LSTM. Although the
sequence to sequence model is a popular paradigm in various computer vision and
language translation tasks the main contribution of our work lies in designing
an end-to-end network for a decade old popular problem in Document Image
Analysis community. Tamil Telugu and Devanagari characters of LIPI Toolkit
dataset are used for our experiments. Our proposed method has achieved superior
performance compared to the other conventional approaches.
",0,0,1
"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
  Pre-training","  We propose Unicoder-VL a universal encoder that aims to learn joint
representations of vision and language in a pre-training manner. Borrow ideas
from cross-lingual pre-trained models such as XLM and Unicoder both visual
and linguistic contents are fed into a multi-layer Transformer for the
cross-modal pre-training where three pre-trained tasks are employed including
Masked Language Modeling (MLM) Masked Object Classification (MOC) and
Visual-linguistic Matching (VLM). The first two tasks learn context-aware
representations for input tokens based on linguistic and visual contents
jointly. The last task tries to predict whether an image and a text describe
each other. After pretraining on large-scale image-caption pairs we transfer
Unicoder-VL to caption-based image-text retrieval and visual commonsense
reasoning with just one additional output layer. We achieve state-of-the-art
or comparable results on both two tasks and show the powerful ability of the
cross-modal pre-training.
",0,0,1
Uncertainty Estimation in One-Stage Object Detection,"  Environment perception is the task for intelligent vehicles on which all
subsequent steps rely. A key part of perception is to safely detect other road
users such as vehicles pedestrians and cyclists. With modern deep learning
techniques huge progress was made over the last years in this field. However
such deep learning based object detection models cannot predict how certain
they are in their predictions potentially hampering the performance of later
steps such as tracking or sensor fusion. We present a viable approaches to
estimate uncertainty in an one-stage object detector while improving the
detection performance of the baseline approach. The proposed model is evaluated
on a large scale automotive pedestrian dataset. Experimental results show that
the uncertainty outputted by our system is coupled with detection accuracy and
the occlusion level of pedestrians.
",0,0,1
"Detection of Bleeding in Wireless Capsule Endoscopy Images Using Range
  Ratio Color","  Wireless Capsule Endoscopy (WCE) is device to detect abnormalities in
colonesophagussmall intestinal and stomach to distinguish bleeding in WCE
images from non bleeding is a hard job by human reviewing and very time
consuming. Consequently automation for classifying bleeding frames not only
will expedite the process but will reduce the burden on the doctors. Using the
purity of the red color we can detect the Bleeding areas in WCE images. But we
could find various intensity of red color values in different parts of the
small intestinalso it is not enough to depend on the red color feature alone.
We select RGB(RedGreenBlue) because it takes raw level values and it is easy
to use. In this paper we will put range ratio color for each of RGand B.
Therefore we divide each image into multiple pixels and apply the range ratio
color condition for each pixel. Then we count the number of the pixels that
achieved our condition. If the number of pixels grater than zero then the
frame is classified as a bleeding type. Otherwise it is a non-bleeding. Our
experimental results show that this method could achieve a very high accuracy
in detecting bleeding images for the different parts of the small intestinal
",0,0,1
"Equivocations Exponents and Second-Order Coding Rates under Various
  R'enyi Information Measures","  We evaluate the asymptotics of equivocations their exponents as well as
their second-order coding rates under various R'enyi information measures.
Specifically we consider the effect of applying a hash function on a source
and we quantify the level of non-uniformity and dependence of the compressed
source from another correlated source when the number of copies of the sources
is large. Unlike previous works that use Shannon information measures to
quantify randomness information or uniformity we define our security measures
in terms of a more general class of information measures--the R'enyi
information measures and their Gallager-type counterparts. A special case of
these R'enyi information measure is the class of Shannon information
measures. We prove tight asymptotic results for the security measures and their
exponential rates of decay. We also prove bounds on the second-order
asymptotics and show that these bounds match when the magnitudes of the
second-order coding rates are large. We do so by establishing new classes
non-asymptotic bounds on the equivocation and evaluating these bounds using
various probabilistic limit theorems asymptotically.
",1,0,0
"Performance of Transfer Learning Model vs Traditional Neural Network in
  Low System Resource Environment","  Recently the use of pre-trained model to build neural network based on
transfer learning methodology is increasingly popular. These pre-trained models
present the benefit of using less computing resources to train model with
smaller amount of training data. The rise of state-of-the-art models such as
BERT XLNet and GPT boost accuracy and benefit as a base model for transfer
leanring. However these models are still too complex and consume many
computing resource to train for transfer learning with low GPU memory. We will
compare the performance and cost between lighter transfer learning model and
purposely built neural network for NLP application of text classification and
NER model.
",0,1,0
"Iterative Refinement in the Continuous Space for Non-Autoregressive
  Neural Machine Translation","  We propose an efficient inference procedure for non-autoregressive machine
translation that iteratively refines translation purely in the continuous
space. Given a continuous latent variable model for machine translation (Shu et
al. 2020) we train an inference network to approximate the gradient of the
marginal log probability of the target sentence using only the latent variable
as input. This allows us to use gradient-based optimization to find the target
sentence at inference time that approximately maximizes its marginal
probability. As each refinement step only involves computation in the latent
space of low dimensionality (we use 8 in our experiments) we avoid
computational overhead incurred by existing non-autoregressive inference
procedures that often refine in token space. We compare our approach to a
recently proposed EM-like inference procedure (Shu et al. 2020) that optimizes
in a hybrid space consisting of both discrete and continuous variables. We
evaluate our approach on WMT'14 En-De WMT'16 Ro-En and IWSLT'16 De-En and
observe two advantages over the EM-like inference: (1) it is computationally
efficient i.e. each refinement step is twice as fast and (2) it is more
effective resulting in higher marginal probabilities and BLEU scores with the
same number of refinement steps. On WMT'14 En-De for instance our approach is
able to decode 6.2 times faster than the autoregressive model with minimal
degradation to translation quality (0.9 BLEU).
",0,1,0
A lower bound on the 2-adic complexity of modified Jacobi sequence,"  Let $pq$ be distinct primes satisfying $mathrmgcd(p-1q-1)=d$ and let
$D_i$ $i=01cdotsd-1$ be Whiteman's generalized cyclotomic classes with
$Z_pq^ast=cup_i=0^d-1D_i$. In this paper we give the values of Gauss
periods based on the generalized cyclotomic sets
$D_0^ast=sum_i=0^fracd2-1D_2i$ and
$D_1^ast=sum_i=0^fracd2-1D_2i+1$. As an application we
determine a lower bound on the 2-adic complexity of modified Jacobi sequence.
Our result shows that the 2-adic complexity of modified Jacobi sequence is at
least $pq-p-q-1$ with period $N=pq$. This indicates that the 2-adic complexity
of modified Jacobi sequence is large enough to resist the attack of the
rational approximation algorithm (RAA) for feedback with carry shift registers
(FCSRs).
",1,0,0
Neural network identifiability for a family of sigmoidal nonlinearities,"  This paper addresses the following question of neural network
identifiability: Does the input-output map realized by a feed-forward neural
network with respect to a given nonlinearity uniquely specify the network
architecture weights and biases? Existing literature on the subject Sussman
1992 Albertini Sontag et al. 1993 Fefferman 1994 suggests that the answer
should be yes up to certain symmetries induced by the nonlinearity and
provided the networks under consideration satisfy certain ""genericity
conditions"". The results in Sussman 1992 and Albertini Sontag et al. 1993
apply to networks with a single hidden layer and in Fefferman 1994 the networks
need to be fully connected. In an effort to answer the identifiability question
in greater generality we derive necessary genericity conditions for the
identifiability of neural networks of arbitrary depth and connectivity with an
arbitrary nonlinearity. Moreover we construct a family of nonlinearities for
which these genericity conditions are minimal i.e. both necessary and
sufficient. This family is large enough to approximate many commonly
encountered nonlinearities to within arbitrary precision in the uniform norm.
",1,0,0
"Deconstruct to Reconstruct a Configurable Evaluation Metric for
  Open-Domain Dialogue Systems","  Many automatic evaluation metrics have been proposed to score the overall
quality of a response in open-domain dialogue. Generally the overall quality
is comprised of various aspects such as relevancy specificity and empathy
and the importance of each aspect differs according to the task. For instance
specificity is mandatory in a food-ordering dialogue task whereas fluency is
preferred in a language-teaching dialogue system. However existing metrics are
not designed to cope with such flexibility. For example BLEU score
fundamentally relies only on word overlapping whereas BERTScore relies on
semantic similarity between reference and candidate response. Thus they are
not guaranteed to capture the required aspects i.e. specificity. To design a
metric that is flexible to a task we first propose making these qualities
manageable by grouping them into three groups: understandability sensibleness
and likability where likability is a combination of qualities that are
essential for a task. We also propose a simple method to composite metrics of
each aspect to obtain a single metric called USL-H which stands for
Understandability Sensibleness and Likability in Hierarchy. We demonstrated
that USL-H score achieves good correlations with human judgment and maintains
its configurability towards different aspects and metrics.
",0,1,0
Real-Time Optimized N-gram For Mobile Devices,"  With the increasing number of mobile devices there has been continuous
research on generating optimized Language Models (LMs) for soft keyboard. In
spite of advances in this domain building a single LM for low-end feature
phones as well as high-end smartphones is still a pressing need. Hence we
propose a novel technique Optimized N-gram (Op-Ngram) an end-to-end N-gram
pipeline that utilises mobile resources efficiently for faster Word Completion
(WC) and Next Word Prediction (NWP). Op-Ngram applies Stupid Backoff and
pruning strategies to generate a light-weight model. The LM loading time on
mobile is linear with respect to model size. We observed that Op-Ngram gives
37% improvement in Language Model (LM)-ROM size 76% in LM-RAM size 88% in
loading time and 89% in average suggestion time as compared to SORTED array
variant of BerkeleyLM. Moreover our method shows significant performance
improvement over KenLM as well.
",0,1,0
"Gradient Pursuit-Based Channel Estimation for MmWave Massive MIMO
  Systems with One-Bit ADCs","  In this paper channel estimation for millimeter wave (mmWave) massive
multiple-input multiple-output (MIMO) systems with one-bit analog-to-digital
converters (ADCs) is considered. In the mmWave band the number of propagation
paths is small which results in sparse virtual channels. To estimate sparse
virtual channels based on the maximum a posteriori (MAP) criterion
sparsity-constrained optimization comes into play. In general optimizing
objective functions with sparsity constraints is NP-hard because of their
combinatorial complexity. Furthermore the coarse quantization of one-bit ADCs
makes channel estimation a challenging task. In the field of compressed sensing
(CS) the gradient support pursuit (GraSP) and gradient hard thresholding
pursuit (GraHTP) algorithms were proposed to approximately solve
sparsity-constrained optimization problems iteratively by pursuing the gradient
of the objective function via hard thresholding. The accuracy guarantee of
these algorithms however breaks down when the objective function is
ill-conditioned which frequently occurs in the mmWave band. To prevent the
breakdown of gradient pursuit-based algorithms the band maximum selecting
(BMS) technique which is a hard thresholder selecting only the ""band maxima""
is applied to GraSP and GraHTP to propose the BMSGraSP and BMSGraHTP algorithms
in this paper.
",1,0,0
"MIX : a Multi-task Learning Approach to Solve Open-Domain Question
  Answering","  In this paper we introduce MIX : a multi-task deep learning approach to
solve Open-Domain Question Answering. First we design our system as a
multi-stage pipeline made of 3 building blocks : a BM25-based Retriever to
reduce the search space; RoBERTa based Scorer and Extractor to rank retrieved
paragraphs and extract relevant spans of text respectively. Eventually we
further improve computational efficiency of our system to deal with the
scalability challenge : thanks to multi-task learning we parallelize the close
tasks solved by the Scorer and the Extractor. Our system is on par with
state-of-the-art performances on the squad-open benchmark while being simpler
conceptually.
",0,1,0
Plug-and-Play: Improve Depth Estimation via Sparse Data Propagation,"  We propose a novel plug-and-play (PnP) module for improving depth prediction
with taking arbitrary patterns of sparse depths as input. Given any pre-trained
depth prediction model our PnP module updates the intermediate feature map
such that the model outputs new depths consistent with the given sparse depths.
Our method requires no additional training and can be applied to practical
applications such as leveraging both RGB and sparse LiDAR points to robustly
estimate dense depth map. Our approach achieves consistent improvements on
various state-of-the-art methods on indoor (i.e. NYU-v2) and outdoor (i.e.
KITTI) datasets. Various types of LiDARs are also synthesized in our
experiments to verify the general applicability of our PnP module in practice.
For project page see https://zswang666.github.io/PnP-Depth-Project-Page/
",0,0,1
Image Captioning: Transforming Objects into Words,"  Image captioning models typically follow an encoder-decoder architecture
which uses abstract image feature vectors as input to the encoder. One of the
most successful algorithms uses feature vectors extracted from the region
proposals obtained from an object detector. In this work we introduce the
Object Relation Transformer that builds upon this approach by explicitly
incorporating information about the spatial relationship between input detected
objects through geometric attention. Quantitative and qualitative results
demonstrate the importance of such geometric attention for image captioning
leading to improvements on all common captioning metrics on the MS-COCO
dataset.
",0,0,1
"Training Neural Machine Translation (NMT) Models using Tensor Train
  Decomposition on TensorFlow (T3F)","  We implement a Tensor Train layer in the TensorFlow Neural Machine
Translation (NMT) model using the t3f library. We perform training runs on the
IWSLT English-Vietnamese '15 and WMT German-English '16 datasets with learning
rates $in 0.00040.00080.0012$ maximum ranks $in 24816$ and a
range of core dimensions. We compare against a target BLEU test score of 24.0
obtained by our benchmark run. For the IWSLT English-Vietnamese training we
obtain BLEU test/dev scores of 24.0/21.9 and 24.2/21.9 using core dimensions
$(2 2 256) times (2 2 512)$ with learning rate 0.0012 and rank
distributions $(1441)$ and $(14161)$ respectively. These runs use 113%
and 397% of the flops of the benchmark run respectively. We find that of the
parameters surveyed a higher learning rate and more `rectangular' core
dimensions generally produce higher BLEU scores. For the WMT German-English
dataset we obtain BLEU scores of 24.0/23.8 using core dimensions $(4 4 128)
times (4 4 256)$ with learning rate 0.0012 and rank distribution
$(1221)$. We discuss the potential for future optimization and application
of Tensor Train decomposition to other NMT models.
",0,1,0
Attention-based Ensemble for Deep Metric Learning,"  Deep metric learning aims to learn an embedding function modeled as deep
neural network. This embedding function usually puts semantically similar
images close while dissimilar images far from each other in the learned
embedding space. Recently ensemble has been applied to deep metric learning to
yield state-of-the-art results. As one important aspect of ensemble the
learners should be diverse in their feature embeddings. To this end we propose
an attention-based ensemble which uses multiple attention masks so that each
learner can attend to different parts of the object. We also propose a
divergence loss which encourages diversity among the learners. The proposed
method is applied to the standard benchmarks of deep metric learning and
experimental results show that it outperforms the state-of-the-art methods by a
significant margin on image retrieval tasks.
",0,0,1
"Understanding Character Recognition using Visual Explanations Derived
  from the Human Visual System and Deep Networks","  Human observers engage in selective information uptake when classifying
visual patterns. The same is true of deep neural networks which currently
constitute the best performing artificial vision systems. Our goal is to
examine the congruence or lack thereof in the information-gathering
strategies of the two systems. We have operationalized our investigation as a
character recognition task. We have used eye-tracking to assay the spatial
distribution of information hotspots for humans via fixation maps and an
activation mapping technique for obtaining analogous distributions for deep
networks through visualization maps. Qualitative comparison between
visualization maps and fixation maps reveals an interesting correlate of
congruence. The deep learning model considered similar regions in character
which humans have fixated in the case of correctly classified characters. On
the other hand when the focused regions are different for humans and deep
nets the characters are typically misclassified by the latter. Hence we
propose to use the visual fixation maps obtained from the eye-tracking
experiment as a supervisory input to align the model's focus on relevant
character regions. We find that such supervision improves the model's
performance significantly and does not require any additional parameters. This
approach has the potential to find applications in diverse domains such as
medical analysis and surveillance in which explainability helps to determine
system fidelity.
",0,0,1
"Towards one-shot learning for rare-word translation with external
  experts","  Neural machine translation (NMT) has significantly improved the quality of
automatic translation models. One of the main challenges in current systems is
the translation of rare words. We present a generic approach to address this
weakness by having external models annotate the training data as Experts and
control the model-expert interaction with a pointer network and reinforcement
learning. Our experiments using phrase-based models to simulate Experts to
complement neural machine translation models show that the model can be trained
to copy the annotations into the output consistently. We demonstrate the
benefit of our proposed framework in outof-domain translation scenarios with
only lexical resources improving more than 1.0 BLEU point in both translation
directions English to Spanish and German to English
",0,1,0
"Finite-SNR Bounds on the Sum-Rate Capacity of Rayleigh Block-Fading
  Multiple-Access Channels with no a Priori CSI","  We provide nonasymptotic upper and lower bounds on the sum-rate capacity of
Rayleigh block-fading multiple-access channels for the setup where a priori
channel state information is not available. The upper bound relies on a dual
formula for channel capacity and on the assumption that the users can cooperate
perfectly. The lower bound is derived assuming a noncooperative scenario where
each user employs unitary space-time modulation (independently from the other
users). Numerical results show that the gap between the upper and the lower
bound is small already at moderate SNR values. This suggests that the sum-rate
capacity gains obtainable through user cooperation are minimal.
",1,0,0
Towards Understanding the Effect of Leak in Spiking Neural Networks,"  Spiking Neural Networks (SNNs) are being explored to emulate the astounding
capabilities of human brain that can learn and compute functions robustly and
efficiently with noisy spiking activities. A variety of spiking neuron models
have been proposed to resemble biological neuronal functionalities. With
varying levels of bio-fidelity these models often contain a leak path in their
internal states called membrane potentials. While the leaky models have been
argued as more bioplausible a comparative analysis between models with and
without leak from a purely computational point of view demands attention. In
this paper we investigate the questions regarding the justification of leak
and the pros and cons of using leaky behavior. Our experimental results reveal
that leaky neuron model provides improved robustness and better generalization
compared to models with no leak. However leak decreases the sparsity of
computation contrary to the common notion. Through a frequency domain analysis
we demonstrate the effect of leak in eliminating the high-frequency components
from the input thus enabling SNNs to be more robust against noisy
spike-inputs.
",0,0,1
Learning Depth with Convolutional Spatial Propagation Network,"  Depth prediction is one of the fundamental problems in computer vision. In
this paper we propose a simple yet effective convolutional spatial propagation
network (CSPN) to learn the affinity matrix for various depth estimation tasks.
Specifically it is an efficient linear propagation model in which the
propagation is performed with a manner of recurrent convolutional operation
and the affinity among neighboring pixels is learned through a deep
convolutional neural network (CNN). We can append this module to any output
from a state-of-the-art (SOTA) depth estimation networks to improve their
performances. In practice we further extend CSPN in two aspects: 1) take
sparse depth map as additional input which is useful for the task of depth
completion; 2) similar to commonly used 3D convolution operation in CNNs we
propose 3D CSPN to handle features with one additional dimension which is
effective in the task of stereo matching using 3D cost volume. For the tasks of
sparse to dense a.k.a depth completion. We experimented the proposed CPSN
conjunct algorithms over the popular NYU v2 and KITTI datasets where we show
that our proposed algorithms not only produce high quality (e.g. 30% more
reduction in depth error) but also run faster (e.g. 2 to 5x faster) than
previous SOTA spatial propagation network. We also evaluated our stereo
matching algorithm on the Scene Flow and KITTI Stereo datasets and rank 1st on
both the KITTI Stereo 2012 and 2015 benchmarks which demonstrates the
effectiveness of the proposed module. The code of CSPN proposed in this work
will be released at https://github.com/XinJCheng/CSPN.
",0,0,1
"Channel Equalization and Beamforming for Quaternion-Valued Wireless
  Communication Systems","  Quaternion-valued wireless communication systems have been studied in the
past. Although progress has been made in this promising area a crucial missing
link is lack of effective and efficient quaternion-valued signal processing
algorithms for channel equalization and beamforming. With most recent
developments in quaternion-valued signal processing in this work we fill the
gap to solve the problem by studying two quaternion-valued adaptive algorithms:
one is the reference signal based quaternion-valued least mean square (QLMS)
algorithm and the other one is the quaternion-valued constant modulus algorithm
(QCMA). The quaternion-valued Wiener solution for possible block-based
calculation is also derived. Simulation results are provided to show the
working of the system.
",1,0,0
"Resource Allocation for Downlink Channel Transmission Based on
  Superposition Coding","  We analyze the problem of transmitting information to multiple users over a
shared wireless channel. The problem of resource allocation (RA) for the users
with the knowledge of their channel state information has been treated
extensively in the literature where various approaches trading off the users'
throughput and fairness were proposed. The emphasis was mostly on the
time-sharing (TS) approach where the resource allocated to the user is
equivalent to its time share of the channel access. In this work we propose to
take advantage of the broadcast nature of the channel and we adopt
superposition coding (SC)-known to outperform TS in multiple users broadcasting
scenarios. In SC users' messages are simultaneously transmitted by superposing
their codewords with different power fractions under a total power constraint.
The main challenge is to find a simple way to allocate these power fractions to
all users taking into account the fairness/throughput tradeoff. We present an
algorithm with this purpose and we apply it in the case of popular proportional
fairness (PF). The obtained results using SC are illustrated with various
numerical examples where comparing to TS a rate increase between 20% and 300%
is observed.
",1,0,0
"Decidability of cutpoint isolation for probabilistic finite automata on
  letter-bounded inputs","  We show the surprising result that the cutpoint isolation problem is
decidable for Probabilistic Finite Automata (PFA) where input words are taken
from a letter-bounded context-free language. A context-free language
$mathcalL$ is letter-bounded when $mathcalL subseteq a_1^*a_2^* cdots
a_ell^*$ for some finite $ell > 0$ where each letter is distinct. A cutpoint
is isolated when it cannot be approached arbitrarily closely. The decidability
of this problem is in marked contrast to the situation for the (strict)
emptiness problem for PFA which is undecidable under the even more severe
restrictions of PFA with polynomial ambiguity commutative matrices and input
over a letter-bounded language as well as to the injectivity problem which is
undecidable for PFA over letter-bounded languages. We provide a constructive
nondeterministic algorithm to solve the cutpoint isolation problem which holds
even when the PFA is exponentially ambiguous. We also show that the problem is
at least NP-hard and use our decision procedure to solve several related
problems.
",0,1,0
"Joint Device Positioning and Clock Synchronization in 5G Ultra-Dense
  Networks","  In this article we address the prospects and key enabling technologies for
highly efficient and accurate device positioning and tracking in 5G radio
access networks. Building on the premises of ultra-dense networks as well as on
the adoption of multicarrier waveforms and antenna arrays in the access nodes
(ANs) we first formulate extended Kalman filter (EKF)-based solutions for
computationally efficient joint estimation and tracking of the time of arrival
(ToA) and direction of arrival (DoA) of the user nodes (UNs) using uplink
reference signals. Then a second EKF stage is proposed in order to fuse the
individual DoA/ToA estimates from one or several ANs into a UN position
estimate. Since all the processing takes place at the network side the
computing complexity and energy consumption at the UN side are kept to a
minimum. The cascaded EKFs proposed in this article also take into account the
unavoidable relative clock offsets between UNs and ANs such that reliable
clock synchronization of the access-link is obtained as a valuable by-product.
The proposed cascaded EKF scheme is then revised and extended to more general
and challenging scenarios where not only the UNs have clock offsets against the
network time but also the ANs themselves are not mutually synchronized in
time. Finally comprehensive performance evaluations of the proposed solutions
on a realistic 5G network setup building on the METIS project based outdoor
Madrid map model together with complete ray tracing based propagation modeling
are provided. The obtained results clearly demonstrate that by using the
developed methods sub-meter scale positioning and tracking accuracy of moving
devices is indeed technically feasible in future 5G radio access networks
operating at sub-6GHz frequencies despite the realistic assumptions related to
clock offsets and potentially even under unsynchronized network elements.
",1,0,0
Convergence Analysis of MAP based Blur Kernel Estimation,"  One popular approach for blind deconvolution is to formulate a maximum a
posteriori (MAP) problem with sparsity priors on the gradients of the latent
image and then alternatingly estimate the blur kernel and the latent image.
While several successful MAP based methods have been proposed there has been
much controversy and confusion about their convergence because sparsity priors
have been shown to prefer blurry images to sharp natural images. In this paper
we revisit this problem and provide an analysis on the convergence of MAP based
approaches. We first introduce a slight modification to a conventional joint
energy function for blind deconvolution. The reformulated energy function
yields the same alternating estimation process but more clearly reveals how
blind deconvolution works. We then show the energy function can actually favor
the right solution instead of the no-blur solution under certain conditions
which explains the success of previous MAP based approaches. The reformulated
energy function and our conditions for the convergence also provide a way to
compare the qualities of different blur kernels and we demonstrate its
applicability to automatic blur kernel size selection blur kernel estimation
using light streaks and defocus estimation.
",0,0,1
Zero-Shot Detection,"  As we move towards large-scale object detection it is unrealistic to expect
annotated training data in the form of bounding box annotations around
objects for all object classes at sufficient scale and so methods capable of
unseen object detection are required. We propose a novel zero-shot method based
on training an end-to-end model that fuses semantic attribute prediction with
visual features to propose object bounding boxes for seen and unseen classes.
While we utilize semantic features during training our method is agnostic to
semantic information for unseen classes at test-time. Our method retains the
efficiency and effectiveness of YOLOv2 for objects seen during training while
improving its performance for novel and unseen objects. The ability of
state-of-art detection methods to learn discriminative object features to
reject background proposals also limits their performance for unseen objects.
We posit that to detect unseen objects we must incorporate semantic
information into the visual domain so that the learned visual features reflect
this information and leads to improved recall rates for unseen objects. We test
our method on PASCAL VOC and MS COCO dataset and observed significant
improvements on the average precision of unseen classes.
",0,0,1
Semantic Neural Machine Translation using AMR,"  It is intuitive that semantic representations can be useful for machine
translation mainly because they can help in enforcing meaning preservation and
handling data sparsity (many sentences correspond to one meaning) of machine
translation models. On the other hand little work has been done on leveraging
semantics for neural machine translation (NMT). In this work we study the
usefulness of AMR (short for abstract meaning representation) on NMT.
Experiments on a standard English-to-German dataset show that incorporating AMR
as additional knowledge can significantly improve a strong attention-based
sequence-to-sequence neural translation model.
",0,1,0
Are All Languages Created Equal in Multilingual BERT?,"  Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly
good cross-lingual performance on several NLP tasks even without explicit
cross-lingual signals. However these evaluations have focused on cross-lingual
transfer with high-resource languages covering only a third of the languages
covered by mBERT. We explore how mBERT performs on a much wider set of
languages focusing on the quality of representation for low-resource
languages measured by within-language performance. We consider three tasks:
Named Entity Recognition (99 languages) Part-of-speech Tagging and Dependency
Parsing (54 languages each). mBERT does better than or comparable to baselines
on high resource languages but does much worse for low resource languages.
Furthermore monolingual BERT models for these languages do even worse. Paired
with similar languages the performance gap between monolingual BERT and mBERT
can be narrowed. We find that better models for low resource languages require
more efficient pretraining techniques or more data.
",0,1,0
On Rate-Splitting by a Secondary Link in Multiple Access Primary Network,"  An achievable rate region is obtained for a primary multiple access network
coexisting with a secondary link of one transmitter and a corresponding
receiver. The rate region depicts the sum primary rate versus the secondary
rate and is established assuming that the secondary link performs
rate-splitting. The achievable rate region is the union of two types of
achievable rate regions. The first type is a rate region established assuming
that the secondary receiver cannot decode any primary signal whereas the
second is established assuming that the secondary receiver can decode the
signal of one primary receiver. The achievable rate region is determined first
assuming discrete memoryless channel (DMC) then the results are applied to a
Gaussian channel. In the Gaussian channel the performance of rate-splitting is
characterized for the two types of rate regions. Moreover a necessary and
sufficient condition to determine which primary signal that the secondary
receiver can decode without degrading the range of primary achievable sum rates
is provided. When this condition is satisfied by a certain primary user the
secondary receiver can decode its signal and achieve larger rates without
reducing the primary achievable sum rates from the case in which it does not
decode any primary signal. It is also shown that the probability of having at
least one primary user satisfying this condition grows with the primary signal
to noise ratio.
",1,0,0
"Iris segmentation techniques to recognize the behavior of a vigilant
  driver","  In this paper we clarify how to recognize different levels of vigilance for
vehicle drivers. In order to avoid the classical problems of crisp logic we
preferred to employ a fuzzy logic-based system that depends on two variables to
make the final decision. Two iris segmentation techniques are well illustrated.
A new technique for pupil position detection is also provided here with the
possibility to correct the pupil detected position when dealing with some noisy
cases.
",0,0,1
"A theoretical contribution to the fast implementation of null linear
  discriminant analysis method using random matrix multiplication with scatter
  matrices","  The null linear discriminant analysis method is a competitive approach for
dimensionality reduction. The implementation of this method however is
computationally expensive. Recently a fast implementation of null linear
discriminant analysis method using random matrix multiplication with scatter
matrices was proposed. However if the random matrix is chosen arbitrarily the
orientation matrix may be rank deficient and some useful discriminant
information will be lost. In this paper we investigate how to choose the
random matrix properly such that the two criteria of the null LDA method are
satisfied theoretically. We give a necessary and sufficient condition to
guarantee full column rank of the orientation matrix. Moreover the geometric
characterization of the condition is also described.
",0,0,1
"Improving Long Handwritten Text Line Recognition with Convolutional
  Multi-way Associative Memory","  Convolutional Recurrent Neural Networks (CRNNs) excel at scene text
recognition. Unfortunately they are likely to suffer from vanishing/exploding
gradient problems when processing long text images which are commonly found in
scanned documents. This poses a major challenge to goal of completely solving
Optical Character Recognition (OCR) problem. Inspired by recently proposed
memory-augmented neural networks (MANNs) for long-term sequential modeling we
present a new architecture dubbed Convolutional Multi-way Associative Memory
(CMAM) to tackle the limitation of current CRNNs. By leveraging recent memory
accessing mechanisms in MANNs our architecture demonstrates superior
performance against other CRNN counterparts in three real-world long text OCR
datasets.
",0,0,1
Model Pruning Based on Quantified Similarity of Feature Maps,"  A high-accuracy CNN is often accompanied by huge parameters which are
usually stored in the high-dimensional tensors. However there are few methods
can figure out the redundant information of the parameters stored in the
high-dimensional tensors which leads to the lack of theoretical guidance for
the compression of CNNs. In this paper we propose a novel theory to find
redundant information in three dimensional tensors namely Quantified
Similarity of Feature Maps (QSFM) and use this theory to prune convolutional
neural networks to enhance the inference speed. Our method belongs to filter
pruning which can be implemented without using any special libraries. We
perform our method not only on common convolution layers but also on special
convolution layers such as depthwise separable convolution layers. The
experiments prove that QSFM can find the redundant information in the neural
network effectively. Without any fine-tuning operation QSFM can compress
ResNet-56 on CIFAR-10 significantly (48.27% FLOPs and 57.90% parameters
reduction) with only a loss of 0.54% in the top-1 accuracy. QSFM also prunes
ResNet-56 VGG-16 and MobileNetV2 with fine-tuning operation which also shows
excellent results.
",0,0,1
On the Construction of Nonbinary Quantum BCH Codes,"  Four quantum code constructions generating several new families of good
nonbinary quantum nonprimitive non-narrow-sense Bose-Chaudhuri-Hocquenghem
(BCH) codes are presented in this paper. The first two ones are based on
Calderbank-Shor-Steane (CSS) construction derived from two nonprimitive BCH
codes not necessarily self-orthogonal. The third one is based on nonbinary
Steane's enlargement of CSS codes applied to suitable sub-families of
nonprimitive non-narrow-sense BCH codes. The fourth construction is derived
from suitable sub-families of Hermitian self-orthogonal nonprimitive
non-narrow-sense BCH codes. These constructions generate new families of
quantum BCH codes whose parameters are better than the ones available in the
literature.
",1,0,0
"Comments on Proakis Analysis of the Characteristic Function of Complex
  Gaussian Quadratic Forms","  An analysis of the characteristic function of Gaussian quadratic forms is
presented in [1] to study the performance of multichannel communication
systems. This technical report reviews this analysis obtaining alternative
expressions to original ones in compact matrix format.
",1,0,0
Causal Erasure Channels,"  We consider the communication problem over binary causal adversarial erasure
channels. Such a channel maps $n$ input bits to $n$ output symbols in
$01wedge$ where $wedge$ denotes erasure. The channel is causal if for
every $i$ the channel adversarially decides whether to erase the $i$th bit of
its input based on inputs $1...i$ before it observes bits $i+1$ to $n$. Such
a channel is $p$-bounded if it can erase at most a $p$ fraction of the input
bits over the whole transmission duration. Causal channels provide a natural
model for channels that obey basic physical restrictions but are otherwise
unpredictable or highly variable. For a given erasure rate $p$ our goal is to
understand the optimal rate (the capacity) at which a randomized (stochastic)
encoder/decoder can transmit reliably across all causal $p$-bounded erasure
channels. In this paper we introduce the causal erasure model and provide new
upper bounds and lower bounds on the achievable rate. Our bounds separate the
achievable rate in the causal erasures setting from the rates achievable in two
related models: random erasure channels (strictly weaker) and fully adversarial
erasure channels (strictly stronger). Specifically we show:
  - A strict separation between random and causal erasures for all constant
erasure rates $pin(01)$.
  - A strict separation between causal and fully adversarial erasures for
$pin(0phi)$ where $phi approx 0.348$.
  - For $pin[phi1/2)$ we show codes for causal erasures that have higher
rate than the best known constructions for fully adversarial channels.
  Our results contrast with existing results on correcting causal bit-flip
errors (as opposed to erasures) [Dey et. al 2008 2009] [Haviv-Langberg 2011].
For the separations we provide the analogous separations for bit-flip models
are either not known at all or much weaker.
",1,0,0
"Cloud Radio Access meets Heterogeneous Small Cell Networks: A Cognitive
  Hierarchy Perspective","  In this paper the problem of distributed power allocation is considered for
the downlink of a cloud radio access network (CRAN) that is coexisting with a
heterogeneous network. In this multi-tier system the heterogeneous network
base stations (BSs) as well as the CRAN remote radio heads seek to choose their
optimal power to maximize their users' rates. The problem is formulated as a
noncooperative game in which the players are the CRAN's cloud and the BSs.
Given the difference of capabilities between the CRAN and the various BSs the
game is cast within the framework of cognitive hierarchy theory. In this
framework players are organized in a hierarchy in such a way that a player can
choose its strategy while considering players of only similar or lower
hierarchies. Using such a hierarchical design one can reduce the interference
caused by the CRAN and high-powered base stations on low-powered BSs. For this
game the properties of the Nash equilibrium and the cognitive hierarchy
equilibrium are analyzed. Simulation results show that the proposed cognitive
hierarchy model yields significant performance gains in terms of the total
rate reaching up to twice the rate achieved by a classical noncooperative
game's Nash equilibrium.
",1,0,0
"Rapid treatment planning for low-dose-rate prostate brachytherapy with
  TP-GAN","  Treatment planning in low-dose-rate prostate brachytherapy (LDR-PB) aims to
produce arrangement of implantable radioactive seeds that deliver a minimum
prescribed dose to the prostate whilst minimizing toxicity to healthy tissues.
There can be multiple seed arrangements that satisfy this dosimetric criterion
not all deemed 'acceptable' for implant from a physician's perspective. This
leads to plans that are subjective to the physician's/centre's preference
planning style and expertise. We propose a method that aims to reduce this
variability by training a model to learn from a large pool of successful
retrospective LDR-PB data (961 patients) and create consistent plans that mimic
the high-quality manual plans. Our model is based on conditional generative
adversarial networks that use a novel loss function for penalizing the model on
spatial constraints of the seeds. An optional optimizer based on a simulated
annealing (SA) algorithm can be used to further fine-tune the plans if
necessary (determined by the treating physician). Performance analysis was
conducted on 150 test cases demonstrating comparable results to that of the
manual prehistorical plans. On average the clinical target volume covering
100% of the prescribed dose was 98.9% for our method compared to 99.4% for
manual plans. Moreover using our model the planning time was significantly
reduced to an average of 2.5 mins/plan with SA and less than 3 seconds without
SA. Compared to this manual planning at our centre takes around 20 mins/plan.
",0,0,1
Efficient-CapsNet: Capsule Network with Self-Attention Routing,"  Deep convolutional neural networks assisted by architectural design
strategies make extensive use of data augmentation techniques and layers with
a high number of feature maps to embed object transformations. That is highly
inefficient and for large datasets implies a massive redundancy of features
detectors. Even though capsules networks are still in their infancy they
constitute a promising solution to extend current convolutional networks and
endow artificial visual perception with a process to encode more efficiently
all feature affine transformations. Indeed a properly working capsule network
should theoretically achieve higher results with a considerably lower number of
parameters count due to intrinsic capability to generalize to novel viewpoints.
Nevertheless little attention has been given to this relevant aspect. In this
paper we investigate the efficiency of capsule networks and pushing their
capacity to the limits with an extreme architecture with barely 160K
parameters we prove that the proposed architecture is still able to achieve
state-of-the-art results on three different datasets with only 2% of the
original CapsNet parameters. Moreover we replace dynamic routing with a novel
non-iterative highly parallelizable routing algorithm that can easily cope
with a reduced number of capsules. Extensive experimentation with other capsule
implementations has proved the effectiveness of our methodology and the
capability of capsule networks to efficiently embed visual representations more
prone to generalization.
",0,0,1
"Capacity Region of Multiple Access Channel with States Known Noncausally
  at One Encoder and Only Strictly Causally at the Other Encoder","  We consider a two-user state-dependent multiaccess channel in which the
states of the channel are known non-causally to one of the encoders and only
strictly causally to the other encoder. Both encoders transmit a common message
and in addition the encoder that knows the states non-causally transmits an
individual message. We find explicit characterizations of the capacity region
of this communication model in both discrete memoryless (DM) and memoryless
Gaussian cases. In particular the capacity region analysis demonstrates the
utility of the knowledge of the states only strictly causally at the encoder
that sends only the common message in general. More specifically in the DM
setting we show that such a knowledge is beneficial and increases the capacity
region in general. In the Gaussian setting we show that such a knowledge does
not help and the capacity is same as if the states were completely unknown at
the encoder that sends only the common message. The analysis also reveals
optimal ways of exploiting the knowledge of the state only strictly causally at
the encoder that sends only the common message when such a knowledge is
beneficial. The encoders collaborate to convey to the decoder a lossy version
of the state in addition to transmitting the information messages through a
generalized Gel'fand-Pinsker binning. Particularly important in this problem
are the questions of 1) optimal ways of performing the state compression and 2)
whether or not the compression indices should be decoded uniquely. We show that
both compression `a-la noisy network coding i.e. with no binning and
non-unique decoding and compression using Wyner-Ziv binning with backward
decoding and non-unique or unique decoding are optimal.
",1,0,0
MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation,"  Cross view feature fusion is the key to address the occlusion problem in
human pose estimation. The current fusion methods need to train a separate
model for every pair of cameras making them difficult to scale. In this work
we introduce MetaFuse a pre-trained fusion model learned from a large number
of cameras in the Panoptic dataset. The model can be efficiently adapted or
finetuned for a new pair of cameras using a small number of labeled images. The
strong adaptation power of MetaFuse is due in large part to the proposed
factorization of the original fusion model into two parts (1) a generic fusion
model shared by all cameras and (2) lightweight camera-dependent
transformations. Furthermore the generic model is learned from many cameras by
a meta-learning style algorithm to maximize its adaptation capability to
various camera poses. We observe in experiments that MetaFuse finetuned on the
public datasets outperforms the state-of-the-arts by a large margin which
validates its value in practice.
",0,0,1
"Joint Cell Nuclei Detection and Segmentation in Microscopy Images Using
  3D Convolutional Networks","  We propose a 3D convolutional neural network to simultaneously segment and
detect cell nuclei in confocal microscopy images. Mirroring the co-dependency
of these tasks our proposed model consists of two serial components: the first
part computes a segmentation of cell bodies while the second module identifies
the centers of these cells. Our model is trained end-to-end from scratch on a
mouse parotid salivary gland stem cell nuclei dataset comprising 107 image
stacks from three independent cell preparations each containing several
hundred individual cell nuclei in 3D. In our experiments we conduct a thorough
evaluation of both detection accuracy and segmentation quality on two
different datasets. The results show that the proposed method provides
significantly improved detection and segmentation accuracy compared to
state-of-the-art and benchmark algorithms. Finally we use a previously
described test-time drop-out strategy to obtain uncertainty estimates on our
predictions and validate these estimates by demonstrating that they are
strongly correlated with accuracy.
",0,0,1
"MetaHistoSeg: A Python Framework for Meta Learning in Histopathology
  Image Segmentation","  Few-shot learning is a standard practice in most deep learning based
histopathology image segmentation given the relatively low number of digitized
slides that are generally available. While many models have been developed for
domain specific histopathology image segmentation cross-domain generalization
remains a key challenge for properly validating models. Here tooling and
datasets to benchmark model performance across histopathological domains are
lacking. To address this limitation we introduce MetaHistoSeg - a Python
framework that implements unique scenarios in both meta learning and instance
based transfer learning. Designed for easy extension to customized datasets and
task sampling schemes the framework empowers researchers with the ability of
rapid model design and experimentation. We also curate a histopathology meta
dataset - a benchmark dataset for training and validating models on
out-of-distribution performance across a range of cancer types. In experiments
we showcase the usage of MetaHistoSeg with the meta dataset and find that both
meta-learning and instance based transfer learning deliver comparable results
on average but in some cases tasks can greatly benefit from one over the
other.
",0,0,1
"Hybrid Analog and Digital Precoding: From Practical RF System Models to
  Information Theoretic Bounds","  Hybrid analog-digital precoding is a key millimeter wave access technology
where an antenna array with reduced number of radio frequency (RF) chains is
used with an RF precoding matrix to increase antenna gain at a reasonable cost.
However digital and RF precoder algorithms must be accompa- nied by a detailed
system model of the RF precoder. In this work we provide fundamental RF system
models for these precoders and show their impact on achievable rates. We show
that hybrid precoding systems suffer from significant degradation once the
limitations of RF precoding network are accounted. We subsequently quantify
this performance degradation and use it as a reference for comparing the
performance of different precoding methods. These results indicate that hybrid
precoders must be redesigned (and their rates recomputed) to account for
practical factors.
",1,0,0
"The Data Representativeness Criterion: Predicting the Performance of
  Supervised Classification Based on Data Set Similarity","  In a broad range of fields it may be desirable to reuse a supervised
classification algorithm and apply it to a new data set. However
generalization of such an algorithm and thus achieving a similar classification
performance is only possible when the training data used to build the algorithm
is similar to new unseen data one wishes to apply it to. It is often unknown in
advance how an algorithm will perform on new unseen data being a crucial
reason for not deploying an algorithm at all. Therefore tools are needed to
measure the similarity of data sets. In this paper we propose the Data
Representativeness Criterion (DRC) to determine how representative a training
data set is of a new unseen data set. We present a proof of principle to see
whether the DRC can quantify the similarity of data sets and whether the DRC
relates to the performance of a supervised classification algorithm. We
compared a number of magnetic resonance imaging (MRI) data sets ranging from
subtle to severe difference is acquisition parameters. Results indicate that
based on the similarity of data sets the DRC is able to give an indication as
to when the performance of a supervised classifier decreases. The strictness of
the DRC can be set by the user depending on what one considers to be an
acceptable underperformance.
",0,0,1
"Improving Sentiment Analysis over non-English Tweets using Multilingual
  Transformers and Automatic Translation for Data-Augmentation","  Tweets are specific text data when compared to general text. Although
sentiment analysis over tweets has become very popular in the last decade for
English it is still difficult to find huge annotated corpora for non-English
languages. The recent rise of the transformer models in Natural Language
Processing allows to achieve unparalleled performances in many tasks but these
models need a consequent quantity of text to adapt to the tweet domain. We
propose the use of a multilingual transformer model that we pre-train over
English tweets and apply data-augmentation using automatic translation to adapt
the model to non-English languages. Our experiments in French Spanish German
and Italian suggest that the proposed technique is an efficient way to improve
the results of the transformers over small corpora of tweets in a non-English
language.
",0,1,0
"Small-floating Target Detection in Sea Clutter via Visual Feature
  Classifying in the Time-Doppler Spectra","  It is challenging to detect small-floating object in the sea clutter for a
surface radar. In this paper we have observed that the backscatters from the
target brake the continuity of the underlying motion of the sea surface in the
time-Doppler spectra (TDS) images. Following this visual clue we exploit the
local binary pattern (LBP) to measure the variations of texture in the TDS
images. It is shown that the radar returns containing target and those only
having clutter are separable in the feature space of LBP. An unsupervised
one-class support vector machine (SVM) is then utilized to detect the deviation
of the LBP histogram of the clutter. The outiler of the detector is classified
as the target. In the real-life IPIX radar data sets our visual feature based
detector shows favorable detection rate compared to other three existing
approaches.
",0,0,1
Phone Features Improve Speech Translation,"  End-to-end models for speech translation (ST) more tightly couple speech
recognition (ASR) and machine translation (MT) than a traditional cascade of
separate ASR and MT models with simpler model architectures and the potential
for reduced error propagation. Their performance is often assumed to be
superior though in many conditions this is not yet the case. We compare
cascaded and end-to-end models across high medium and low-resource
conditions and show that cascades remain stronger baselines. Further we
introduce two methods to incorporate phone features into ST models. We show
that these features improve both architectures closing the gap between
end-to-end models and cascades and outperforming previous academic work -- by
up to 9 BLEU on our low-resource setting.
",0,1,0
"Dual Graph Convolutional Networks with Transformer and Curriculum
  Learning for Image Captioning","  Existing image captioning methods just focus on understanding the
relationship between objects or instances in a single image without exploring
the contextual correlation existed among contextual image. In this paper we
propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and
curriculum learning for image captioning. In particular we not only use an
object-level GCN to capture the object to object spatial relation within a
single image but also adopt an image-level GCN to capture the feature
information provided by similar images. With the well-designed Dual-GCN we can
make the linguistic transformer better understand the relationship between
different objects in a single image and make full use of similar images as
auxiliary information to generate a reasonable caption description for a single
image. Meanwhile with a cross-review strategy introduced to determine
difficulty levels we adopt curriculum learning as the training strategy to
increase the robustness and generalization of our proposed model. We conduct
extensive experiments on the large-scale MS COCO dataset and the experimental
results powerfully demonstrate that our proposed method outperforms recent
state-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2
score of 67.6. Our source code is available at em
colormagentaurlhttps://github.com/Unbear430/DGCN-for-image-captioning.
",0,0,1
"End to End Recognition System for Recognizing Offline Unconstrained
  Vietnamese Handwriting","  Inspired by recent successes in neural machine translation and image caption
generation we present an attention based encoder decoder model (AED) to
recognize Vietnamese Handwritten Text. The model composes of two parts: a
DenseNet for extracting invariant features and a Long Short-Term Memory
network (LSTM) with an attention model incorporated for generating output text
(LSTM decoder) which are connected from the CNN part to the attention model.
The input of the CNN part is a handwritten text image and the target of the
LSTM decoder is the corresponding text of the input image. Our model is trained
end-to-end to predict the text from a given input image since all the parts are
differential components. In the experiment section we evaluate our proposed
AED model on the VNOnDB-Word and VNOnDB-Line datasets to verify its efficiency.
The experiential results show that our model achieves 12.30% of word error rate
without using any language model. This result is competitive with the
handwriting recognition system provided by Google in the Vietnamese Online
Handwritten Text Recognition competition.
",0,0,1
"Entropy of the Sum of Two Independent Non-Identically-Distributed
  Exponential Random Variables","  In this letter we give a concise closed-form expression for the
differential entropy of the sum of two independent non-identically-distributed
exponential random variables. The derivation is straightforward but such a
concise entropy has not been previously given in the literature. The usefulness
of the expression is demonstrated with examples.
",1,0,0
"Classifying Component Function in Product Assemblies with Graph Neural
  Networks","  Function is defined as the ensemble of tasks that enable the product to
complete the designed purpose. Functional tools such as functional modeling
offer decision guidance in the early phase of product design where explicit
design decisions are yet to be made. Function-based design data is often sparse
and grounded in individual interpretation. As such function-based design tools
can benefit from automatic function classification to increase data fidelity
and provide function representation models that enable function-based
intelligent design agents. Function-based design data is commonly stored in
manually generated design repositories. These design repositories are a
collection of expert knowledge and interpretations of function in product
design bounded by function-flow and component taxonomies. In this work we
represent a structured taxonomy-based design repository as assembly-flow
graphs then leverage a graph neural network (GNN) model to perform automatic
function classification. We support automated function classification by
learning from repository data to establish the ground truth of component
function assignment. Experimental results show that our GNN model achieves a
micro-average F$_1$-score of 0.832 for tier 1 (broad) 0.756 for tier 2 and
0.783 for tier 3 (specific) functions. Given the imbalance of data features
the results are encouraging. Our efforts in this paper can be a starting point
for more sophisticated applications in knowledge-based CAD systems and
Design-for-X consideration in function-based design.
",0,0,1
"Sparse Progressive Distillation: Resolving Overfitting under
  Pretrain-and-Finetune Paradigm","  Various pruning approaches have been proposed to reduce the footprint
requirements of Transformer-based language models. Conventional wisdom is that
pruning reduces the model expressiveness and thus is more likely to underfit
than overfit compared to the original model. However under the trending
pretrain-and-finetune paradigm we argue that pruning increases the risk of
overfitting if pruning was performed at the fine-tuning phase as it increases
the amount of information a model needs to learn from the downstream task
resulting in relative data deficiency. In this paper we aim to address the
overfitting issue under the pretrain-and-finetune paradigm to improve pruning
performance via progressive knowledge distillation (KD) and sparse pruning.
Furthermore to mitigate the interference between different strategies of
learning rate pruning and distillation we propose a three-stage learning
framework. We show for the first time that reducing the risk of overfitting can
help the effectiveness of pruning under the pretrain-and-finetune paradigm.
Experiments on multiple datasets of GLUE benchmark show that our method
achieves highly competitive pruning performance over the state-of-the-art
competitors across different pruning ratio constraints.
",0,1,0
Scaling Up Online Speech Recognition Using ConvNets,"  We design an online end-to-end speech recognition system based on Time-Depth
Separable (TDS) convolutions and Connectionist Temporal Classification (CTC).
We improve the core TDS architecture in order to limit the future context and
hence reduce latency while maintaining accuracy. The system has almost three
times the throughput of a well tuned hybrid ASR baseline while also having
lower latency and a better word error rate. Also important to the efficiency of
the recognizer is our highly optimized beam search decoder. To show the impact
of our design choices we analyze throughput latency accuracy and discuss
how these metrics can be tuned based on the user requirements.
",0,1,0
"Energy Efficiency of an Unlicensed Wireless Network in the Presence of
  Retransmissions","  This paper analysis the energy efficiency of an unlicensed wireless network
in which retransmission is possible if the transmitted message is decoded in
outage. A wireless sensor network is considered in which the sensor nodes are
unlicensed users of a wireless network which transmit its data in the uplink
channel used by the licensed users. Poisson point process is used to model the
distributions of the nodes and the interference caused by the licensed users
for the sensor nodes. After finding the optimal throughput in the presence of
retransmissions we focus on analyzing the total power consumption and energy
efficiency of the network and how retransmissions network density and outage
threshold affects the energy efficiency of the network.
",1,0,0
"Hate Speech Detection: A Solved Problem? The Challenging Case of Long
  Tail on Twitter","  In recent years the increasing propagation of hate speech on social media
and the urgent need for effective counter-measures have drawn significant
investment from governments companies and researchers. A large number of
methods have been developed for automated hate speech detection online. This
aims to classify textual content into non-hate or hate speech in which case
the method may also identify the targeting characteristics (i.e. types of
hate such as race and religion) in the hate speech. However we notice
significant difference between the performance of the two (i.e. non-hate v.s.
hate). In this work we argue for a focus on the latter problem for practical
reasons. We show that it is a much more challenging task as our analysis of
the language in the typical datasets shows that hate speech lacks unique
discriminative features and therefore is found in the 'long tail' in a dataset
that is difficult to discover. We then propose Deep Neural Network structures
serving as feature extractors that are particularly effective for capturing the
semantics of hate speech. Our methods are evaluated on the largest collection
of hate speech datasets based on Twitter and are shown to be able to
outperform the best performing method by up to 5 percentage points in
macro-average F1 or 8 percentage points in the more challenging case of
identifying hateful content.
",0,1,0
"Semi-supervised transfer learning for language expansion of end-to-end
  speech recognition models to low-resource languages","  In this paper we propose a three-stage training methodology to improve the
speech recognition accuracy of low-resource languages. We explore and propose
an effective combination of techniques such as transfer learning encoder
freezing data augmentation using Text-To-Speech (TTS) and Semi-Supervised
Learning (SSL). To improve the accuracy of a low-resource Italian ASR we
leverage a well-trained English model unlabeled text corpus and unlabeled
audio corpus using transfer learning TTS augmentation and SSL respectively.
In the first stage we use transfer learning from a well-trained English model.
This primarily helps in learning the acoustic information from a resource-rich
language. This stage achieves around 24% relative Word Error Rate (WER)
reduction over the baseline. In stage two We utilize unlabeled text data via
TTS data-augmentation to incorporate language information into the model. We
also explore freezing the acoustic encoder at this stage. TTS data augmentation
helps us further reduce the WER by ~ 21% relatively. Finally In stage three we
reduce the WER by another 4% relative by using SSL from unlabeled audio data.
Overall our two-pass speech recognition system with a Monotonic Chunkwise
Attention (MoChA) in the first pass and a full-attention in the second pass
achieves a WER reduction of ~ 42% relative to the baseline.
",0,1,0
"Generalisable and distinctive 3D local deep descriptors for point cloud
  registration","  An effective 3D descriptor should be invariant to different geometric
transformations such as scale and rotation repeatable in the case of
occlusions and clutter and generalisable in different contexts when data is
captured with different sensors. We present a simple but yet effective method
to learn generalisable and distinctive 3D local descriptors that can be used to
register point clouds captured in different contexts with different sensors.
Point cloud patches are extracted canonicalised with respect to their local
reference frame and encoded into scale and rotation-invariant compact
descriptors by a point permutation-invariant deep neural network. Our
descriptors can effectively generalise across sensor modalities from locally
and randomly sampled points. We evaluate and compare our descriptors with
alternative handcrafted and deep learning-based descriptors on several indoor
and outdoor datasets reconstructed using both RGBD sensors and laser scanners.
Our descriptors outperform most recent descriptors by a large margin in terms
of generalisation and become the state of the art also in benchmarks where
training and testing are performed in the same scenarios.
",0,0,1
Deep speech inpainting of time-frequency masks,"  Transient loud intrusions often occurring in noisy environments can
completely overpower speech signal and lead to an inevitable loss of
information. While existing algorithms for noise suppression can yield
impressive results their efficacy remains limited for very low signal-to-noise
ratios or when parts of the signal are missing. To address these limitations
here we propose an end-to-end framework for speech inpainting the
context-based retrieval of missing or severely distorted parts of
time-frequency representation of speech. The framework is based on a
convolutional U-Net trained via deep feature losses obtained using speechVGG
a deep speech feature extractor pre-trained on an auxiliary word classification
task. Our evaluation results demonstrate that the proposed framework can
recover large portions of missing or distorted time-frequency representation of
speech up to 400 ms and 3.2 kHz in bandwidth. In particular our approach
provided a substantial increase in STOI & PESQ objective metrics of the
initially corrupted speech samples. Notably using deep feature losses to train
the framework led to the best results as compared to conventional approaches.
",0,1,0
Cascaded Head-colliding Attention,"  Transformers have advanced the field of natural language processing (NLP) on
a variety of important tasks. At the cornerstone of the Transformer
architecture is the multi-head attention (MHA) mechanism which models pairwise
interactions between the elements of the sequence. Despite its massive success
the current framework ignores interactions among different heads leading to
the problem that many of the heads are redundant in practice which greatly
wastes the capacity of the model. To improve parameter efficiency we
re-formulate the MHA as a latent variable model from a probabilistic
perspective. We present cascaded head-colliding attention (CODA) which
explicitly models the interactions between attention heads through a
hierarchical variational distribution. We conduct extensive experiments and
demonstrate that CODA outperforms the transformer baseline by $0.6$ perplexity
on textttWikitext-103 in language modeling and by $0.6$ BLEU on
textttWMT14 EN-DE in machine translation due to its improvements on the
parameter efficiency.footnoteOur implementation is publicly available at
urlhttps://github.com/LZhengisme/CODA.
",0,1,0
"A Spatial Guided Self-supervised Clustering Network for Medical Image
  Segmentation","  The segmentation of medical images is a fundamental step in automated
clinical decision support systems. Existing medical image segmentation methods
based on supervised deep learning however remain problematic because of their
reliance on large amounts of labelled training data. Although medical imaging
data repositories continue to expand there has not been a commensurate
increase in the amount of annotated data. Hence we propose a new spatial
guided self-supervised clustering network (SGSCN) for medical image
segmentation where we introduce multiple loss functions designed to aid in
grouping image pixels that are spatially connected and have similar feature
representations. It iteratively learns feature representations and clustering
assignment of each pixel in an end-to-end fashion from a single image. We also
propose a context-based consistency loss that better delineates the shape and
boundaries of image regions. It enforces all the pixels belonging to a cluster
to be spatially close to the cluster centre. We evaluated our method on 2
public medical image datasets and compared it to existing conventional and
self-supervised clustering methods. Experimental results show that our method
was most accurate for medical image segmentation.
",0,0,1
"Fidelity-Controllable Extreme Image Compression with Generative
  Adversarial Networks","  We propose a GAN-based image compression method working at extremely low
bitrates below 0.1bpp. Most existing learned image compression methods suffer
from blur at extremely low bitrates. Although GAN can help to reconstruct sharp
images there are two drawbacks. First GAN makes training unstable. Second
the reconstructions often contain unpleasing noise or artifacts. To address
both of the drawbacks our method adopts two-stage training and network
interpolation. The two-stage training is effective to stabilize the training.
Moreover the network interpolation utilizes the models in both stages and
reduces undesirable noise and artifacts while maintaining important edges.
Hence we can control the trade-off between perceptual quality and fidelity
without re-training models. The experimental results show that our model can
reconstruct high quality images. Furthermore our user study confirms that our
reconstructions are preferable to state-of-the-art GAN-based image compression
model. The code will be available.
",0,0,1
"One-Step Time-Dependent Future Video Frame Prediction with a
  Convolutional Encoder-Decoder Neural Network","  There is an inherent need for autonomous cars drones and other robots to
have a notion of how their environment behaves and to anticipate changes in the
near future. In this work we focus on anticipating future appearance given the
current frame of a video. Existing work focuses on either predicting the future
appearance as the next frame of a video or predicting future motion as optical
flow or motion trajectories starting from a single video frame. This work
stretches the ability of CNNs (Convolutional Neural Networks) to predict an
anticipation of appearance at an arbitrarily given future time not necessarily
the next video frame. We condition our predicted future appearance on a
continuous time variable that allows us to anticipate future frames at a given
temporal distance directly from the input video frame. We show that CNNs can
learn an intrinsic representation of typical appearance changes over time and
successfully generate realistic predictions at a deliberate time difference in
the near future.
",0,0,1
"Distributed Structured Actor-Critic Reinforcement Learning for Universal
  Dialogue Management","  The task-oriented spoken dialogue system (SDS) aims to assist a human user in
accomplishing a specific task (e.g. hotel booking). The dialogue management is
a core part of SDS. There are two main missions in dialogue management:
dialogue belief state tracking (summarising conversation history) and dialogue
decision-making (deciding how to reply to the user). In this work we only
focus on devising a policy that chooses which dialogue action to respond to the
user. The sequential system decision-making process can be abstracted into a
partially observable Markov decision process (POMDP). Under this framework
reinforcement learning approaches can be used for automated policy
optimization. In the past few years there are many deep reinforcement learning
(DRL) algorithms which use neural networks (NN) as function approximators
investigated for dialogue policy.
",0,1,0
"What is usual in unusual videos? Trajectory snippet histograms for
  discovering unusualness","  Unusual events are important as being possible indicators of undesired
consequences. Moreover unusualness in everyday life activities may also be
amusing to watch as proven by the popularity of such videos shared in social
media. Discovery of unusual events in videos is generally attacked as a problem
of finding usual patterns and then separating the ones that do not resemble to
those. In this study we address the problem from the other side and try to
answer what type of patterns are shared among unusual videos that make them
resemble to each other regardless of the ongoing event. With this challenging
problem at hand we propose a novel descriptor to encode the rapid motions in
videos utilizing densely extracted trajectories. The proposed descriptor which
is referred to as trajectory snipped histograms is used to distinguish unusual
videos from usual videos and further exploited to discover snapshots in which
unusualness happen. Experiments on domain specific people falling videos and
unrestricted funny videos show the effectiveness of our method in capturing
unusualness.
",0,0,1
An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma,"  Pelvic ring disruptions result from blunt injury mechanisms and are often
found in patients with multi-system trauma. To grade pelvic fracture severity
in trauma victims based on whole-body CT the Tile AO/OTA classification is
frequently used. Due to the high volume of whole-body trauma CTs generated in
busy trauma centers an automated approach to Tile classification would provide
substantial value e.g. to prioritize the reading queue of the attending
trauma radiologist. In such scenario an automated method should perform
grading based on a transparent process and based on interpretable features to
enable interaction with human readers and lower their workload by offering
insights from a first automated read of the scan. This paper introduces an
automated yet interpretable pelvic trauma decision support system to assist
radiologists in fracture detection and Tile grade classification. The method
operates similarly to human interpretation of CT scans and first detects
distinct pelvic fractures on CT with high specificity using a Faster-RCNN model
that are then interpreted using a structural causal model based on clinical
best practices to infer an initial Tile grade. The Bayesian causal model and
finally the object detector are then queried for likely co-occurring fractures
that may have been rejected initially due to the highly specific operating
point of the detector resulting in an updated list of detected fractures and
corresponding final Tile grade. Our method is transparent in that it provides
finding location and type using the object detector as well as information on
important counterfactuals that would invalidate the system's recommendation and
achieves an AUC of 83.3%/85.1% for translational/rotational instability.
Despite being designed for human-machine teaming our approach does not
compromise on performance compared to previous black-box approaches.
",0,0,1
Distributed Source Coding Using Continuous-Valued Syndromes,"  This paper addresses the problem of coding a continuous random source
correlated with another source which is only available at the decoder. The
proposed approach is based on the extension of the channel coding concept of
syndrome from the discrete into the continuous domain. If the correlation
between the sources can be described by an additive Gaussian backward channel
and capacity-achieving linear codes are employed it is shown that the
performance of the system is asymptotically close to the Wyner-Ziv bound. Even
if such an additive channel is not Gaussian the design procedure can fit the
desired correlation and transmission rate. Experiments based on trellis-coded
quantization show that the proposed system achieves a performance within 3-4 dB
of the theoretical bound in the 0.5-3 bit/sample rate range for any Gaussian
correlation with a reasonable computational complexity.
",1,0,0
Span-based Localizing Network for Natural Language Video Localization,"  Given an untrimmed video and a text query natural language video
localization (NLVL) is to locate a matching span from the video that
semantically corresponds to the query. Existing solutions formulate NLVL either
as a ranking task and apply multimodal matching architecture or as a
regression task to directly regress the target video span. In this work we
address NLVL task with a span-based QA approach by treating the input video as
text passage. We propose a video span localizing network (VSLNet) on top of
the standard span-based QA framework to address NLVL. The proposed VSLNet
tackles the differences between NLVL and span-based QA through a simple yet
effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to
search for matching video span within a highlighted region. Through extensive
experiments on three benchmark datasets we show that the proposed VSLNet
outperforms the state-of-the-art methods; and adopting span-based QA framework
is a promising direction to solve NLVL.
",0,0,1
Deep Investigation of Cross-Language Plagiarism Detection Methods,"  This paper is a deep investigation of cross-language plagiarism detection
methods on a new recently introduced open dataset which contains parallel and
comparable collections of documents with multiple characteristics (different
genres languages and sizes of texts). We investigate cross-language plagiarism
detection methods for 6 language pairs on 2 granularities of text units in
order to draw robust conclusions on the best methods while deeply analyzing
correlations across document styles and languages.
",0,1,0
"Adaptive Geometric Multiscale Approximations for Intrinsically
  Low-dimensional Data","  We consider the problem of efficiently approximating and encoding
high-dimensional data sampled from a probability distribution $rho$ in
$mathbbR^D$ that is nearly supported on a $d$-dimensional set $mathcalM$
- for example supported on a $d$-dimensional Riemannian manifold. Geometric
Multi-Resolution Analysis (GMRA) provides a robust and computationally
efficient procedure to construct low-dimensional geometric approximations of
$mathcalM$ at varying resolutions. We introduce a thresholding algorithm on
the geometric wavelet coefficients leading to what we call adaptive GMRA
approximations. We show that these data-driven empirical approximations
perform well when the threshold is chosen as a suitable universal function of
the number of samples $n$ on a wide variety of measures $rho$ that are
allowed to exhibit different regularity at different scales and locations
thereby efficiently encoding data from more complex measures than those
supported on manifolds. These approximations yield a data-driven dictionary
together with a fast transform mapping data to coefficients and an inverse of
such a map. The algorithms for both the dictionary construction and the
transforms have complexity $C n log n$ with the constant linear in $D$ and
exponential in $d$. Our work therefore establishes adaptive GMRA as a fast
dictionary learning algorithm with approximation guarantees. We include several
numerical experiments on both synthetic and real data confirming our
theoretical results and demonstrating the effectiveness of adaptive GMRA.
",1,0,0
Advances of Transformer-Based Models for News Headline Generation,"  Pretrained language models based on Transformer architecture are the reason
for recent breakthroughs in many areas of NLP including sentiment analysis
question answering named entity recognition. Headline generation is a special
kind of text summarization task. Models need to have strong natural language
understanding that goes beyond the meaning of individual words and sentences
and an ability to distinguish essential information to succeed in it. In this
paper we fine-tune two pretrained Transformer-based models (mBART and
BertSumAbs) for that task and achieve new state-of-the-art results on the RIA
and Lenta datasets of Russian news. BertSumAbs increases ROUGE on average by
2.9 and 2.0 points respectively over previous best score achieved by
Phrase-Based Attentional Transformer and CopyNet.
",0,1,0
Neural Rendering and Reenactment of Human Actor Videos,"  We propose a method for generating video-realistic animations of real humans
under user control. In contrast to conventional human character rendering we
do not require the availability of a production-quality photo-realistic 3D
model of the human but instead rely on a video sequence in conjunction with a
(medium-quality) controllable 3D template model of the person. With that our
approach significantly reduces production cost compared to conventional
rendering approaches based on production-quality 3D models and can also be
used to realistically edit existing videos. Technically this is achieved by
training a neural network that translates simple synthetic images of a human
character into realistic imagery. For training our networks we first track the
3D motion of the person in the video using the template model and subsequently
generate a synthetically rendered version of the video. These images are then
used to train a conditional generative adversarial network that translates
synthetic images of the 3D model into realistic imagery of the human. We
evaluate our method for the reenactment of another person that is tracked in
order to obtain the motion data and show video results generated from
artist-designed skeleton motion. Our results outperform the state-of-the-art in
learning-based human image synthesis. Project page:
http://gvv.mpi-inf.mpg.de/projects/wxu/HumanReenactment/
",0,0,1
"VML-MOC: Segmenting a multiply oriented and curved handwritten text
  lines dataset","  This paper publishes a natural and very complicated dataset of handwritten
documents with multiply oriented and curved text lines namely VML-MOC dataset.
These text lines were written as remarks on the page margins by different
writers over the years. They appear at different locations within the
orientations that range between 0 and 180 or as curvilinear forms. We evaluate
a multi-oriented Gaussian based method to segment these handwritten text lines
that are skewed or curved in any orientation. It achieves a mean pixel
Intersection over Union score of 80.96% on the test documents. The results are
compared with the results of a single-oriented Gaussian based text line
segmentation method.
",0,0,1
Learning Rotation for Kernel Correlation Filter,"  Kernel Correlation Filters have shown a very promising scheme for visual
tracking in terms of speed and accuracy on several benchmarks. However it
suffers from problems that affect its performance like occlusion rotation and
scale change. This paper tries to tackle the problem of rotation by
reformulating the optimization problem for learning the correlation filter.
This modification (RKCF) includes learning rotation filter that utilizes
circulant structure of HOG feature to guesstimate rotation from one frame to
another and enhance the detection of KCF. Hence it gains boost in overall
accuracy in many of OBT50 detest videos with minimal additional computation.
",0,0,1
"Watset: Local-Global Graph Clustering with Applications in Sense and
  Frame Induction","  We present a detailed theoretical and computational analysis of the Watset
meta-algorithm for fuzzy graph clustering which has been found to be widely
applicable in a variety of domains. This algorithm creates an intermediate
representation of the input graph that reflects the ""ambiguity"" of its nodes.
Then it uses hard clustering to discover clusters in this ""disambiguated""
intermediate graph. After outlining the approach and analyzing its
computational complexity we demonstrate that Watset shows competitive results
in three applications: unsupervised synset induction from a synonymy graph
unsupervised semantic frame induction from dependency triples and unsupervised
semantic class induction from a distributional thesaurus. Our algorithm is
generic and can be also applied to other networks of linguistic data.
",0,1,0
Bounds on Codes with Locality and Availability,"  In this paper we investigate bounds on rate and minimum distance of codes
with $t$ availability. We present bounds on minimum distance of a code with $t$
availability that are tighter than existing bounds. For bounds on rate of a
code with $t$ availability we restrict ourselves to a sub-class of codes with
$t$ availability called codes with strict $t$ availability and derive a tighter
rate bound. Codes with strict $t$ availability can be defined as the null space
of an $(m times n)$ parity-check matrix $H$ where each row has weight $(r+1)$
and each column has weight $t$ with intersection between support of any two
rows atmost one. We also present two general constructions for codes with $t$
availability.
",1,0,0
Design of Real-time Semantic Segmentation Decoder for Automated Driving,"  Semantic segmentation remains a computationally intensive algorithm for
embedded deployment even with the rapid growth of computation power. Thus
efficient network design is a critical aspect especially for applications like
automated driving which requires real-time performance. Recently there has
been a lot of research on designing efficient encoders that are mostly task
agnostic. Unlike image classification and bounding box object detection tasks
decoders are computationally expensive as well for semantic segmentation task.
In this work we focus on efficient design of the segmentation decoder and
assume that an efficient encoder is already designed to provide shared features
for a multi-task learning system. We design a novel efficient non-bottleneck
layer and a family of decoders which fit into a small run-time budget using
VGG10 as efficient encoder. We demonstrate in our dataset that experimentation
with various design choices led to an improvement of 10% from a baseline
performance.
",0,0,1
"Poly-YOLO: higher speed more precise detection and instance
  segmentation for YOLOv3","  We present a new version of YOLO with better performance and extended with
instance segmentation called Poly-YOLO. Poly-YOLO builds on the original ideas
of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels
and inefficient distribution of anchors. Poly-YOLO reduces the issues by
aggregating features from a light SE-Darknet-53 backbone with a hypercolumn
technique using stairstep upsampling and produces a single scale output with
high resolution. In comparison with YOLOv3 Poly-YOLO has only 60% of its
trainable parameters but improves mAP by a relative 40%. We also present
Poly-YOLO lite with fewer parameters and a lower output resolution. It has the
same precision as YOLOv3 but it is three times smaller and twice as fast thus
suitable for embedded devices. Finally Poly-YOLO performs instance
segmentation using bounding polygons. The network is trained to detect
size-independent polygons defined on a polar grid. Vertices of each polygon are
being predicted with their confidence and therefore Poly-YOLO produces
polygons with a varying number of vertices.
",0,0,1
Detecting Byzantine Attacks Without Clean Reference,"  We consider an amplify-and-forward relay network composed of a source two
relays and a destination. In this network the two relays are untrusted in the
sense that they may perform Byzantine attacks by forwarding altered symbols to
the destination. Note that every symbol received by the destination may be
altered and hence no clean reference observation is available to the
destination. For this network we identify a large family of Byzantine attacks
that can be detected in the physical layer. We further investigate how the
channel conditions impact the detection against this family of attacks. In
particular we prove that all Byzantine attacks in this family can be detected
with asymptotically small miss detection and false alarm probabilities by using
a sufficiently large number of channel observations emphif and only if the
network satisfies a non-manipulability condition. No pre-shared secret or
secret transmission is needed for the detection of these attacks demonstrating
the value of this physical-layer security technique for counteracting Byzantine
attacks.
",1,0,0
"Affect2MM: Affective Analysis of Multimedia Content Using Emotion
  Causality","  We present Affect2MM a learning method for time-series emotion prediction
for multimedia content. Our goal is to automatically capture the varying
emotions depicted by characters in real-life human-centric situations and
behaviors. We use the ideas from emotion causation theories to computationally
model and determine the emotional state evoked in clips of movies. Affect2MM
explicitly models the temporal causality using attention-based methods and
Granger causality. We use a variety of components like facial features of
actors involved scene understanding visual aesthetics action/situation
description and movie script to obtain an affective-rich representation to
understand and perceive the scene. We use an LSTM-based learning model for
emotion perception. To evaluate our method we analyze and compare our
performance on three datasets SENDv1 MovieGraphs and the LIRIS-ACCEDE
dataset and observe an average of 10-15% increase in the performance over SOTA
methods for all three datasets.
",0,0,1
Hierarchical and Efficient Learning for Person Re-Identification,"  Recent works in the person re-identification task mainly focus on the model
accuracy while ignore factors related to the efficiency e.g. model size and
latency which are critical for practical application. In this paper we
propose a novel Hierarchical and Efficient Network (HENet) that learns
hierarchical global partial and recovery features ensemble under the
supervision of multiple loss combinations. To further improve the robustness
against the irregular occlusion we propose a new dataset augmentation
approach dubbed Random Polygon Erasing (RPE) to random erase irregular area
of the input image for imitating the body part missing. We also propose an
Efficiency Score (ES) metric to evaluate the model efficiency. Extensive
experiments on Market1501 DukeMTMC-ReID and CUHK03 datasets shows the
efficiency and superiority of our approach compared with epoch-making methods.
",0,0,1
"Unsupervised Understanding of Location and Illumination Changes in
  Egocentric Videos","  Wearable cameras stand out as one of the most promising devices for the
upcoming years and as a consequence the demand of computer algorithms to
automatically understand the videos recorded with them is increasing quickly.
An automatic understanding of these videos is not an easy task and its mobile
nature implies important challenges to be faced such as the changing light
conditions and the unrestricted locations recorded. This paper proposes an
unsupervised strategy based on global features and manifold learning to endow
wearable cameras with contextual information regarding the light conditions and
the location captured. Results show that non-linear manifold methods can
capture contextual patterns from global features without compromising large
computational resources. The proposed strategy is used as an application case
as a switching mechanism to improve the hand-detection problem in egocentric
videos.
",0,0,1
An Image Forensic Technique Based on JPEG Ghosts,"  The unprecedented growth in the easy availability of photo-editing tools has
endangered the power of digital images.An image was supposed to be worth more
than a thousand wordsbut now this can be said only if it can be authenticated
orthe integrity of the image can be proved to be intact. In thispaper we
propose a digital image forensic technique for JPEG images. It can detect any
forgery in the image if the forged portion called a ghost image is having a
compression quality different from that of the cover image. It is based on
resaving the JPEG image at different JPEG qualities and the detection of the
forged portion is maximum when it is saved at the same JPEG quality as the
cover image. Also we can precisely predictthe JPEG quality of the cover image
by analyzing the similarity using Structural Similarity Index Measure (SSIM) or
the energyof the images. The first maxima in SSIM or the first minima inenergy
correspond to the cover image JPEG quality. We created adataset for varying
JPEG compression qualities of the ghost and the cover images and validated the
scalability of the experimental results.We also experimented with varied
attack scenarios e.g. high-quality ghost image embedded in low quality of
cover imagelow-quality ghost image embedded in high-quality of cover imageand
ghost image and cover image both at the same quality.The proposed method is
able to localize the tampered portions accurately even for forgeries as small
as 10x10 sized pixel blocks.Our technique is also robust against other attack
scenarios like copy-move forgery inserting text into image rescaling
(zoom-out/zoom-in) ghost image and then pasting on cover image.
",0,0,1
"Does a Hybrid Neural Network based Feature Selection Model Improve Text
  Classification?","  Text classification is a fundamental problem in the field of natural language
processing. Text classification mainly focuses on giving more importance to all
the relevant features that help classify the textual data. Apart from these
the text can have redundant or highly correlated features. These features
increase the complexity of the classification algorithm. Thus many
dimensionality reduction methods were proposed with the traditional machine
learning classifiers. The use of dimensionality reduction methods with machine
learning classifiers has achieved good results. In this paper we propose a
hybrid feature selection method for obtaining relevant features by combining
various filter-based feature selection methods and fastText classifier. We then
present three ways of implementing a feature selection and neural network
pipeline. We observed a reduction in training time when feature selection
methods are used along with neural networks. We also observed a slight increase
in accuracy on some datasets.
",0,1,0
Long-Span Summarization via Local Attention and Content Selection,"  Transformer-based models have achieved state-of-the-art results in a wide
range of natural language processing (NLP) tasks including document
summarization. Typically these systems are trained by fine-tuning a large
pre-trained model to the target task. One issue with these transformer-based
models is that they do not scale well in terms of memory and compute
requirements as the input length grows. Thus for long document summarization
it can be challenging to train or fine-tune these models. In this work we
exploit large pre-trained transformer-based models and address long-span
dependencies in abstractive summarization using two methods: local
self-attention; and explicit content selection. These approaches are compared
on a range of network configurations. Experiments are carried out on standard
long-span summarization tasks including Spotify Podcast arXiv and PubMed
datasets. We demonstrate that by combining these methods we can achieve
state-of-the-art results on all three tasks in the ROUGE scores. Moreover
without a large-scale GPU card our approach can achieve comparable or better
results than existing approaches.
",0,1,0
"Language Modeling Lexical Translation Reordering: The Training Process
  of NMT through the Lens of Classical SMT","  Differently from the traditional statistical MT that decomposes the
translation task into distinct separately learned components neural machine
translation uses a single neural network to model the entire translation
process. Despite neural machine translation being de-facto standard it is
still not clear how NMT models acquire different competences over the course of
training and how this mirrors the different models in traditional SMT. In this
work we look at the competences related to three core SMT components and find
that during training NMT first focuses on learning target-side language
modeling then improves translation quality approaching word-by-word
translation and finally learns more complicated reordering patterns. We show
that this behavior holds for several models and language pairs. Additionally
we explain how such an understanding of the training process can be useful in
practice and as an example show how it can be used to improve vanilla
non-autoregressive neural machine translation by guiding teacher model
selection.
",0,1,0
Sexism in the Judiciary,"  We analyze 6.7 million case law documents to determine the presence of gender
bias within our judicial system. We find that current bias detectino methods in
NLP are insufficient to determine gender bias in our case law database and
propose an alternative approach. We show that existing algorithms' inconsistent
results are consequences of prior research's definition of biases themselves.
Bias detection algorithms rely on groups of words to represent bias (e.g.
'salary' 'job' and 'boss' to represent employment as a potentially biased
theme against women in text). However the methods to build these groups of
words have several weaknesses primarily that the word lists are based on the
researchers' own intuitions. We suggest two new methods of automating the
creation of word lists to represent biases. We find that our methods outperform
current NLP bias detection methods. Our research improves the capabilities of
NLP technology to detect bias and highlights gender biases present in
influential case law. In order test our NLP bias detection method's
performance we regress our results of bias in case law against U.S census data
of women's participation in the workforce in the last 100 years.
",0,1,0
"OFDM demodulation using virtual time reversal processing in underwater
  acoustic communication","  The extremely long underwater channel delay spread causes severe inter-symbol
interference (ISI) for underwater acoustic communications. Passive time
reversal processing (PTRP) can effectively reduce the channel time dispersion
in a simple way via convolving the received packet with a time reversed probe
signal. However the probe signal itself may introduce extra noise and
interference (self-correlation of the probe signal). In this paper we propose
a virtual time reversal processing (VTRP) for single input single output (SISO)
Orthogonal Frequency Division Multiplexing (OFDM) systems. It convolves the
received packet with the reversed estimated channel instead of the probe
signal to reduce the interference. Two sparse channel estimation methods
matching pursuit (MP) and basis pursuit de-noising (BPDN) are adopted to
estimate the channel impulse response (CIR). We compare the performance of VTRP
with the PTRP and without any time reversal processing through MATLAB
simulations and the pool experiments. The results reveal that VTRP has
outstanding performance over time-invariant channels.
",1,0,0
Restricting the Flow: Information Bottlenecks for Attribution,"  Attribution methods provide insights into the decision-making of machine
learning models like artificial neural networks. For a given input sample they
assign a relevance score to each individual input variable such as the pixels
of an image. In this work we adapt the information bottleneck concept for
attribution. By adding noise to intermediate feature maps we restrict the flow
of information and can quantify (in bits) how much information image regions
provide. We compare our method against ten baselines using three different
metrics on VGG-16 and ResNet-50 and find that our methods outperform all
baselines in five out of six settings. The method's information-theoretic
foundation provides an absolute frame of reference for attribution values
(bits) and a guarantee that regions scored close to zero are not necessary for
the network's decision. For reviews: https://openreview.net/forum?id=S1xWh1rYwB
For code: https://github.com/BioroboticsLab/IBA
",0,0,1
"Cycle-free CycleGAN using Invertible Generator for Unsupervised Low-Dose
  CT Denoising","  Recently CycleGAN was shown to provide high-performance ultra-fast
denoising for low-dose X-ray computed tomography (CT) without the need for a
paired training dataset. Although this was possible thanks to cycle
consistency CycleGAN requires two generators and two discriminators to enforce
cycle consistency demanding significant GPU resources and technical skills for
training. A recent proposal of tunable CycleGAN with Adaptive Instance
Normalization (AdaIN) alleviates the problem in part by using a single
generator. However two discriminators and an additional AdaIN code generator
are still required for training. To solve this problem here we present a novel
cycle-free Cycle-GAN architecture which consists of a single generator and a
discriminator but still guarantees cycle consistency. The main innovation comes
from the observation that the use of an invertible generator automatically
fulfills the cycle consistency condition and eliminates the additional
discriminator in the CycleGAN formulation. To make the invertible generator
more effective our network is implemented in the wavelet residual domain.
Extensive experiments using various levels of low-dose CT images confirm that
our method can significantly improve denoising performance using only 10% of
learnable parameters and faster training time compared to the conventional
CycleGAN.
",0,0,1
Sign Language Gibberish for syntactic parsing evaluation,"  Sign Language (SL) automatic processing slowly progresses bottom-up. The
field has seen proposition to handle the video signal to recognize and
synthesize sublexical and lexical units. It starts to see the development of
supra-lexical processing. But the recognition at this level lacks data. The
syntax of SL appears very specific as it uses massively the multiplicity of
articulators and its access to the spatial dimensions. Therefore new parsing
techniques are developed. However these need to be evaluated. The shortage on
real data restrains the corpus-based models to small sizes. We propose here a
solution to produce data-sets for the evaluation of parsers on the specific
properties of SL. The article first describes the general model used to
generates dependency grammars and the phrase generation from these lasts. It
then discusses the limits of approach. The solution shows to be of particular
interest to evaluate the scalability of the techniques on big models.
",0,1,0
ShaResNet: reducing residual network parameter number by sharing weights,"  Deep Residual Networks have reached the state of the art in many image
processing tasks such image classification. However the cost for a gain in
accuracy in terms of depth and memory is prohibitive as it requires a higher
number of residual blocks up to double the initial value. To tackle this
problem we propose in this paper a way to reduce the redundant information of
the networks. We share the weights of convolutional layers between residual
blocks operating at the same spatial scale. The signal flows multiple times in
the same convolutional layer. The resulting architecture called ShaResNet
contains block specific layers and shared layers. These ShaResNet are trained
exactly in the same fashion as the commonly used residual networks. We show on
the one hand that they are almost as efficient as their sequential
counterparts while involving less parameters and on the other hand that they
are more efficient than a residual network with the same number of parameters.
For example a 152-layer-deep residual network can be reduced to 106
convolutional layers i.e. a parameter gain of 39% while loosing less than
0.2% accuracy on ImageNet.
",0,0,1
"Prototypical Region Proposal Networks for Few-Shot Localization and
  Classification","  Recently proposed few-shot image classification methods have generally
focused on use cases where the objects to be classified are the central subject
of images. Despite success on benchmark vision datasets aligned with this use
case these methods typically fail on use cases involving densely-annotated
busy images: images common in the wild where objects of relevance are not the
central subject instead appearing potentially occluded small or among other
incidental objects belonging to other classes of potential interest. To
localize relevant objects we employ a prototype-based few-shot segmentation
model which compares the encoded features of unlabeled query images with
support class centroids to produce region proposals indicating the presence and
location of support set classes in a query image. These region proposals are
then used as additional conditioning input to few-shot image classifiers. We
develop a framework to unify the two stages (segmentation and classification)
into an end-to-end classification model -- PRoPnet -- and empirically
demonstrate that our methods improve accuracy on image datasets with natural
scenes containing multiple object classes.
",0,0,1
Enhancing Traffic Scene Predictions with Generative Adversarial Networks,"  We present a new two-stage pipeline for predicting frames of traffic scenes
where relevant objects can still reliably be detected. Using a recent video
prediction network we first generate a sequence of future frames based on past
frames. A second network then enhances these frames in order to make them
appear more realistic. This ensures the quality of the predicted frames to be
sufficient to enable accurate detection of objects which is especially
important for autonomously driving cars. To verify this two-stage approach we
conducted experiments on the Cityscapes dataset. For enhancing we trained two
image-to-image translation methods based on generative adversarial networks
one for blind motion deblurring and one for image super-resolution. All
resulting predictions were quantitatively evaluated using both traditional
metrics and a state-of-the-art object detection network showing that the
enhanced frames appear qualitatively improved. While the traditional image
comparison metrics i.e. MSE PSNR and SSIM failed to confirm this visual
impression the object detection evaluation resembles it well. The best
performing prediction-enhancement pipeline is able to increase the average
precision values for detecting cars by about 9% for each prediction step
compared to the non-enhanced predictions.
",0,0,1
"Multicell Coordinated Beamforming with Rate Outage Constraint--Part I:
  Complexity Analysis","  This paper studies the coordinated beamforming (CoBF) design in the
multiple-input single-output interference channel assuming only channel
distribution information given a priori at the transmitters. The CoBF design is
formulated as an optimization problem that maximizes a predefined system
utility e.g. the weighted sum rate or the weighted max-min-fairness (MMF)
rate subject to constraints on the individual probability of rate outage and
power budget. While the problem is non-convex and appears difficult to handle
due to the intricate outage probability constraints so far it is still unknown
if this outage constrained problem is computationally tractable. To answer
this we conduct computational complexity analysis of the outage constrained
CoBF problem. Specifically we show that the outage constrained CoBF problem
with the weighted sum rate utility is intrinsically difficult i.e. NP-hard.
Moreover the outage constrained CoBF problem with the weighted MMF rate
utility is also NP-hard except the case when all the transmitters are equipped
with single antenna. The presented analysis results confirm that efficient
approximation methods are indispensable to the outage constrained CoBF problem.
",1,0,0
"The Impact of Preprocessing on Arabic-English Statistical and Neural
  Machine Translation","  Neural networks have become the state-of-the-art approach for machine
translation (MT) in many languages. While linguistically-motivated tokenization
techniques were shown to have significant effects on the performance of
statistical MT it remains unclear if those techniques are well suited for
neural MT. In this paper we systematically compare neural and statistical MT
models for Arabic-English translation on data preprecossed by various prominent
tokenization schemes. Furthermore we consider a range of data and vocabulary
sizes and compare their effect on both approaches. Our empirical results show
that the best choice of tokenization scheme is largely based on the type of
model and the size of data. We also show that we can gain significant
improvements using a system selection that combines the output from neural and
statistical MT.
",0,1,0
CIDEr: Consensus-based Image Description Evaluation,"  Automatically describing an image with a sentence is a long-standing
challenge in computer vision and natural language processing. Due to recent
progress in object detection attribute classification action recognition
etc. there is renewed interest in this area. However evaluating the quality
of descriptions has proven to be challenging. We propose a novel paradigm for
evaluating image descriptions that uses human consensus. This paradigm consists
of three main parts: a new triplet-based method of collecting human annotations
to measure consensus a new automated metric (CIDEr) that captures consensus
and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences
describing each image. Our simple metric captures human judgment of consensus
better than existing metrics across sentences generated by various sources. We
also evaluate five state-of-the-art image description approaches using this new
protocol and provide a benchmark for future comparisons. A version of CIDEr
named CIDEr-D is available as a part of MS COCO evaluation server to enable
systematic evaluation and benchmarking.
",0,1,0
"EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware
  Multi-Task NLP Inference","  Transformer-based language models such as BERT provide significant accuracy
improvement for a multitude of natural language processing (NLP) tasks.
However their hefty computational and memory demands make them challenging to
deploy to resource-constrained edge platforms with strict latency requirements.
We present EdgeBERT an in-depth algorithm-hardware co-design for latency-aware
energy optimization for multi-task NLP. EdgeBERT employs entropy-based early
exit predication in order to perform dynamic voltage-frequency scaling (DVFS)
at a sentence granularity for minimal energy consumption while adhering to a
prescribed target latency. Computation and memory footprint overheads are
further alleviated by employing a calibrated combination of adaptive attention
span selective network pruning and floating-point quantization. Furthermore
in order to maximize the synergistic benefits of these algorithms in always-on
and intermediate edge computing settings we specialize a 12nm scalable
hardware accelerator system integrating a fast-switching low-dropout voltage
regulator (LDO) an all-digital phase-locked loop (ADPLL) as well as
high-density embedded non-volatile memories (eNVMs) wherein the sparse
floating-point bit encodings of the shared multi-task parameters are carefully
stored. Altogether latency-aware multi-task NLP inference acceleration on the
EdgeBERT hardware system generates up to 7x 2.5x and 53x lower energy
compared to the conventional inference without early stopping the
latency-unbounded early exit approach and CUDA adaptations on an Nvidia Jetson
Tegra X2 mobile GPU respectively.
",0,1,0
Sherlock: Scalable Fact Learning in Images,"  We study scalable and uniform understanding of facts in images. Existing
visual recognition systems are typically modeled differently for each fact type
such as objects actions and interactions. We propose a setting where all
these facts can be modeled simultaneously with a capacity to understand
unbounded number of facts in a structured way. The training data comes as
structured facts in images including (1) objects (e.g. $<$boy$>$) (2)
attributes (e.g. $<$boy tall$>$) (3) actions (e.g. $<$boy playing$>$) and
(4) interactions (e.g. $<$boy riding a horse $>$). Each fact has a semantic
language view (e.g. $<$ boy playing$>$) and a visual view (an image with this
fact). We show that learning visual facts in a structured way enables not only
a uniform but also generalizable visual understanding. We propose and
investigate recent and strong approaches from the multiview learning literature
and also introduce two learning representation models as potential baselines.
We applied the investigated methods on several datasets that we augmented with
structured facts and a large scale dataset of more than 202000 facts and
814000 images. Our experiments show the advantage of relating facts by the
structure by the proposed models compared to the designed baselines on
bidirectional fact retrieval.
",0,0,1
"Hyper RPCA: Joint Maximum Correntropy Criterion and Laplacian Scale
  Mixture Modeling On-the-Fly for Moving Object Detection","  Moving object detection is critical for automated video analysis in many
vision-related tasks such as surveillance tracking video compression coding
etc. Robust Principal Component Analysis (RPCA) as one of the most popular
moving object modelling methods aims to separate the temporally varying (i.e.
moving) foreground objects from the static background in video assuming the
background frames to be low-rank while the foreground to be spatially sparse.
Classic RPCA imposes sparsity of the foreground component using l1-norm and
minimizes the modeling error via 2-norm. We show that such assumptions can be
too restrictive in practice which limits the effectiveness of the classic
RPCA especially when processing videos with dynamic background camera jitter
camouflaged moving object etc. In this paper we propose a novel RPCA-based
model called Hyper RPCA to detect moving objects on the fly. Different from
classic RPCA the proposed Hyper RPCA jointly applies the maximum correntropy
criterion (MCC) for the modeling error and Laplacian scale mixture (LSM) model
for foreground objects. Extensive experiments have been conducted and the
results demonstrate that the proposed Hyper RPCA has competitive performance
for foreground detection to the state-of-the-art algorithms on several
well-known benchmark datasets.
",0,0,1
Learning with Privileged Information for Multi-Label Classification,"  In this paper we propose a novel approach for learning multi-label
classifiers with the help of privileged information. Specifically we use
similarity constraints to capture the relationship between available
information and privileged information and use ranking constraints to capture
the dependencies among multiple labels. By integrating similarity constraints
and ranking constraints into the learning process of classifiers the
privileged information and the dependencies among multiple labels are exploited
to construct better classifiers during training. A maximum margin classifier is
adopted and an efficient learning algorithm of the proposed method is also
developed. We evaluate the proposed method on two applications: multiple object
recognition from images with the help of implicit information about object
importance conveyed by the list of manually annotated image tags; and multiple
facial action unit detection from low-resolution images augmented by
high-resolution images. Experimental results demonstrate that the proposed
method can effectively take full advantage of privileged information and
dependencies among multiple labels for better object recognition and better
facial action unit detection.
",0,0,1
"Vulgaris: Analysis of a Corpus for Middle-Age Varieties of Italian
  Language","  Italian is a Romance language that has its roots in Vulgar Latin. The birth
of the modern Italian started in Tuscany around the 14th century and it is
mainly attributed to the works of Dante Alighieri Francesco Petrarca and
Giovanni Boccaccio who are among the most acclaimed authors of the medieval
age in Tuscany. However Italy has been characterized by a high variety of
dialects which are often loosely related to each other due to the past
fragmentation of the territory. Italian has absorbed influences from many of
these dialects as also from other languages due to dominion of portions of the
country by other nations such as Spain and France. In this work we present
Vulgaris a project aimed at studying a corpus of Italian textual resources
from authors of different regions ranging in a time period between 1200 and
1600. Each composition is associated to its author and authors are also
grouped in families i.e. sharing similar stylistic/chronological
characteristics. Hence the dataset is not only a valuable resource for
studying the diachronic evolution of Italian and the differences between its
dialects but it is also useful to investigate stylistic aspects between single
authors. We provide a detailed statistical analysis of the data and a
corpus-driven study in dialectology and diachronic varieties.
",0,1,0
State Information in Bayesian Games,"  Two-player zero-sum repeated games are well understood. Computing the value
of such a game is straightforward. Additionally if the payoffs are dependent
on a random state of the game known to one both or neither of the players
the resulting value of the game has been analyzed under the framework of
Bayesian games. This investigation considers the optimal performance in a game
when a helper is transmitting state information to one of the players.
  Encoding information for an adversarial setting (game) requires a different
result than rate-distortion theory provides. Game theory has accentuated the
importance of randomization (mixed strategy) which does not find a significant
role in most communication modems and source coding codecs. Higher rates of
communication used in the right way allow the message to include the
necessary random component useful in games.
",1,0,0
"Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid
  Approach","  We propose a novel coherence model for written asynchronous conversations
(e.g. forums emails) and show its applications in coherence assessment and
thread reconstruction tasks. We conduct our research in two steps. First we
propose improvements to the recently proposed neural entity grid model by
lexicalizing its entity transitions. Then we extend the model to asynchronous
conversations by incorporating the underlying conversational structure in the
entity grid representation and feature computation. Our model achieves state of
the art results on standard coherence assessment tasks in monologue and
conversations outperforming existing models. We also demonstrate its
effectiveness in reconstructing thread structures.
",0,1,0
Some New Research Trends in Wirelessly Powered Communications,"  The vision of seamlessly integrating information transfer (IT) and microwave
based power transfer (PT) in the same system has led to the emergence of a new
research area called wirelessly power communications (WPC). Extensive research
has been conducted on developing WPC theory and techniques building on the
extremely rich wireless communications litera- ture covering diversified topics
such as transmissions resource allocations medium access control and network
protocols and architectures. Despite these research efforts transforming WPC
from theory to practice still faces many unsolved prob- lems concerning issues
such as mobile complexity power transfer efficiency and safety. Furthermore
the fundamental limits of WPC remain largely unknown. Recent attempts to
address these open issues has resulted in the emergence of numerous new
research trends in the WPC area. A few promising trends are introduced in this
article. From the practical perspective the use of backscatter antennas can
support WPC for low-complexity passive devices the design of spiky waveforms
can improve the PT efficiency and analog spatial decoupling is proposed for
solving the PT-IT near-far problem in WPC. From the theoretic perspective the
fundamental limits of WPC can be quantified by leveraging recent results on
super-directivity and the limit can be improved by the deployment of
large-scale distributed antenna arrays. Specific research problems along these
trends are discussed whose solutions can lead to significant advancements in
WPC.
",1,0,0
Episodic Memory in Lifelong Language Learning,"  We introduce a lifelong language learning setup where a model needs to learn
from a stream of text examples without any dataset identifier. We propose an
episodic memory model that performs sparse experience replay and local
adaptation to mitigate catastrophic forgetting in this setup. Experiments on
text classification and question answering demonstrate the complementary
benefits of sparse experience replay and local adaptation to allow the model to
continuously learn from new datasets. We also show that the space complexity of
the episodic memory module can be reduced significantly (~50-90%) by randomly
choosing which examples to store in memory with a minimal decrease in
performance. We consider an episodic memory component as a crucial building
block of general linguistic intelligence and see our model as a first step in
that direction.
",0,1,0
"Image Denoising Using Tensor Product Complex Tight Framelets with
  Increasing Directionality","  Tensor product real-valued wavelets have been employed in many applications
such as image processing with impressive performance. Though edge singularities
are ubiquitous and play a fundamental role in two-dimensional problems tensor
product real-valued wavelets are known to be only sub-optimal since they can
only capture edges well along the coordinate axis directions. The dual tree
complex wavelet transform (DTCWT) proposed by Kingsbury [16] and further
developed by Selesnick et al. [24] is one of the most popular and successful
enhancements of the classical tensor product real-valued wavelets. The
two-dimensional DTCWT is obtained via tensor product and offers improved
directionality with 6 directions. In this paper we shall further enhance the
performance of DTCWT for the problem of image denoising. Using framelet-based
approach and the notion of discrete affine systems we shall propose a family
of tensor product complex tight framelets TPCTF_n for all integers n>2 with
increasing directionality where n refers to the number of filters in the
underlying one-dimensional complex tight framelet filter bank. For dimension
two such tensor product complex tight framelet TPCTF_n offers (n-1)(n-3)/2+4
directions when n is odd and (n-4)(n+2)/2+6 directions when n is even. In
particular TPCTF_4 which is different to DTCWT in both nature and design
provides an alternative to DTCWT. Indeed TPCTF_4 behaves quite similar to
DTCWT by offering 6 directions in dimension two employing the tensor product
structure and enjoying slightly less redundancy than DTCWT. When TPCTF_4 is
applied to image denoising its performance is comparable to DTCWT. Moreover
better results on image denoising can be obtained by using TPCTF_6. Moreover
TPCTF_n allows us to further improve DTCWT by using TPCTF_n as the first stage
filter bank in DTCWT.
",1,0,0
"Computational Analysis of Deformable Manifolds: from Geometric Modelling
  to Deep Learning","  Leo Tolstoy opened his monumental novel Anna Karenina with the now famous
words: Happy families are all alike; every unhappy family is unhappy in its own
way A similar notion also applies to mathematical spaces: Every flat space is
alike; every unflat space is unflat in its own way. However rather than being
a source of unhappiness we will show that the diversity of non-flat spaces
provides a rich area of study. The genesis of the so-called big data era and
the proliferation of social and scientific databases of increasing size has led
to a need for algorithms that can efficiently process analyze and even
generate high dimensional data. However the curse of dimensionality leads to
the fact that many classical approaches do not scale well with respect to the
size of these problems. One technique to avoid some of these ill-effects is to
exploit the geometric structure of coherent data. In this thesis we will
explore geometric methods for shape processing and data analysis. More
specifically we will study techniques for representing manifolds and signals
supported on them through a variety of mathematical tools including but not
limited to computational differential geometry variational PDE modeling and
deep learning. First we will explore non-isometric shape matching through
variational modeling. Next we will use ideas from parallel transport on
manifolds to generalize convolution and convolutional neural networks to
deformable manifolds. Finally we conclude by proposing a novel auto-regressive
model for capturing the intrinsic geometry and topology of data. Throughout
this work we will use the idea of computing correspondences as a though-line
to both motivate our work and analyze our results.
",0,0,1
C3AE: Exploring the Limits of Compact Model for Age Estimation,"  Age estimation is a classic learning problem in computer vision. Many larger
and deeper CNNs have been proposed with promising performance such as AlexNet
VggNet GoogLeNet and ResNet. However these models are not practical for the
embedded/mobile devices. Recently MobileNets and ShuffleNets have been
proposed to reduce the number of parameters yielding lightweight models.
However their representation has been weakened because of the adoption of
depth-wise separable convolution. In this work we investigate the limits of
compact model for small-scale image and propose an extremely Compact yet
efficient Cascade Context-based Age Estimation model(C3AE). This model
possesses only 1/9 and 1/2000 parameters compared with MobileNets/ShuffleNets
and VggNet while achieves competitive performance. In particular we re-define
age estimation problem by two-points representation which is implemented by a
cascade model. Moreover to fully utilize the facial context information
multi-branch CNN network is proposed to aggregate multi-scale context.
Experiments are carried out on three age estimation datasets. The
state-of-the-art performance on compact model has been achieved with a
relatively large margin.
",0,0,1
"Heterogeneity Loss to Handle Intersubject and Intrasubject Variability
  in Cancer","  Developing nations lack adequate number of hospitals with modern equipment
and skilled doctors. Hence a significant proportion of these nations'
population particularly in rural areas is not able to avail specialized and
timely healthcare facilities. In recent years deep learning (DL) models a
class of artificial intelligence (AI) methods have shown impressive results in
medical domain. These AI methods can provide immense support to developing
nations as affordable healthcare solutions. This work is focused on one such
application of blood cancer diagnosis. However there are some challenges to DL
models in cancer research because of the unavailability of a large data for
adequate training and the difficulty of capturing heterogeneity in data at
different levels ranging from acquisition characteristics session to
subject-level (within subjects and across subjects). These challenges render DL
models prone to overfitting and hence models lack generalization on
prospective subjects' data. In this work we address these problems in the
application of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep
learning. We propose heterogeneity loss that captures subject-level
heterogeneity thereby forcing the neural network to learn subject-independent
features. We also propose an unorthodox ensemble strategy that helps us in
providing improved classification over models trained on 7-folds giving a
weighted-$F_1$ score of 95.26% on unseen (test) subjects' data that are so
far the best results on the C-NMC 2019 dataset for B-ALL classification.
",0,0,1
"Automatic 2D-3D Registration without Contrast Agent during Neurovascular
  Interventions","  Fusing live fluoroscopy images with a 3D rotational reconstruction of the
vasculature allows to navigate endovascular devices in minimally invasive
neuro-vascular treatment while reducing the usage of harmful iodine contrast
medium. The alignment of the fluoroscopy images and the 3D reconstruction is
initialized using the sensor information of the X-ray C-arm geometry. Patient
motion is then corrected by an image-based registration algorithm based on a
gradient difference similarity measure using digital reconstructed radiographs
of the 3D reconstruction. This algorithm does not require the vessels in the
fluoroscopy image to be filled with iodine contrast agent but rather relies on
gradients in the image (bone structures sinuses) as landmark features. This
paper investigates the accuracy robustness and computation time aspects of the
image-based registration algorithm. Using phantom experiments 97% of the
registration attempts passed the success criterion of a residual registration
error of less than 1 mm translation and 3deg rotation. The paper establishes
a new method for validation of 2D-3D registration without requiring changes to
the clinical workflow such as attaching fiducial markers. As a consequence
this method can be retrospectively applied to pre-existing clinical data. For
clinical data experiments 87% of the registration attempts passed the
criterion of a residual translational error of < 1 mm and 84% possessed a
rotational error of < 3deg.
",0,0,1
"Face Alignment Using K-Cluster Regression Forests With Weighted
  Splitting","  In this work we present a face alignment pipeline based on two novel methods:
weighted splitting for K-cluster Regression Forests and 3D Affine Pose
Regression for face shape initialization. Our face alignment method is based on
the Local Binary Feature framework where instead of standard regression
forests and pixel difference features used in the original method we use our
K-cluster Regression Forests with Weighted Splitting (KRFWS) and Pyramid HOG
features. We also use KRFWS to perform Affine Pose Regression (APR) and
3D-Affine Pose Regression (3D-APR) which intend to improve the face shape
initialization. APR applies a rigid 2D transform to the initial face shape that
compensates for inaccuracy in the initial face location size and in-plane
rotation. 3D-APR estimates the parameters of a 3D transform that additionally
compensates for out-of-plane rotation. The resulting pipeline consisting of
APR and 3D-APR followed by face alignment shows an improvement of 20% over
standard LBF on the challenging IBUG dataset and state-of-theart accuracy on
the entire 300-W dataset.
",0,0,1
Synthesizing Robust Adversarial Examples,"  Standard methods for generating adversarial examples for neural networks do
not consistently fool neural network classifiers in the physical world due to a
combination of viewpoint shifts camera noise and other natural
transformations limiting their relevance to real-world systems. We demonstrate
the existence of robust 3D adversarial objects and we present the first
algorithm for synthesizing examples that are adversarial over a chosen
distribution of transformations. We synthesize two-dimensional adversarial
images that are robust to noise distortion and affine transformation. We
apply our algorithm to complex three-dimensional objects using 3D-printing to
manufacture the first physical adversarial objects. Our results demonstrate the
existence of 3D adversarial objects in the physical world.
",0,0,1
"Unsupervised Learning for Target Tracking and Background Subtraction in
  Satellite Imagery","  This paper describes an unsupervised machine learning methodology capable of
target tracking and background suppression via a novel dual-model approach.
``Jekyll`` produces a video bit-mask describing an estimate of the locations of
moving objects and ``Hyde`` outputs a pseudo-background frame to subtract from
the original input image sequence. These models were trained with a
custom-modified version of Cross Entropy Loss.
  Simulated data were used to compare the performance of Jekyll and Hyde
against a more traditional supervised Machine Learning approach. The results
from these comparisons show that the unsupervised methods developed are
competitive in output quality with supervised techniques without the
associated cost of acquiring labeled training data.
",0,0,1
"A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human
  Action Recognition","  Current researches of action recognition mainly focus on single-view and
multi-view recognition which can hardly satisfies the requirements of
human-robot interaction (HRI) applications to recognize actions from arbitrary
views. The lack of datasets also sets up barriers. To provide data for
arbitrary-view action recognition we newly collect a large-scale RGB-D action
dataset for arbitrary-view action analysis including RGB videos depth and
skeleton sequences. The dataset includes action samples captured in 8 fixed
viewpoints and varying-view sequences which covers the entire 360 degree view
angles. In total 118 persons are invited to act 40 action categories and
25600 video samples are collected. Our dataset involves more participants
more viewpoints and a large number of samples. More importantly it is the
first dataset containing the entire 360 degree varying-view sequences. The
dataset provides sufficient data for multi-view cross-view and arbitrary-view
action analysis. Besides we propose a View-guided Skeleton CNN (VS-CNN) to
tackle the problem of arbitrary-view action recognition. Experiment results
show that the VS-CNN achieves superior performance.
",0,0,1
Automatic Weight Estimation of Harvested Fish from Images,"  Approximately 2500 weights and corresponding images of harvested Lates
calcarifer (Asian seabass or barramundi) were collected at three different
locations in Queensland Australia. Two instances of the LinkNet-34
segmentation Convolutional Neural Network (CNN) were trained. The first one was
trained on 200 manually segmented fish masks with excluded fins and tails. The
second was trained on 100 whole-fish masks. The two CNNs were applied to the
rest of the images and yielded automatically segmented masks. The one-factor
and two-factor simple mathematical weight-from-area models were fitted on 1072
area-weight pairs from the first two locations where area values were
extracted from the automatically segmented masks. When applied to 1400 test
images (from the third location) the one-factor whole-fish mask model achieved
the best mean absolute percentage error (MAPE) MAPE=4.36%. Direct
weight-from-image regression CNNs were also trained where the no-fins based
CNN performed best on the test images with MAPE=4.28%.
",0,0,1
Physical Attribute Prediction Using Deep Residual Neural Networks,"  Images taken from the Internet have been used alongside Deep Learning for
many different tasks such as: smile detection ethnicity hair style hair
colour gender and age prediction. After witnessing these usages we were
wondering what other attributes can be predicted from facial images available
on the Internet. In this paper we tackle the prediction of physical attributes
from face images using Convolutional Neural Networks trained on our dataset
named FIRW. We crawled around 61 000 images from the web then use face
detection to crop faces from these real world images. We choose ResNet-50 as
our base network architecture. This network was pretrained for the task of face
recognition by using the VGG-Face dataset and we finetune it by using our own
dataset to predict physical attributes. Separate networks are trained for the
prediction of body type ethnicity gender height and weight; our models
achieve the following accuracies for theses tasks respectively: 84.58%
87.34% 97.97% 70.51% 63.99%. To validate our choice of ResNet-50 as the base
architecture we also tackle the famous CelebA dataset. Our models achieve an
averagy accuracy of 91.19% on CelebA which is comparable to state-of-the-art
approaches.
",0,0,1
"Sum Rate Maximization for Reconfigurable Intelligent Surface Assisted
  Device-to-Device Communications","  In this letter we propose to employ reconfigurable intelligent surfaces
(RISs) for enhancing the D2D underlaying system performance. We study the joint
power control receive beamforming and passive beamforming for RIS assisted
D2D underlaying cellular communication systems which is formulated as a sum
rate maximization problem. To address this issue we develop a block coordinate
descent method where uplink power receive beamformer and refection phase
shifts are alternatively optimized. Then we provide the closed-form solutions
for both uplink power and receive beamformer. We further propose a quadratic
transform based semi-definite relaxation algorithm to optimize the RIS phase
shifts where the original passive beamforming problem is translated into a
separable quadratically constrained quadratic problem. Numerical results
demonstrate that the proposed RIS assisted design significantly improves the
sum-rate performance.
",1,0,0
"PaMIR: Parametric Model-Conditioned Implicit Representation for
  Image-based Human Reconstruction","  Modeling 3D humans accurately and robustly from a single image is very
challenging and the key for such an ill-posed problem is the 3D representation
of the human models. To overcome the limitations of regular 3D representations
we propose Parametric Model-Conditioned Implicit Representation (PaMIR) which
combines the parametric body model with the free-form deep implicit function.
In our PaMIR-based reconstruction framework a novel deep neural network is
proposed to regularize the free-form deep implicit function using the semantic
features of the parametric model which improves the generalization ability
under the scenarios of challenging poses and various clothing topologies.
Moreover a novel depth-ambiguity-aware training loss is further integrated to
resolve depth ambiguities and enable successful surface detail reconstruction
with imperfect body reference. Finally we propose a body reference
optimization method to improve the parametric model estimation accuracy and to
enhance the consistency between the parametric model and the implicit function.
With the PaMIR representation our framework can be easily extended to
multi-image input scenarios without the need of multi-camera calibration and
pose synchronization. Experimental results demonstrate that our method achieves
state-of-the-art performance for image-based 3D human reconstruction in the
cases of challenging poses and clothing types.
",0,0,1
DeepSat - A Learning framework for Satellite Imagery,"  Satellite image classification is a challenging problem that lies at the
crossroads of remote sensing computer vision and machine learning. Due to the
high variability inherent in satellite data most of the current object
classification approaches are not suitable for handling satellite datasets. The
progress of satellite image analytics has also been inhibited by the lack of a
single labeled high-resolution dataset with multiple class labels. The
contributions of this paper are twofold - (1) first we present two new
satellite datasets called SAT-4 and SAT-6 and (2) then we propose a
classification framework that extracts features from an input image normalizes
them and feeds the normalized feature vectors to a Deep Belief Network for
classification. On the SAT-4 dataset our best network produces a
classification accuracy of 97.95% and outperforms three state-of-the-art object
recognition algorithms namely - Deep Belief Networks Convolutional Neural
Networks and Stacked Denoising Autoencoders by ~11%. On SAT-6 it produces a
classification accuracy of 93.9% and outperforms the other algorithms by ~15%.
Comparative studies with a Random Forest classifier show the advantage of an
unsupervised learning approach over traditional supervised learning techniques.
A statistical analysis based on Distribution Separability Criterion and
Intrinsic Dimensionality Estimation substantiates the effectiveness of our
approach in learning better representations for satellite imagery.
",0,0,1
Semantic Hilbert Space for Text Representation Learning,"  Capturing the meaning of sentences has long been a challenging task. Current
models tend to apply linear combinations of word features to conduct semantic
composition for bigger-granularity units e.g. phrases sentences and
documents. However the semantic linearity does not always hold in human
language. For instance the meaning of the phrase `ivory tower' can not be
deduced by linearly combining the meanings of `ivory' and `tower'. To address
this issue we propose a new framework that models different levels of semantic
units (e.g. sememe word sentence and semantic abstraction) on a single
textitSemantic Hilbert Space which naturally admits a non-linear semantic
composition by means of a complex-valued vector word representation. An
end-to-end neural network~footnotehttps://github.com/wabyking/qnn is
proposed to implement the framework in the text classification task and
evaluation results on six benchmarking text classification datasets demonstrate
the effectiveness robustness and self-explanation power of the proposed model.
Furthermore intuitive case studies are conducted to help end users to
understand how the framework works.
",0,1,0
A Reproducible Study on Remote Heart Rate Measurement,"  This paper studies the problem of reproducible research in remote
photoplethysmography (rPPG). Most of the work published in this domain is
assessed on privately-owned databases making it difficult to evaluate proposed
algorithms in a standard and principled manner. As a consequence we present a
new publicly available database containing a relatively large number of
subjects recorded under two different lighting conditions. Also three
state-of-the-art rPPG algorithms from the literature were selected implemented
and released as open source free software. After a thorough unbiased
experimental evaluation in various settings it is shown that none of the
selected algorithms is precise enough to be used in a real-world scenario.
",0,0,1
"Asynchronous Adaptation and Learning over Networks --- Part I: Modeling
  and Stability Analysis","  In this work and the supporting Parts II [2] and III [3] we provide a rather
detailed analysis of the stability and performance of asynchronous strategies
for solving distributed optimization and adaptation problems over networks. We
examine asynchronous networks that are subject to fairly general sources of
uncertainties such as changing topologies random link failures random data
arrival times and agents turning on and off randomly. Under this model agents
in the network may stop updating their solutions or may stop sending or
receiving information in a random manner and without coordination with other
agents. We establish in Part I conditions on the first and second-order moments
of the relevant parameter distributions to ensure mean-square stable behavior.
We derive in Part II expressions that reveal how the various parameters of the
asynchronous behavior influence network performance. We compare in Part III the
performance of asynchronous networks to the performance of both centralized
solutions and synchronous networks. One notable conclusion is that the
mean-square-error performance of asynchronous networks shows a degradation only
of the order of $O(nu)$ where $nu$ is a small step-size parameter while the
convergence rate remains largely unaltered. The results provide a solid
justification for the remarkable resilience of cooperative networks in the face
of random failures at multiple levels: agents links data arrivals and
topology.
",1,0,0
"Assessing the Influencing Factors on the Accuracy of Underage Facial Age
  Estimation","  Swift response to the detection of endangered minors is an ongoing concern
for law enforcement. Many child-focused investigations hinge on digital
evidence discovery and analysis. Automated age estimation techniques are needed
to aid in these investigations to expedite this evidence discovery process and
decrease investigator exposure to traumatic material. Automated techniques also
show promise in decreasing the overflowing backlog of evidence obtained from
increasing numbers of devices and online services. A lack of sufficient
training data combined with natural human variance has been long hindering
accurate automated age estimation -- especially for underage subjects. This
paper presented a comprehensive evaluation of the performance of two cloud age
estimation services (Amazon Web Service's Rekognition service and Microsoft
Azure's Face API) against a dataset of over 21800 underage subjects. The
objective of this work is to evaluate the influence that certain human
biometric factors facial expressions and image quality (i.e. blur noise
exposure and resolution) have on the outcome of automated age estimation
services. A thorough evaluation allows us to identify the most influential
factors to be overcome in future age estimation systems.
",0,0,1
Learning to Match Features with Seeded Graph Matching Network,"  Matching local features across images is a fundamental problem in computer
vision. Targeting towards high accuracy and efficiency we propose Seeded Graph
Matching Network a graph neural network with sparse structure to reduce
redundant connectivity and learn compact representation. The network consists
of 1) Seeding Module which initializes the matching by generating a small set
of reliable matches as seeds. 2) Seeded Graph Neural Network which utilizes
seed matches to pass messages within/across images and predicts assignment
costs. Three novel operations are proposed as basic elements for message
passing: 1) Attentional Pooling which aggregates keypoint features within the
image to seed matches. 2) Seed Filtering which enhances seed features and
exchanges messages across images. 3) Attentional Unpooling which propagates
seed features back to original keypoints. Experiments show that our method
reduces computational and memory complexity significantly compared with typical
attention-based networks while competitive or higher performance is achieved.
",0,0,1
"Many-to-One Throughput Capacity of IEEE 80211 Multi-hop Wireless
  Networks","  This paper investigates the many-to-one throughput capacity (and by symmetry
one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has
generally been assumed in prior studies that the many-to-one throughput
capacity is upper-bounded by the link capacity L. Throughput capacity L is not
achievable under 802.11. This paper introduces the notion of ""canonical
networks"" which is a class of regularly-structured networks whose capacities
can be analyzed more easily than unstructured networks. We show that the
throughput capacity of canonical networks under 802.11 has an analytical upper
bound of 3L/4 when the source nodes are two or more hops away from the sink;
and simulated throughputs of 0.690L (0.740L) when the source nodes are many
hops away. We conjecture that 3L/4 is also the upper bound for general
networks. When all links have equal length 2L/3 can be shown to be the upper
bound for general networks. Our simulations show that 802.11 networks with
random topologies operated with AODV routing can only achieve throughputs far
below the upper bounds. Fortunately by properly selecting routes near the
gateway (or by properly positioning the relay nodes leading to the gateway) to
fashion after the structure of canonical networks the throughput can be
improved significantly by more than 150%. Indeed in a dense network it is
worthwhile to deactivate some of the relay nodes near the sink judiciously.
",1,0,0
Segmentation-free Compositional $n$-gram Embedding,"  We propose a new type of representation learning method that models words
phrases and sentences seamlessly. Our method does not depend on word
segmentation and any human-annotated resources (e.g. word dictionaries) yet
it is very effective for noisy corpora written in unsegmented languages such as
Chinese and Japanese. The main idea of our method is to ignore word boundaries
completely (i.e. segmentation-free) and construct representations for all
character $n$-grams in a raw corpus with embeddings of compositional
sub-$n$-grams. Although the idea is simple our experiments on various
benchmarks and real-world datasets show the efficacy of our proposal.
",0,1,0
"The Effect of Network Width on Stochastic Gradient Descent and
  Generalization: an Empirical Study","  We investigate how the final parameters found by stochastic gradient descent
are influenced by over-parameterization. We generate families of models by
increasing the number of channels in a base network and then perform a large
hyper-parameter search to study how the test error depends on learning rate
batch size and network width. We find that the optimal SGD hyper-parameters
are determined by a ""normalized noise scale"" which is a function of the batch
size learning rate and initialization conditions. In the absence of batch
normalization the optimal normalized noise scale is directly proportional to
width. Wider networks with their higher optimal noise scale also achieve
higher test accuracy. These observations hold for MLPs ConvNets and ResNets
and for two different parameterization schemes (""Standard"" and ""NTK""). We
observe a similar trend with batch normalization for ResNets. Surprisingly
since the largest stable learning rate is bounded the largest batch size
consistent with the optimal normalized noise scale decreases as the width
increases.
",0,0,1
"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact
  Checking of Claims","  We contribute the largest publicly available dataset of naturally occurring
factual claims for the purpose of automatic claim verification. It is collected
from 26 fact checking websites in English paired with textual sources and rich
metadata and labelled for veracity by human expert journalists. We present an
in-depth analysis of the dataset highlighting characteristics and challenges.
Further we present results for automatic veracity prediction both with
established baselines and with a novel method for joint ranking of evidence
pages and predicting veracity that outperforms all baselines. Significant
performance increases are achieved by encoding evidence and by modelling
metadata. Our best-performing model achieves a Macro F1 of 49.2% showing that
this is a challenging testbed for claim veracity prediction.
",0,1,0
On cyclic DNA codes over $mathbbF_2+umathbbF_2+u^2mathbbF_2$,"  In the present paper we study the structure of cyclic DNA codes of even
lenght over the ring $mathbbF_2+umathbbF_2+u^2mathbbF_2$ where
$u^3=0$. We investigate two presentations of cyclic codes of even lenght over
$mathbbF_2+umathbbF_2+u^2mathbbF_2$ satisfying the reverse constraint
and reverse-complement constraint.
",1,0,0
Visual Saliency Based on Scale-Space Analysis in the Frequency Domain,"  We address the issue of visual saliency from three perspectives. First we
consider saliency detection as a frequency domain analysis problem. Second we
achieve this by employing the concept of it non-saliency. Third we
simultaneously consider the detection of salient regions of different size. The
paper proposes a new bottom-up paradigm for detecting visual saliency
characterized by a scale-space analysis of the amplitude spectrum of natural
images. We show that the convolution of the it image amplitude spectrum with
a low-pass Gaussian kernel of an appropriate scale is equivalent to such an
image saliency detector. The saliency map is obtained by reconstructing the 2-D
signal using the original phase and the amplitude spectrum filtered at a scale
selected by minimizing saliency map entropy. A Hypercomplex Fourier Transform
performs the analysis in the frequency domain. Using available databases we
demonstrate experimentally that the proposed model can predict human fixation
data. We also introduce a new image database and use it to show that the
saliency detector can highlight both small and large salient regions as well
as inhibit repeated distractors in cluttered images. In addition we show that
it is able to predict salient regions on which people focus their attention.
",0,0,1
"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A
  Retrospective Study of Question Answering Modeling Approaches","  Datasets are not only resources for training accurate deployable systems
but are also benchmarks for developing new modeling approaches. While large
natural datasets are necessary for training accurate systems are they
necessary for driving modeling innovation? For example while the popular SQuAD
question answering benchmark has driven the development of new modeling
approaches could synthetic or smaller benchmarks have led to similar
innovations?
  This counterfactual question is impossible to answer but we can study a
necessary condition: the ability for a benchmark to recapitulate findings made
on SQuAD. We conduct a retrospective study of 20 SQuAD modeling approaches
investigating how well 32 existing and synthesized benchmarks concur with SQuAD
-- i.e. do they rank the approaches similarly? We carefully construct small
targeted synthetic benchmarks that do not resemble natural language yet have
high concurrence with SQuAD demonstrating that naturalness and size are not
necessary for reflecting historical modeling improvements on SQuAD. Our results
raise the intriguing possibility that small and carefully designed synthetic
benchmarks may be useful for driving the development of new modeling
approaches.
",0,1,0
Incremental Reading for Question Answering,"  Any system which performs goal-directed continual learning must not only
learn incrementally but process and absorb information incrementally. Such a
system also has to understand when its goals have been achieved. In this paper
we consider these issues in the context of question answering. Current
state-of-the-art question answering models reason over an entire passage not
incrementally. As we will show naive approaches to incremental reading such
as restriction to unidirectional language models in the model perform poorly.
We present extensions to the DocQA [2] model to allow incremental reading
without loss of accuracy. The model also jointly learns to provide the best
answer given the text that is seen so far and predict whether this best-so-far
answer is sufficient.
",0,1,0
StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery,"  Inspired by the ability of StyleGAN to generate highly realistic images in a
variety of domains much recent work has focused on understanding how to use
the latent spaces of StyleGAN to manipulate generated and real images. However
discovering semantically meaningful latent manipulations typically involves
painstaking human examination of the many degrees of freedom or an annotated
collection of images for each desired manipulation. In this work we explore
leveraging the power of recently introduced Contrastive Language-Image
Pre-training (CLIP) models in order to develop a text-based interface for
StyleGAN image manipulation that does not require such manual effort. We first
introduce an optimization scheme that utilizes a CLIP-based loss to modify an
input latent vector in response to a user-provided text prompt. Next we
describe a latent mapper that infers a text-guided latent manipulation step for
a given input image allowing faster and more stable text-based manipulation.
Finally we present a method for mapping a text prompts to input-agnostic
directions in StyleGAN's style space enabling interactive text-driven image
manipulation. Extensive results and comparisons demonstrate the effectiveness
of our approaches.
",0,1,0
Cascade context encoder for improved inpainting,"  In this paper we analyze if cascade usage of the context encoder with
increasing input can improve the results of the inpainting. For this purpose
we train context encoder for 64x64 pixels images in a standard way and use its
resized output to fill in the missing input region of the 128x128 context
encoder both in training and evaluation phase. As the result the inpainting
is visibly more plausible. In order to thoroughly verify the results we
introduce normalized squared-distortion a measure for quantitative inpainting
evaluation and we provide its mathematical explanation. This is the first
attempt to formalize the inpainting measure which is based on the properties
of latent feature representation instead of L2 reconstruction loss.
",0,0,1
Image Synthesis via Semantic Composition,"  In this paper we present a novel approach to synthesize realistic images
based on their semantic layouts. It hypothesizes that for objects with similar
appearance they share similar representation. Our method establishes
dependencies between regions according to their appearance correlation
yielding both spatially variant and associated representations. Conditioning on
these features we propose a dynamic weighted network constructed by spatially
conditional computation (with both convolution and normalization). More than
preserving semantic distinctions the given dynamic network strengthens
semantic relevance benefiting global structure and detail synthesis. We
demonstrate that our method gives the compelling generation performance
qualitatively and quantitatively with extensive experiments on benchmarks.
",0,0,1
On the Evolution of Word Order,"  Most natural languages have a predominant or fixed word order. For example in
English the word order is usually Subject-Verb-Object. This work attempts to
explain this phenomenon as well as other typological findings regarding word
order from a functional perspective. In particular we examine whether fixed
word order provides a functional advantage explaining why these languages are
prevalent. To this end we consider an evolutionary model of language and
demonstrate both theoretically and using genetic algorithms that a language
with a fixed word order is optimal. We also show that adding information to the
sentence such as case markers and noun-verb distinction reduces the need for
fixed word order in accordance with the typological findings.
",0,1,0
"A Survey of Natural Language Generation Techniques with a Focus on
  Dialogue Systems - Past Present and Future Directions","  One of the hardest problems in the area of Natural Language Processing and
Artificial Intelligence is automatically generating language that is coherent
and understandable to humans. Teaching machines how to converse as humans do
falls under the broad umbrella of Natural Language Generation. Recent years
have seen unprecedented growth in the number of research articles published on
this subject in conferences and journals both by academic and industry
researchers. There have also been several workshops organized alongside
top-tier NLP conferences dedicated specifically to this problem. All this
activity makes it hard to clearly define the state of the field and reason
about its future directions. In this work we provide an overview of this
important and thriving area covering traditional approaches statistical
approaches and also approaches that use deep neural networks. We provide a
comprehensive review towards building open domain dialogue systems an
important application of natural language generation. We find that
predominantly the approaches for building dialogue systems use seq2seq or
language models architecture. Notably we identify three important areas of
further research towards building more effective dialogue systems: 1)
incorporating larger context including conversation context and world
knowledge; 2) adding personae or personality in the NLG system; and 3)
overcoming dull and generic responses that affect the quality of
system-produced responses. We provide pointers on how to tackle these open
problems through the use of cognitive architectures that mimic human language
understanding and generation capabilities.
",0,1,0
"Zero-shot super-resolution with a physically-motivated downsampling
  kernel for endomicroscopy","  Super-resolution (SR) methods have seen significant advances thanks to the
development of convolutional neural networks (CNNs). CNNs have been
successfully employed to improve the quality of endomicroscopy imaging. Yet
the inherent limitation of research on SR in endomicroscopy remains the lack of
ground truth high-resolution (HR) images commonly used for both supervised
training and reference-based image quality assessment (IQA). Therefore
alternative methods such as unsupervised SR are being explored. To address the
need for non-reference image quality improvement we designed a novel zero-shot
super-resolution (ZSSR) approach that relies only on the endomicroscopy data to
be processed in a self-supervised manner without the need for ground-truth HR
images. We tailored the proposed pipeline to the idiosyncrasies of
endomicroscopy by introducing both: a physically-motivated Voronoi downscaling
kernel accounting for the endomicroscope's irregular fibre-based sampling
pattern and realistic noise patterns. We also took advantage of video
sequences to exploit a sequence of images for self-supervised zero-shot image
quality improvement. We run ablation studies to assess our contribution in
regards to the downscaling kernel and noise simulation. We validate our
methodology on both synthetic and original data. Synthetic experiments were
assessed with reference-based IQA while our results for original images were
evaluated in a user study conducted with both expert and non-expert observers.
The results demonstrated superior performance in image quality of ZSSR
reconstructions in comparison to the baseline method. The ZSSR is also
competitive when compared to supervised single-image SR especially being the
preferred reconstruction technique by experts.
",0,0,1
FLATM: A Fuzzy Logic Approach Topic Model for Medical Documents,"  One of the challenges for text analysis in medical domains is analyzing
large-scale medical documents. As a consequence finding relevant documents has
become more difficult. One of the popular methods to retrieve information based
on discovering the themes in the documents is topic modeling. The themes in the
documents help to retrieve documents on the same topic with and without a
query. In this paper we present a novel approach to topic modeling using fuzzy
clustering. To evaluate our model we experiment with two text datasets of
medical documents. The evaluation metrics carried out through document
classification and document modeling show that our model produces better
performance than LDA indicating that fuzzy set theory can improve the
performance of topic models in medical domains.
",0,1,0
Phrase Pair Mappings for Hindi-English Statistical Machine Translation,"  In this paper we present our work on the creation of lexical resources for
the Machine Translation between English and Hindi. We describes the development
of phrase pair mappings for our experiments and the comparative performance
evaluation between different trained models on top of the baseline Statistical
Machine Translation system. We focused on augmenting the parallel corpus with
more vocabulary as well as with various inflected forms by exploring different
ways. We have augmented the training corpus with various lexical resources such
as lexical words synset words function words and verb phrases. We have
described the case studies automatic and subjective evaluations detailed
error analysis for both the English to Hindi and Hindi to English machine
translation systems. We further analyzed that there is an incremental growth
in the quality of machine translation with the usage of various lexical
resources. Thus lexical resources do help uplift the translation quality of
resource poor langugaes.
",0,1,0
"Proba-V-ref: Repurposing the Proba-V challenge for reference-aware super
  resolution","  The PROBA-V Super-Resolution challenge distributes real low-resolution image
series and corresponding high-resolution targets to advance research on
Multi-Image Super Resolution (MISR) for satellite images. However in the
PROBA-V dataset the low-resolution image corresponding to the high-resolution
target is not identified. We argue that in doing so the challenge ranks the
proposed methods not only by their MISR performance but mainly by the
heuristics used to guess which image in the series is the most similar to the
high-resolution target. We demonstrate this by improving the performance
obtained by the two winners of the challenge only by using a different
reference image which we compute following a simple heuristic. Based on this
we propose PROBA-V-REF a variant of the PROBA-V dataset in which the reference
image in the low-resolution series is provided and show that the ranking
between the methods changes in this setting. This is relevant to many practical
use cases of MISR where the goal is to super-resolve a specific image of the
series i.e. the reference is known. The proposed PROBA-V-REF should better
reflect the performance of the different methods for this reference-aware MISR
problem.
",0,0,1
"It's Moving! A Probabilistic Model for Causal Motion Segmentation in
  Moving Camera Videos","  The human ability to detect and segment moving objects works in the presence
of multiple objects complex background geometry motion of the observer and
even camouflage. In addition to all of this the ability to detect motion is
nearly instantaneous. While there has been much recent progress in motion
segmentation it still appears we are far from human capabilities. In this
work we derive from first principles a new likelihood function for assessing
the probability of an optical flow vector given the 3D motion direction of an
object. This likelihood uses a novel combination of the angle and magnitude of
the optical flow to maximize the information about the true motions of objects.
Using this new likelihood and several innovations in initialization we develop
a motion segmentation algorithm that beats current state-of-the-art methods by
a large margin. We compare to five state-of-the-art methods on two established
benchmarks and a third new data set of camouflaged animals which we introduce
to push motion segmentation to the next level.
",0,0,1
"Synergistic Learning of Lung Lobe Segmentation and Hierarchical
  Multi-Instance Classification for Automated Severity Assessment of COVID-19
  in CT Images","  Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19)
will help detect infections early and assess the disease progression.
Especially automated severity assessment of COVID-19 in CT images plays an
essential role in identifying cases that are in great need of intensive
clinical care. However it is often challenging to accurately assess the
severity of this disease in CT images due to variable infection regions in the
lungs similar imaging biomarkers and large inter-case variations. To this
end we propose a synergistic learning framework for automated severity
assessment of COVID-19 in 3D CT images by jointly performing lung lobe
segmentation and multi-instance classification. Considering that only a few
infection regions in a CT image are related to the severity assessment we
first represent each input image by a bag that contains a set of 2D image
patches (with each cropped from a specific slice). A multi-task multi-instance
deep network (called M$^2$UNet) is then developed to assess the severity of
COVID-19 patients and also segment the lung lobe simultaneously. Our M$^2$UNet
consists of a patch-level encoder a segmentation sub-network for lung lobe
segmentation and a classification sub-network for severity assessment (with a
unique hierarchical multi-instance learning strategy). Here the context
information provided by segmentation can be implicitly employed to improve the
performance of severity assessment. Extensive experiments were performed on a
real COVID-19 CT image dataset consisting of 666 chest CT images with results
suggesting the effectiveness of our proposed method compared to several
state-of-the-art methods.
",0,0,1
"MeronymNet: A Hierarchical Approach for Unified and Controllable
  Multi-Category Object Generation","  We introduce MeronymNet a novel hierarchical approach for controllable
part-based generation of multi-category objects using a single unified model.
We adopt a guided coarse-to-fine strategy involving semantically conditioned
generation of bounding box layouts pixel-level part layouts and ultimately
the object depictions themselves. We use Graph Convolutional Networks Deep
Recurrent Networks along with custom-designed Conditional Variational
Autoencoders to enable flexible diverse and category-aware generation of 2-D
objects in a controlled manner. The performance scores for generated objects
reflect MeronymNet's superior performance compared to multiple strong baselines
and ablative variants. We also showcase MeronymNet's suitability for
controllable object generation and interactive object editing at various levels
of structural and semantic granularity.
",0,0,1
Rank error-correcting pairs,"  Error-correcting pairs were introduced independently by Pellikaan and
K""otter as a general method of decoding linear codes with respect to the
Hamming metric using coordinatewise products of vectors and are used for many
well-known families of codes. In this paper we define new types of vector
products extending the coordinatewise product some of which preserve symbolic
products of linearized polynomials after evaluation and some of which coincide
with usual products of matrices. Then we define rank error-correcting pairs for
codes that are linear over the extension field and for codes that are linear
over the base field and relate both types. Bounds on the minimum rank distance
of codes and MRD conditions are given. Finally we show that some well-known
families of rank-metric codes admit rank error-correcting pairs and show that
the given algorithm generalizes the classical algorithm using error-correcting
pairs for the Hamming metric.
",1,0,0
Explaining Deep Neural Networks,"  Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas such as computer vision natural
language processing and speech recognition. However the decision-making
processes of these models are generally not interpretable to users. In various
domains such as healthcare finance or law it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore
several directions for explaining neural models have recently been explored. In
this thesis I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods that is methods that aim to explain an already trained and fixed
model (post-hoc) and that provide explanations in terms of input features
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations that is models that have a built-in module that
generates explanations for the predictions of the model.
",0,1,0
Robust fine-tuning of zero-shot models,"  Large pre-trained models such as CLIP or ALIGN offer consistent accuracy
across a range of data distributions when performing zero-shot inference (i.e.
without fine-tuning on a specific dataset). Although existing fine-tuning
methods substantially improve accuracy on a given target distribution they
often reduce robustness to distribution shifts. We address this tension by
introducing a simple and effective method for improving robustness while
fine-tuning: ensembling the weights of the zero-shot and fine-tuned models
(WiSE-FT). Compared to standard fine-tuning WiSE-FT provides large accuracy
improvements under distribution shift while preserving high accuracy on the
target distribution. On ImageNet and five derived distribution shifts WiSE-FT
improves accuracy under distribution shift by 4 to 6 percentage points (pp)
over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves
similarly large robustness gains (2 to 23 pp) on a diverse set of six further
distribution shifts and accuracy gains of 0.8 to 3.3 pp compared to standard
fine-tuning on seven commonly used transfer learning datasets. These
improvements come at no additional computational cost during fine-tuning or
inference.
",0,0,1
"Complete intersection vanishing ideals on sets of clutter type over
  finite fields","  In this paper we give a classification of complete intersection vanishing
ideals on parameterized sets of clutter type over finite fields.
",1,0,0
Meta Internal Learning,"  Internal learning for single-image generation is a framework where a
generator is trained to produce novel images based on a single image. Since
these models are trained on a single image they are limited in their scale and
application. To overcome these issues we propose a meta-learning approach that
enables training over a collection of images in order to model the internal
statistics of the sample image more effectively. In the presented meta-learning
approach a single-image GAN model is generated given an input image via a
convolutional feedforward hypernetwork $f$. This network is trained over a
dataset of images allowing for feature sharing among different models and for
interpolation in the space of generative models. The generated single-image
model contains a hierarchy of multiple generators and discriminators. It is
therefore required to train the meta-learner in an adversarial manner which
requires careful design choices that we justify by a theoretical analysis. Our
results show that the models obtained are as suitable as single-image GANs for
many common image applications significantly reduce the training time per
image without loss in performance and introduce novel capabilities such as
interpolation and feedforward modeling of novel images.
",0,0,1
New MDS Self-dual Codes over Finite Fields of Odd Characteristic,"  In this paper we produce new classes of MDS self-dual codes via (extended)
generalized Reed-Solomon codes over finite fields of odd characteristic. Among
our constructions there are many MDS self-dual codes with new parameters which
have never been reported. For odd prime power $q$ with $q$ square the total
number of lengths for MDS self-dual codes over $mathbbF_q$ presented in this
paper is much more than those in all the previous results.
",1,0,0
A Novel Disparity Transformation Algorithm for Road Segmentation,"  The disparity information provided by stereo cameras has enabled advanced
driver assistance systems to estimate road area more accurately and
effectively. In this paper a novel disparity transformation algorithm is
proposed to extract road areas from dense disparity maps by making the
disparity value of the road pixels become similar. The transformation is
achieved using two parameters: roll angle and fitted disparity value with
respect to each row. To achieve a better processing efficiency golden section
search and dynamic programming are utilised to estimate the roll angle and the
fitted disparity value respectively. By performing a rotation around the
estimated roll angle the disparity distribution of each row becomes very
compact. This further improves the accuracy of the road model estimation as
demonstrated by the various experimental results in this paper. Finally the
Otsu's thresholding method is applied to the transformed disparity map and the
roads can be accurately segmented at pixel level.
",0,0,1
Probabilistic modeling of rational communication with conditionals,"  While a large body of work has scrutinized the meaning of conditional
sentences considerably less attention has been paid to formal models of their
pragmatic use and interpretation. Here we take a probabilistic approach to
pragmatic reasoning about indicative conditionals which flexibly integrates
gradient beliefs about richly structured world states. We model listeners'
update of their prior beliefs about the causal structure of the world and the
joint probabilities of the consequent and antecedent based on assumptions about
the speaker's utterance production protocol. We show that when supplied with
natural contextual assumptions our model uniformly explains a number of
inferences attested in the literature including epistemic inferences
Conditional Perfection and the dependency between antecedent and consequent of
a conditional. We argue that this approach also helps explain three puzzles
introduced by Douven (2012) about updating with conditionals: depending on the
utterance context the listener's belief in the antecedent may increase
decrease or remain unchanged.
",0,1,0
"6D Camera Relocalization in Ambiguous Scenes via Continuous Multimodal
  Inference","  We present a multimodal camera relocalization framework that captures
ambiguities and uncertainties with continuous mixture models defined on the
manifold of camera poses. In highly ambiguous environments which can easily
arise due to symmetries and repetitive structures in the scene computing one
plausible solution (what most state-of-the-art methods currently regress) may
not be sufficient. Instead we predict multiple camera pose hypotheses as well
as the respective uncertainty for each prediction. Towards this aim we use
Bingham distributions to model the orientation of the camera pose and a
multivariate Gaussian to model the position with an end-to-end deep neural
network. By incorporating a Winner-Takes-All training scheme we finally obtain
a mixture model that is well suited for explaining ambiguities in the scene
yet does not suffer from mode collapse a common problem with mixture density
networks. We introduce a new dataset specifically designed to foster camera
localization research in ambiguous environments and exhaustively evaluate our
method on synthetic as well as real data on both ambiguous scenes and on
non-ambiguous benchmark datasets. We plan to release our code and dataset under
$hrefhttps://multimodal3dvision.github.iomultimodal3dvision.github.io$.
",0,0,1
"DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with
  Dynamic Voxelization and 3D Group Convolution","  3D point cloud interpretation is a challenging task due to the randomness and
sparsity of the component points. Many of the recently proposed methods like
PointNet and PointCNN have been focusing on learning shape descriptions from
point coordinates as point-wise input features which usually involves
complicated network architectures. In this work we draw attention back to the
standard 3D convolutions towards an efficient 3D point cloud interpretation.
Instead of converting the entire point cloud into voxel representations like
the other volumetric methods we voxelize the sub-portions of the point cloud
only at necessary locations within each convolution layer on-the-fly using our
dynamic voxelization operation with self-adaptive voxelization resolution. In
addition we incorporate 3D group convolution into our dense convolution kernel
implementation to further exploit the rotation invariant features of point
cloud. Benefiting from its simple fully-convolutional architecture our network
is able to run and converge at a considerably fast speed while yields on-par
or even better performance compared with the state-of-the-art methods on
several benchmark datasets.
",0,0,1
"On the iterative refinement of densely connected representation levels
  for semantic segmentation","  State-of-the-art semantic segmentation approaches increase the receptive
field of their models by using either a downsampling path composed of
poolings/strided convolutions or successive dilated convolutions. However it
is not clear which operation leads to best results. In this paper we
systematically study the differences introduced by distinct receptive field
enlargement methods and their impact on the performance of a novel
architecture called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a
densely connected backbone composed of residual networks. Following standard
image segmentation architectures receptive field enlargement operations that
change the representation level are interleaved among residual networks. This
allows the model to exploit the benefits of both residual and dense
connectivity patterns namely: gradient flow iterative refinement of
representations multi-scale feature combination and deep supervision. In order
to highlight the potential of our model we test it on the challenging CamVid
urban scene understanding benchmark and make the following observations: 1)
downsampling operations outperform dilations when the model is trained from
scratch 2) dilations are useful during the finetuning step of the model 3)
coarser representations require less refinement steps and 4) ResNets (by model
construction) are good regularizers since they can reduce the model capacity
when needed. Finally we compare our architecture to alternative methods and
report state-of-the-art result on the Camvid dataset with at least twice fewer
parameters.
",0,0,1
Network Estimation from Point Process Data,"  Consider observing a collection of discrete events within a network that
reflect how network nodes influence one another. Such data are common in spike
trains recorded from biological neural networks interactions within a social
network and a variety of other settings. Data of this form may be modeled as
self-exciting point processes in which the likelihood of future events depends
on the past events. This paper addresses the problem of estimating
self-excitation parameters and inferring the underlying functional network
structure from self-exciting point process data. Past work in this area was
limited by strong assumptions which are addressed by the novel approach here.
Specifically in this paper we (1) incorporate saturation in a point process
model which both ensures stability and models non-linear thresholding effects;
(2) impose general low-dimensional structural assumptions that include
sparsity group sparsity and low-rankness that allows bounds to be developed in
the high-dimensional setting; and (3) incorporate long-range memory effects
through moving average and higher-order auto-regressive components. Using our
general framework we provide a number of novel theoretical guarantees for
high-dimensional self-exciting point processes that reflect the role played by
the underlying network structure and long-term memory. We also provide
simulations and real data examples to support our methodology and main results.
",1,0,0
"Convergent Block Coordinate Descent for Training Tikhonov Regularized
  Deep Neural Networks","  By lifting the ReLU function into a higher dimensional space we develop a
smooth multi-convex formulation for training feed-forward deep neural networks
(DNNs). This allows us to develop a block coordinate descent (BCD) training
algorithm consisting of a sequence of numerically well-behaved convex
optimizations. Using ideas from proximal point methods in convex analysis we
prove that this BCD algorithm will converge globally to a stationary point with
R-linear convergence rate of order one. In experiments with the MNIST database
DNNs trained with this BCD algorithm consistently yielded better test-set error
rates than identical DNN architectures trained via all the stochastic gradient
descent (SGD) variants in the Caffe toolbox.
",0,0,1
"Semidefinite Programming Approach to Gaussian Sequential Rate-Distortion
  Trade-offs","  Sequential rate-distortion (SRD) theory provides a framework for studying the
fundamental trade-off between data-rate and data-quality in real-time
communication systems. In this paper we consider the SRD problem for
multi-dimensional time-varying Gauss-Markov processes under mean-square
distortion criteria. We first revisit the sensor-estimator separation
principle which asserts that considered SRD problem is equivalent to a joint
sensor and estimator design problem in which data-rate of the sensor output is
minimized while the estimator's performance satisfies the distortion criteria.
We then show that the optimal joint design can be performed by semidefinite
programming. A semidefinite representation of the corresponding SRD function is
obtained. Implications of the obtained result in the context of zero-delay
source coding theory and applications to networked control theory are also
discussed.
",1,0,0
"A Satisfiability Modulo Theory Approach to Secure State Reconstruction
  in Differentially Flat Systems Under Sensor Attacks","  We address the problem of estimating the state of a differentially flat
system from measurements that may be corrupted by an adversarial attack. In
cyber-physical systems malicious attacks can directly compromise the system's
sensors or manipulate the communication between sensors and controllers. We
consider attacks that only corrupt a subset of sensor measurements. We show
that the possibility of reconstructing the state under such attacks is
characterized by a suitable generalization of the notion of s-sparse
observability previously introduced by some of the authors in the linear case.
We also extend our previous work on the use of Satisfiability Modulo Theory
solvers to estimate the state under sensor attacks to the context of
differentially flat systems. The effectiveness of our approach is illustrated
on the problem of controlling a quadrotor under sensor attacks.
",1,0,0
The Dimension and Minimum Distance of Two Classes of Primitive BCH Codes,"  Reed-Solomon codes a type of BCH codes are widely employed in communication
systems storage devices and consumer electronics. This fact demonstrates the
importance of BCH codes -- a family of cyclic codes -- in practice. In theory
BCH codes are among the best cyclic codes in terms of their error-correcting
capability. A subclass of BCH codes are the narrow-sense primitive BCH codes.
However the dimension and minimum distance of these codes are not known in
general. The objective of this paper is to determine the dimension and minimum
distances of two classes of narrow-sense primitive BCH codes with design
distances $delta=(q-1)q^m-1-1-q^lfloor (m-1)/2rfloor$ and
$delta=(q-1)q^m-1-1-q^lfloor (m+1)/2rfloor$. The weight distributions of
some of these BCH codes are also reported. As will be seen the two classes of
BCH codes are sometimes optimal and sometimes among the best linear codes
known.
",1,0,0
Towards High-fidelity Nonlinear 3D Face Morphable Model,"  Embedding 3D morphable basis functions into deep neural networks opens great
potential for models with better representation power. However to faithfully
learn those models from an image collection it requires strong regularization
to overcome ambiguities involved in the learning process. This critically
prevents us from learning high fidelity face models which are needed to
represent face images in high level of details. To address this problem this
paper presents a novel approach to learn additional proxies as means to
side-step strong regularizations as well as leverages to promote detailed
shape/albedo. To ease the learning we also propose to use a dual-pathway
network a carefully-designed architecture that brings a balance between global
and local-based models. By improving the nonlinear 3D morphable model in both
learning objective and network architecture we present a model which is
superior in capturing higher level of details than the linear or its precedent
nonlinear counterparts. As a result our model achieves state-of-the-art
performance on 3D face reconstruction by solely optimizing latent
representations.
",0,0,1
Optimality of codes with respect to error probability in Gaussian noise,"  We consider geometrical optimization problems related to optimizing the error
probability in the presence of a Gaussian noise. One famous questions in the
field is the ""weak simplex conjecture"". We discuss possible approaches to it
and state related conjectures about the Gaussian measure in particular the
conjecture about minimizing of the Gaussian measure of a simplex. We also
consider antipodal codes apply the vSid'ak inequality and establish some
theoretical and some numerical results about their optimality.
",1,0,0
"Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by
  Machine Translation Systems","  In translating text where sentiment is the main message human translators
give particular attention to sentiment-carrying words. The reason is that an
incorrect translation of such words would miss the fundamental aspect of the
source text i.e. the author's sentiment. In the online world MT systems are
extensively used to translate User-Generated Content (UGC) such as reviews
tweets and social media posts where the main message is often the author's
positive or negative attitude towards the topic of the text. It is important in
such scenarios to accurately measure how far an MT system can be a reliable
real-life utility in transferring the correct affect message. This paper
tackles an under-recognised problem in the field of machine translation
evaluation which is judging to what extent automatic metrics concur with the
gold standard of human evaluation for a correct translation of sentiment. We
evaluate the efficacy of conventional quality metrics in spotting a
mistranslation of sentiment especially when it is the sole error in the MT
output. We propose a numerical `sentiment-closeness' measure appropriate for
assessing the accuracy of a translated affect message in UGC text by an MT
system. We will show that incorporating this sentiment-aware measure can
significantly enhance the correlation of some available quality metrics with
the human judgement of an accurate translation of sentiment.
",0,1,0
"Imitation Learning for Fashion Style Based on Hierarchical Multimodal
  Representation","  Fashion is a complex social phenomenon. People follow fashion styles from
demonstrations by experts or fashion icons. However for machine agent
learning to imitate fashion experts from demonstrations can be challenging
especially for complex styles in environments with high-dimensional multimodal
observations. Most existing research regarding fashion outfit composition
utilizes supervised learning methods to mimic the behaviors of style icons.
These methods suffer from distribution shift: because the agent greedily
imitates some given outfit demonstrations it can drift away from one style to
another styles given subtle differences. In this work we propose an
adversarial inverse reinforcement learning formulation to recover reward
functions based on hierarchical multimodal representation (HM-AIRL) during the
imitation process. The hierarchical joint representation can more
comprehensively model the expert composited outfit demonstrations to recover
the reward function. We demonstrate that the proposed HM-AIRL model is able to
recover reward functions that are robust to changes in multimodal observations
enabling us to learn policies under significant variation between different
styles.
",0,0,1
"Predicting Personalized Academic and Career Roads: First Steps Toward a
  Multi-Uses Recommender System","  Nobody knows what one's do in the future and everyone will have had a
different answer to the question : how do you see yourself in five years after
your current job/diploma? In this paper we introduce concepts large categories
of fields of studies or job domains in order to represent the vision of the
future of the user's trajectory. Then we show how they can influence the
prediction when proposing him a set of next steps to take.
",0,1,0
Backpropagation-Friendly Eigendecomposition,"  Eigendecomposition (ED) is widely used in deep networks. However the
backpropagation of its results tends to be numerically unstable whether using
ED directly or approximating it with the Power Iteration method particularly
when dealing with large matrices. While this can be mitigated by partitioning
the data in small and arbitrary groups doing so has no theoretical basis and
makes its impossible to exploit the power of ED to the full. In this paper we
introduce a numerically stable and differentiable approach to leveraging
eigenvectors in deep networks. It can handle large matrices without requiring
to split them. We demonstrate the better robustness of our approach over
standard ED and PI for ZCA whitening an alternative to batch normalization
and for PCA denoising which we introduce as a new normalization strategy for
deep networks aiming to further denoise the network's features.
",0,0,1
Saliency Methods for Explaining Adversarial Attacks,"  The classification decisions of neural networks can be misled by small
imperceptible perturbations. This work aims to explain the misled
classifications using saliency methods. The idea behind saliency methods is to
explain the classification decisions of neural networks by creating so-called
saliency maps. Unfortunately a number of recent publications have shown that
many of the proposed saliency methods do not provide insightful explanations. A
prominent example is Guided Backpropagation (GuidedBP) which simply performs
(partial) image recovery. However our numerical analysis shows the saliency
maps created by GuidedBP do indeed contain class-discriminative information. We
propose a simple and efficient way to enhance the saliency maps. The proposed
enhanced GuidedBP shows the state-of-the-art performance to explain adversary
classifications.
",0,0,1
"Classification of Hematoma: Joint Learning of Semantic Segmentation and
  Classification","  Cerebral hematoma grows rapidly in 6-24 hours and misprediction of the growth
can be fatal if it is not operated by a brain surgeon. There are two types of
cerebral hematomas: one that grows rapidly and the other that does not grow
rapidly. We are developing the technique of artificial intelligence to
determine whether the CT image includes the cerebral hematoma which leads to
the rapid growth. This problem has various difficulties: the few positive cases
in this classification problem of cerebral hematoma and the targeted hematoma
has deformable object. Other difficulties include the imbalance classification
the covariate shift the small data and the spurious correlation problems. It
is difficult with the plain CNN classification such as VGG. This paper proposes
the joint learning of semantic segmentation and classification and evaluate the
performance of this.
",0,0,1
"Decay-Function-Free Time-Aware Attention to Context and Speaker
  Indicator for Spoken Language Understanding","  To capture salient contextual information for spoken language understanding
(SLU) of a dialogue we propose time-aware models that automatically learn the
latent time-decay function of the history without a manual time-decay function.
We also propose a method to identify and label the current speaker to improve
the SLU accuracy. In experiments on the benchmark dataset used in Dialog State
Tracking Challenge 4 the proposed models achieved significantly higher F1
scores than the state-of-the-art contextual models. Finally we analyze the
effectiveness of the introduced models in detail. The analysis demonstrates
that the proposed methods were effective to improve SLU accuracy individually.
",0,1,0
Neural Paraphrase Identification of Questions with Noisy Pretraining,"  We present a solution to the problem of paraphrase identification of
questions. We focus on a recent dataset of question pairs annotated with binary
paraphrase labels and show that a variant of the decomposable attention model
(Parikh et al. 2016) results in accurate performance on this task while being
far simpler than many competing neural architectures. Furthermore when the
model is pretrained on a noisy dataset of automatically collected question
paraphrases it obtains the best reported performance on the dataset.
",0,1,0
"A Channelized Binning Method for Extraction of Dominant Color Pixel
  Value","  The Color is one of the most important and easily identifiable features for
describing the visual content. The MPEG standard has developed a number of
descriptors that covers different aspects of the visual content. The Dominant
color descriptor is one of them. This paper proposes a channelized binning
approach a novel method for extraction of the dominant color pixel value which
is a variant of the dominant color descriptor. The Channelized binning method
treats the problem as a statistical problem and tries to avoid color
quantization and interpolation guessing of number and centroid of dominant
colors. Channelized binning is an iterative approach which automatically
estimates the number of dominant pixel values and their centroids. It operates
on 24 bit full RGB color space by considering one color channel at a time and
hence avoiding the color quantization. Results show that the proposed method
can successfully extract dominant color pixel values.
",0,0,1
Deep Sparse Subspace Clustering,"  In this paper we present a deep extension of Sparse Subspace Clustering
termed Deep Sparse Subspace Clustering (DSSC). Regularized by the unit sphere
distribution assumption for the learned deep features DSSC can infer a new
data affinity matrix by simultaneously satisfying the sparsity principle of SSC
and the nonlinearity given by neural networks. One of the appealing advantages
brought by DSSC is: when original real-world data do not meet the
class-specific linear subspace distribution assumption DSSC can employ neural
networks to make the assumption valid with its hierarchical nonlinear
transformations. To the best of our knowledge this is among the first deep
learning based subspace clustering methods. Extensive experiments are conducted
on four real-world datasets to show the proposed DSSC is significantly superior
to 12 existing methods for subspace clustering.
",0,0,1
Tightly Coupled 3D Lidar Inertial Odometry and Mapping,"  Ego-motion estimation is a fundamental requirement for most mobile robotic
applications. By sensor fusion we can compensate the deficiencies of
stand-alone sensors and provide more reliable estimations. We introduce a
tightly coupled lidar-IMU fusion method in this paper. By jointly minimizing
the cost derived from lidar and IMU measurements the lidar-IMU odometry (LIO)
can perform well with acceptable drift after long-term experiment even in
challenging cases where the lidar measurements can be degraded. Besides to
obtain more reliable estimations of the lidar poses a rotation-constrained
refinement algorithm (LIO-mapping) is proposed to further align the lidar poses
with the global map. The experiment results demonstrate that the proposed
method can estimate the poses of the sensor pair at the IMU update rate with
high precision even under fast motion conditions or with insufficient
features.
",0,0,1
"Standardization of the formal representation of lexical information for
  NLP","  A survey of dictionary models and formats is presented as well as a
presentation of corresponding recent standardisation activities.
",0,1,0
"Unsupervised feature learning for speech using correspondence and
  Siamese networks","  In zero-resource settings where transcribed speech audio is unavailable
unsupervised feature learning is essential for downstream speech processing
tasks. Here we compare two recent methods for frame-level acoustic feature
learning. For both methods unsupervised term discovery is used to find pairs
of word examples of the same unknown type. Dynamic programming is then used to
align the feature frames between each word pair serving as weak top-down
supervision for the two models. For the correspondence autoencoder (CAE)
matching frames are presented as input-output pairs. The Triamese network uses
a contrastive loss to reduce the distance between frames of the same predicted
word type while increasing the distance between negative examples. For the
first time these feature extractors are compared on the same discrimination
tasks using the same weak supervision pairs. We find that on the two datasets
considered here the CAE outperforms the Triamese network. However we show
that a new hybrid correspondence-Triamese approach (CTriamese) consistently
outperforms both the CAE and Triamese models in terms of average precision and
ABX error rates on both English and Xitsonga evaluation data.
",0,1,0
From Zero-Shot Learning to Cold-Start Recommendation,"  Zero-shot learning (ZSL) and cold-start recommendation (CSR) are two
challenging problems in computer vision and recommender system respectively.
In general they are independently investigated in different communities. This
paper however reveals that ZSL and CSR are two extensions of the same
intension. Both of them for instance attempt to predict unseen classes and
involve two spaces one for direct feature representation and the other for
supplementary description. Yet there is no existing approach which addresses
CSR from the ZSL perspective. This work for the first time formulates CSR as
a ZSL problem and a tailor-made ZSL method is proposed to handle CSR.
Specifically we propose a Low-rank Linear Auto-Encoder (LLAE) which
challenges three cruxes i.e. domain shift spurious correlations and
computing efficiency in this paper. LLAE consists of two parts a low-rank
encoder maps user behavior into user attributes and a symmetric decoder
reconstructs user behavior from user attributes. Extensive experiments on both
ZSL and CSR tasks verify that the proposed method is a win-win formulation
i.e. not only can CSR be handled by ZSL models with a significant performance
improvement compared with several conventional state-of-the-art methods but
the consideration of CSR can benefit ZSL as well.
",0,0,1
"ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale
  Demonstrations","  Object manipulation from 3D visual inputs poses many challenges on building
generalizable perception and policy models. However 3D assets in existing
benchmarks mostly lack the diversity of 3D shapes that align with real-world
intra-class complexity in topology and geometry. Here we propose SAPIEN
Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over
diverse objects in a full-physics simulator. 3D assets in ManiSkill include
large intra-class topological and geometric variations. Tasks are carefully
chosen to cover distinct types of manipulation challenges. Latest progress in
3D vision also makes us believe that we should customize the benchmark so that
the challenge is inviting to researchers working on 3D deep learning. To this
end we simulate a moving panoramic camera that returns ego-centric point
clouds or RGB-D images. In addition we would like ManiSkill to serve a broad
set of researchers interested in manipulation research. Besides supporting the
learning of policies from interactions we also support
learning-from-demonstrations (LfD) methods by providing a large number of
high-quality demonstrations (~36000 successful trajectories ~1.5M point
cloud/RGB-D frames in total). We provide baselines using 3D deep learning and
LfD algorithms. All code of our benchmark (simulator environment SDK and
baselines) is open-sourced and a challenge facing interdisciplinary
researchers will be held based on the benchmark.
",0,0,1
Stateless actor-critic for instance segmentation with high-level priors,"  Instance segmentation is an important computer vision problem which remains
challenging despite impressive recent advances due to deep learning-based
methods. Given sufficient training data fully supervised methods can yield
excellent performance but annotation of ground-truth data remains a major
bottleneck especially for biomedical applications where it has to be performed
by domain experts. The amount of labels required can be drastically reduced by
using rules derived from prior knowledge to guide the segmentation. However
these rules are in general not differentiable and thus cannot be used with
existing methods. Here we relax this requirement by using stateless actor
critic reinforcement learning which enables non-differentiable rewards. We
formulate the instance segmentation problem as graph partitioning and the actor
critic predicts the edge weights driven by the rewards which are based on the
conformity of segmented instances to high-level priors on object shape
position or size. The experiments on toy and real datasets demonstrate that we
can achieve excellent performance without any direct supervision based only on
a rich set of priors.
",0,0,1
CenterFace: Joint Face Detection and Alignment Using Face as Point,"  Face detection and alignment in unconstrained environment is always deployed
on edge devices which have limited memory storage and low computing power. This
paper proposes a one-stage method named CenterFace to simultaneously predict
facial box and landmark location with real-time speed and high accuracy. The
proposed method also belongs to the anchor free category. This is achieved by:
(a) learning face existing possibility by the semantic maps (b) learning
bounding box offsets and five landmarks for each position that potentially
contains a face. Specifically the method can run in real-time on a single CPU
core and 200 FPS using NVIDIA 2080TI for VGA-resolution images and can
simultaneously achieve superior accuracy (WIDER FACE Val/Test-Easy:
0.935/0.932 Medium: 0.924/0.921 Hard: 0.875/0.873 and FDDB discontinuous:
0.980 continuous: 0.732). A demo of CenterFace can be available at
https://github.com/Star-Clouds/CenterFace.
",0,0,1
"Smart Library: Identifying Books in a Library using Richly Supervised
  Deep Scene Text Reading","  Physical library collections are valuable and long standing resources for
knowledge and learning. However managing books in a large bookshelf and
finding books on it often leads to tedious manual work especially for large
book collections where books might be missing or misplaced. Recently deep
neural models such as Convolutional Neural Networks (CNN) and Recurrent Neural
Networks (RNN) have achieved great success for scene text detection and
recognition. Motivated by these recent successes we aim to investigate their
viability in facilitating book management a task that introduces further
challenges including large amounts of cluttered scene text distortion and
varied lighting conditions. In this paper we present a library inventory
building and retrieval system based on scene text reading methods. We
specifically design our scene text recognition model using rich supervision to
accelerate training and achieve state-of-the-art performance on several
benchmark datasets. Our proposed system has the potential to greatly reduce the
amount of human labor required in managing book inventories as well as the
space needed to store book information.
",0,0,1
"A complete character recognition and transliteration technique for
  Devanagari script","  Transliteration involves transformation of one script to another based on
phonetic similarities between the characters of two distinctive scripts. In
this paper we present a novel technique for automatic transliteration of
Devanagari script using character recognition. One of the first tasks performed
to isolate the constituent characters is segmentation. Line segmentation
methodology in this manuscript discusses the case of overlapping lines.
Character segmentation algorithm is designed to segment conjuncts and separate
shadow characters. Presented shadow character segmentation scheme employs
connected component method to isolate the character keeping the constituent
characters intact. Statistical features namely different order moments like
area variance skewness and kurtosis along with structural features of
characters are employed in two phase recognition process. After recognition
constituent Devanagari characters are mapped to corresponding roman alphabets
in way that resulting roman alphabets have similar pronunciation to source
characters.
",0,0,1
"Don't 'have a clue'? Unsupervised co-learning of downward-entailing
  operators","  Researchers in textual entailment have begun to consider inferences involving
'downward-entailing operators' an interesting and important class of lexical
items that change the way inferences are made. Recent work proposed a method
for learning English downward-entailing operators that requires access to a
high-quality collection of 'negative polarity items' (NPIs). However English
is one of the very few languages for which such a list exists. We propose the
first approach that can be applied to the many languages for which there is no
pre-existing high-precision database of NPIs. As a case study we apply our
method to Romanian and show that our method yields good results. Also we
perform a cross-linguistic analysis that suggests interesting connections to
some findings in linguistic typology.
",0,1,0
Sparse Training via Boosting Pruning Plasticity with Neuroregeneration,"  Works on lottery ticket hypothesis (LTH) and single-shot network pruning
(SNIP) have raised a lot of attention currently on post-training pruning
(iterative magnitude pruning) and before-training pruning (pruning at
initialization). The former method suffers from an extremely large computation
cost and the latter usually struggles with insufficient performance. In
comparison during-training pruning a class of pruning methods that
simultaneously enjoys the training/inference efficiency and the comparable
performance temporarily has been less explored. To better understand
during-training pruning we quantitatively study the effect of pruning
throughout training from the perspective of pruning plasticity (the ability of
the pruned networks to recover the original performance). Pruning plasticity
can help explain several other empirical observations about neural network
pruning in literature. We further find that pruning plasticity can be
substantially improved by injecting a brain-inspired mechanism called
neuroregeneration i.e. to regenerate the same number of connections as
pruned. We design a novel gradual magnitude pruning (GMP) method named gradual
pruning with zero-cost neuroregeneration (textbfGraNet) that advances state
of the art. Perhaps most impressively its sparse-to-sparse version for the
first time boosts the sparse-to-sparse training performance over various
dense-to-sparse methods with ResNet-50 on ImageNet without extending the
training time. We release all codes in
https://github.com/Shiweiliuiiiiiii/GraNet.
",0,0,1
ViDeNN: Deep Blind Video Denoising,"  We propose ViDeNN: a CNN for Video Denoising without prior knowledge on the
noise distribution (blind denoising). The CNN architecture uses a combination
of spatial and temporal filtering learning to spatially denoise the frames
first and at the same time how to combine their temporal information handling
objects motion brightness changes low-light conditions and temporal
inconsistencies. We demonstrate the importance of the data used for CNNs
training creating for this purpose a specific dataset for low-light
conditions. We test ViDeNN on common benchmarks and on self-collected data
achieving good results comparable with the state-of-the-art.
",0,0,1
"Yet it moves: Learning from Generic Motions to Generate IMU data from
  YouTube videos","  Human activity recognition (HAR) using wearable sensors has benefited much
less from recent advances in Machine Learning than fields such as computer
vision and natural language processing. This is to a large extent due to the
lack of large scale repositories of labeled training data. In our research we
aim to facilitate the use of online videos which exists in ample quantity for
most activities and are much easier to label than sensor data to simulate
labeled wearable motion sensor data. In previous work we already demonstrate
some preliminary results in this direction focusing on very simple activity
specific simulation models and a single sensor modality (acceleration
norm)cite10.1145/3341162.3345590. In this paper we show how we can train a
regression model on generic motions for both accelerometer and gyro signals and
then apply it to videos of the target activities to generate synthetic IMU data
(acceleration and gyro norms) that can be used to train and/or improve HAR
models. We demonstrate that systems trained on simulated data generated by our
regression model can come to within around 10% of the mean F1 score of a system
trained on real sensor data. Furthermore we show that by either including a
small amount of real sensor data for model calibration or simply leveraging the
fact that (in general) we can easily generate much more simulated data from
video than we can collect in terms of real sensor data the advantage of real
sensor data can be eventually equalized.
",0,0,1
"Performance Bounds for Expander-based Compressed Sensing in the presence
  of Poisson Noise","  This paper provides performance bounds for compressed sensing in the presence
of Poisson noise using expander graphs. The Poisson noise model is appropriate
for a variety of applications including low-light imaging and digital
streaming where the signal-independent and/or bounded noise models used in the
compressed sensing literature are no longer applicable. In this paper we
develop a novel sensing paradigm based on expander graphs and propose a MAP
algorithm for recovering sparse or compressible signals from Poisson
observations. The geometry of the expander graphs and the positivity of the
corresponding sensing matrices play a crucial role in establishing the bounds
on the signal reconstruction error of the proposed algorithm. The geometry of
the expander graphs makes them provably superior to random dense sensing
matrices such as Gaussian or partial Fourier ensembles for the Poisson noise
model. We support our results with experimental demonstrations.
",1,0,0
Relative Saliency and Ranking: Models Metrics Data and Benchmarks,"  Salient object detection is a problem that has been considered in detail and
textcolorblackmany solutions have been proposed. In this paper we argue
that work to date has addressed a problem that is relatively ill-posed.
Specifically there is not universal agreement about what constitutes a salient
object when multiple observers are queried. This implies that some objects are
more likely to be judged salient than others and implies a relative rank
exists on salient objects. Initially we present a novel deep learning solution
based on a hierarchical representation of relative saliency and stage-wise
refinement. Further to this we present data analysis and baseline benchmark
results towards addressing the problem of salient object ranking. Methods for
deriving suitable ranked salient object instances are presented along with
metrics suitable to measuring algorithm performance. In addition we show how a
derived dataset can be successively refined to provide cleaned results that
correlate well with pristine ground truth in its characteristics and value for
training and testing models. Finally we provide a comparison among prevailing
algorithms that address salient object ranking or detection to establish
initial baselines providing a basis for comparison with future efforts
addressing this problem. textcolorblackThe source code and data are
publicly available via our project page:
textrmhrefhttps://ryersonvisionlab.github.io/cocosalrank.htmlryersonvisionlab.github.io/cocosalrank
",0,0,1
"Embedding linear codes into self-orthogonal codes and their optimal
  minimum distances","  We obtain a characterization on self-orthogonality for a given binary linear
code in terms of the number of column vectors in its generator matrix which
extends the result of Bouyukliev et al. (2006). As an application we give an
algorithmic method to embed a given binary $k$-dimensional linear code
$mathcalC$ ($k = 234$) into a self-orthogonal code of the shortest length
which has the same dimension $k$ and minimum distance $d' ge d(mathcalC)$.
For $k > 4$ we suggest a recursive method to embed a $k$-dimensional linear
code to a self-orthogonal code. We also give new explicit formulas for the
minimum distances of optimal self-orthogonal codes for any length $n$ with
dimension 4 and any length $n notequiv 6131421222829 pmod31$ with
dimension 5. We determine the exact optimal minimum distances of $[n4]$
self-orthogonal codes which were left open by Li-Xu-Zhao (2008) when $n equiv
0345101112 pmod15$. Then using MAGMA we observe that our embedding
sends an optimal linear code to an optimal self-orthogonal code.
",1,0,0
Proportional Fairness in ALOHA Networks with RF Energy Harvesting,"  In this paper we study wireless powered communication networks that employ
the slotted ALOHA protocol which is the preferred protocol for simple and
uncoordinated networks. In the energy harvesting (EH) phase the base station
broadcasts radio frequency energy to the EH users (EHUs). The EHUs harvest the
broadcasted energy and use it to transmit information back to the base station
by contending for access to the uplink channel in the random access (RA) phase.
In order to ensure fairness among the users we propose a proportionally fair
resource allocation scheme that exploits the RA nature of slotted ALOHA.
Specifically assuming statistical channel state information we determine the
optimal transmit power at the base station the optimal durations of the EH and
RA phases the channel access probability and the rate of each EHU.
",1,0,0
"Reliable Tuberculosis Detection using Chest X-ray with Deep Learning
  Segmentation and Visualization","  Tuberculosis (TB) is a chronic lung disease that occurs due to bacterial
infection and is one of the top 10 leading causes of death. Accurate and early
detection of TB is very important otherwise it could be life-threatening. In
this work we have detected TB reliably from the chest X-ray images using image
pre-processing data augmentation image segmentation and deep-learning
classification techniques. Several public databases were used to create a
database of 700 TB infected and 3500 normal chest X-ray images for this study.
Nine different deep CNNs (ResNet18 ResNet50 ResNet101 ChexNet InceptionV3
Vgg19 DenseNet201 SqueezeNet and MobileNet) which were used for transfer
learning from their pre-trained initial weights and trained validated and
tested for classifying TB and non-TB normal cases. Three different experiments
were carried out in this work: segmentation of X-ray images using two different
U-net models classification using X-ray images and segmented lung images. The
accuracy precision sensitivity F1-score specificity in the detection of
tuberculosis using X-ray images were 97.07 % 97.34 % 97.07 % 97.14 % and
97.36 % respectively. However segmented lungs for the classification
outperformed than whole X-ray image-based classification and accuracy
precision sensitivity F1-score specificity were 99.9 % 99.91 % 99.9 %
99.9 % and 99.52 % respectively. The paper also used a visualization technique
to confirm that CNN learns dominantly from the segmented lung regions results
in higher detection accuracy. The proposed method with state-of-the-art
performance can be useful in the computer-aided faster diagnosis of
tuberculosis.
",0,0,1
"SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual
  Policies","  Generalization has been a long-standing challenge for reinforcement learning
(RL). Visual RL in particular can be easily distracted by irrelevant factors
in high-dimensional observation space. In this work we consider robust policy
learning which targets zero-shot generalization to unseen visual environments
with large distributional shift. We propose SECANT a novel self-expert cloning
technique that leverages image augmentation in two stages to decouple robust
representation learning from policy optimization. Specifically an expert
policy is first trained by RL from scratch with weak augmentations. A student
network then learns to mimic the expert policy by supervised learning with
strong augmentations making its representation more robust against visual
variations compared to the expert. Extensive experiments demonstrate that
SECANT significantly advances the state of the art in zero-shot generalization
across 4 challenging domains. Our average reward improvements over prior SOTAs
are: DeepMind Control (+26.5%) robotic manipulation (+337.8%) vision-based
autonomous driving (+47.7%) and indoor object navigation (+15.8%). Code
release and video are available at https://linxifan.github.io/secant-site/.
",0,0,1
"""So You Think You're Funny?"": Rating the Humour Quotient in Standup
  Comedy","  Computational Humour (CH) has attracted the interest of Natural Language
Processing and Computational Linguistics communities. Creating datasets for
automatic measurement of humour quotient is difficult due to multiple possible
interpretations of the content. In this work we create a multi-modal
humour-annotated dataset ($sim$40 hours) using stand-up comedy clips. We
devise a novel scoring mechanism to annotate the training data with a humour
quotient score using the audience's laughter. The normalized duration (laughter
duration divided by the clip duration) of laughter in each clip is used to
compute this humour coefficient score on a five-point scale (0-4). This method
of scoring is validated by comparing with manually annotated scores wherein a
quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a
model that provides a ""funniness"" score on a five-point scale given the audio
and its corresponding text. We compare various neural language models for the
task of humour-rating and achieve an accuracy of $0.813$ in terms of Quadratic
Weighted Kappa (QWK). Our ""Open Mic"" dataset is released for further research
along with the code.
",0,1,0
Bipartite entanglement of quantum states in a pair basis,"  The unambiguous detection and quantification of entanglement is a hot topic
of scientific research though it is limited to low dimensions or specific
classes of states. Here we identify an additional class of quantum states for
which bipartite entanglement measures can be efficiently computed providing
new rigorous results. Such states are written in arbitrary $dtimes d$
dimensions where each basis state in the subsystem A is paired with only one
state in B. This new class that we refer to as pair basis states is
remarkably relevant in many physical situations including quantum optics. We
find that negativity is a necessary and sufficient measure of entanglement for
mixtures of states written in the same pair basis. We also provide analytical
expressions for a tight lower-bound estimation of the entanglement of
formation a central quantity in quantum information.
",1,0,0
Probabilistic Frame Induction,"  In natural-language discourse related events tend to appear near each other
to describe a larger scenario. Such structures can be formalized by the notion
of a frame (a.k.a. template) which comprises a set of related events and
prototypical participants and event transitions. Identifying frames is a
prerequisite for information extraction and natural language generation and is
usually done manually. Methods for inducing frames have been proposed recently
but they typically use ad hoc procedures and are difficult to diagnose or
extend. In this paper we propose the first probabilistic approach to frame
induction which incorporates frames events participants as latent topics and
learns those frame and event transitions that best explain the text. The number
of frames is inferred by a novel application of a split-merge method from
syntactic parsing. In end-to-end evaluations from text to induced frames and
extracted facts our method produced state-of-the-art results while
substantially reducing engineering effort.
",0,1,0
Capitalization and Punctuation Restoration: a Survey,"  Ensuring proper punctuation and letter casing is a key pre-processing step
towards applying complex natural language processing algorithms. This is
especially significant for textual sources where punctuation and casing are
missing such as the raw output of automatic speech recognition systems.
Additionally short text messages and micro-blogging platforms offer unreliable
and often wrong punctuation and casing. This survey offers an overview of both
historical and state-of-the-art techniques for restoring punctuation and
correcting word casing. Furthermore current challenges and research directions
are highlighted.
",0,1,0
A Novel Hybrid Approach for Cephalometric Landmark Detection,"  Cephalometric analysis has an important role in dentistry and especially in
orthodontics as a treatment planning tool to gauge the size and special
relationships of the teeth jaws and cranium. The first step of using such
analyses is localizing some important landmarks known as cephalometric
landmarks on craniofacial in x-ray image. The past decade has seen a growing
interest in automating this process. In this paper a novel hybrid approach is
proposed for automatic detection of cephalometric landmarks. Here the
landmarks are categorized into three main sets according to their anatomical
characteristics and usage in well-known cephalometric analyses. Consequently
to have a reliable and accurate detection system three methods named edge
tracing weighted template matching and analysis based estimation are
designed each of which is consistent and well-suited for one category. Edge
tracing method is suggested to predict those landmarks which are located on
edges. Weighted template matching method is well-suited for landmarks located
in an obvious and specific structure which can be extracted or searchable in a
given x-ray image. The last but not the least method is named analysis based
estimation. This method is based on the fact that in cephalometric analyses the
relations between landmarks are used and the locations of some landmarks are
never used individually. Therefore the third suggested method has a novelty in
estimating the desired relations directly. The effectiveness of the proposed
approach is compared with the state of the art methods and the results were
promising especially in real world applications.
",0,0,1
Mirror Descent View for Neural Network Quantization,"  Quantizing large Neural Networks (NN) while maintaining the performance is
highly desirable for resource-limited devices due to reduced memory and time
complexity. It is usually formulated as a constrained optimization problem and
optimized via a modified version of gradient descent. In this work by
interpreting the continuous parameters (unconstrained) as the dual of the
quantized ones we introduce a Mirror Descent (MD) framework for NN
quantization. Specifically we provide conditions on the projections (i.e.
mapping from continuous to quantized ones) which would enable us to derive
valid mirror maps and in turn the respective MD updates. Furthermore we
present a numerically stable implementation of MD that requires storing an
additional set of auxiliary variables (unconstrained) and show that it is
strikingly analogous to the Straight Through Estimator (STE) based method which
is typically viewed as a ""trick"" to avoid vanishing gradients issue. Our
experiments on CIFAR-10/100 TinyImageNet and ImageNet classification datasets
with VGG-16 ResNet-18 and MobileNetV2 architectures show that our MD variants
obtain quantized networks with state-of-the-art performance. Code is available
at https://github.com/kartikgupta-at-anu/md-bnn.
",0,0,1
"Iterative label cleaning for transductive and semi-supervised few-shot
  learning","  Few-shot learning amounts to learning representations and acquiring knowledge
such that novel tasks may be solved with both supervision and data being
limited. Improved performance is possible by transductive inference where the
entire test set is available concurrently and semi-supervised learning where
more unlabeled data is available. Focusing on these two settings we introduce
a new algorithm that leverages the manifold structure of the labeled and
unlabeled data distribution to predict pseudo-labels while balancing over
classes and using the loss value distribution of a limited-capacity classifier
to select the cleanest labels iteratively improving the quality of
pseudo-labels. Our solution surpasses or matches the state of the art results
on four benchmark datasets namely miniImageNet tieredImageNet CUB and
CIFAR-FS while being robust over feature space pre-processing and the quantity
of available data. The publicly available source code can be found in
https://github.com/MichalisLazarou/iLPC.
",0,0,1
"Channel Estimation Techniques for Quantized Distributed Reception in
  MIMO Systems","  The Internet of Things (IoT) could enable the development of cloud
multiple-input multiple-output (MIMO) systems where internet-enabled devices
can work as distributed transmission/reception entities. We expect that spatial
multiplexing with distributed reception using cloud MIMO would be a key factor
of future wireless communication systems. In this paper we first review
practical receivers for distributed reception of spatially multiplexed transmit
data where the fusion center relies on quantized received signals conveyed from
geographically separated receive nodes. Using the structures of these
receivers we propose practical channel estimation techniques for the
block-fading scenario. The proposed channel estimation techniques rely on very
simple operations at the received nodes while achieving near-optimal channel
estimation performance as the training length becomes large.
",1,0,0
"Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A
  framework for single-subject analysis in diffusion tensor imaging","  The purpose of this work is to develop a framework for single-subject
analysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)
is capable of testing whether an individual tract as represented by the major
eigenvector of the diffusion tensor and its corresponding angular dispersion
are significantly different from a group of tracts on a voxel-by-voxel basis.
This work develops two complementary statistical tests based on the elliptical
cone of uncertainty (COU) which is a model of uncertainty or dispersion of the
major eigenvector of the diffusion tensor. The orientation deviation test
examines whether the major eigenvector from a single subject is within the
average elliptical COU formed by a collection of elliptical COUs. The shape
deviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test
between the normalized shape measures (area and circumference) of the
elliptical cones of uncertainty of the single subject against a group of
controls. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)
were incorporated in the orientation deviation test. The shape deviation test
uses FDR only. TOADDI was found to be numerically accurate and statistically
effective. Clinical data from two Traumatic Brain Injury (TBI) patients and one
non-TBI subject were tested against the data obtained from a group of 45
non-TBI controls to illustrate the application of the proposed framework in
single-subject analysis. The frontal portion of the superior longitudinal
fasciculus seemed to be implicated in both tests as significantly different
from that of the control group. The TBI patients and the single non-TBI subject
were well separated under the shape deviation test at the chosen FDR level of
0.0005. TOADDI is a simple but novel geometrically based statistical framework
for analyzing DTI data.
",0,0,1
"A deep learning model for gastric diffuse-type adenocarcinoma
  classification in whole slide images","  Gastric diffuse-type adenocarcinoma represents a disproportionately high
percentage of cases of gastric cancers occurring in the young and its relative
incidence seems to be on the rise. Usually it affects the body of the stomach
and presents shorter duration and worse prognosis compared with the
differentiated (intestinal) type adenocarcinoma. The main difficulty
encountered in the differential diagnosis of gastric adenocarcinomas occurs
with the diffuse-type. As the cancer cells of diffuse-type adenocarcinoma are
often single and inconspicuous in a background desmoplaia and inflammation it
can often be mistaken for a wide variety of non-neoplastic lesions including
gastritis or reactive endothelial cells seen in granulation tissue. In this
study we trained deep learning models to classify gastric diffuse-type
adenocarcinoma from WSIs. We evaluated the models on five test sets obtained
from distinct sources achieving receiver operator curve (ROC) area under the
curves (AUCs) in the range of 0.95-0.99. The highly promising results
demonstrate the potential of AI-based computational pathology for aiding
pathologists in their diagnostic workflow system.
",0,0,1
"Testable uniqueness conditions for empirical assessment of undersampling
  levels in total variation-regularized x-ray CT","  We study recoverability in fan-beam computed tomography (CT) with sparsity
and total variation priors: how many underdetermined linear measurements
suffice for recovering images of given sparsity? Results from compressed
sensing (CS) establish such conditions for e.g. random measurements but not
for CT. Recoverability is typically tested by checking whether a computed
solution recovers the original. This approach cannot guarantee solution
uniqueness and the recoverability decision therefore depends on the
optimization algorithm. We propose new computational methods to test
recoverability by verifying solution uniqueness conditions. Using both
reconstruction and uniqueness testing we empirically study the number of CT
measurements sufficient for recovery on new classes of sparse test images. We
demonstrate an average-case relation between sparsity and sufficient sampling
and observe a sharp phase transition as known from CS but never established
for CT. In addition to assessing recoverability more reliably we show that
uniqueness tests are often the faster option.
",1,0,0
"View Blind-spot as Inpainting: Self-Supervised Denoising with Mask
  Guided Residual Convolution","  In recent years self-supervised denoising methods have shown impressive
performance which circumvent painstaking collection procedure of noisy-clean
image pairs in supervised denoising methods and boost denoising applicability
in real world. One of well-known self-supervised denoising strategies is the
blind-spot training scheme. However a few works attempt to improve blind-spot
based self-denoiser in the aspect of network architecture. In this paper we
take an intuitive view of blind-spot strategy and consider its process of using
neighbor pixels to predict manipulated pixels as an inpainting process.
Therefore we propose a novel Mask Guided Residual Convolution (MGRConv) into
common convolutional neural networks e.g. U-Net to promote blind-spot based
denoising. Our MGRConv can be regarded as soft partial convolution and find a
trade-off among partial convolution learnable attention maps and gated
convolution. It enables dynamic mask learning with appropriate mask constrain.
Different from partial convolution and gated convolution it provides moderate
freedom for network learning. It also avoids leveraging external learnable
parameters for mask activation unlike learnable attention maps. The
experiments show that our proposed plug-and-play MGRConv can assist blind-spot
based denoising network to reach promising results on both existing
single-image based and dataset-based methods.
",0,0,1
"Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social
  Media","  Social media currently provide a window on our lives making it possible to
learn how people from different places with different backgrounds ages and
genders use language. In this work we exploit a newly-created Arabic dataset
with ground truth age and gender labels to learn these attributes both
individually and in a multi-task setting at the sentence level. Our models are
based on variations of deep bidirectional neural networks. More specifically
we build models with gated recurrent units and bidirectional encoder
representations from transformers (BERT). We show the utility of multi-task
learning (MTL) on the two tasks and identify task-specific attention as a
superior choice in this context. We also find that a single-task BERT model
outperform our best MTL models on the two tasks. We report tweet-level accuracy
of 51.43% for the age task (three-way) and 65.30% on the gender task (binary)
both of which outperforms our baselines with a large margin. Our models are
language-agnostic and so can be applied to other languages.
",0,1,0
"Parallel Gaussian Channels Corrupted by Independent States With a
  State-Cognitive Helper","  We consider a state-dependent parallel Gaussian channel with independent
states and a common cognitive helper in which two transmitters wish to send
independent information to their corresponding receivers over two parallel
subchannels. Each channel is corrupted by independent additive Gaussian state.
The states are not known to the transmitters nor to the receivers but known to
a helper in a noncausal manner. The helper's goal is to assist a reliable
communication by mitigating the state. Outer and inner bounds are derived and
segments of the capacity region is characterized for various channel
parameters.
",1,0,0
"BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues","  Video-grounded dialogues are very challenging due to (i) the complexity of
videos which contain both spatial and temporal variations and (ii) the
complexity of user utterances which query different segments and/or different
objects in videos over multiple dialogue turns. However existing approaches to
video-grounded dialogues often focus on superficial temporal-level visual cues
but neglect more fine-grained spatial signals from videos. To address this
drawback we propose Bi-directional Spatio-Temporal Learning (BiST) a
vision-language neural framework for high-resolution queries in videos based on
textual cues. Specifically our approach not only exploits both spatial and
temporal-level information but also learns dynamic information diffusion
between the two feature spaces through spatial-to-temporal and
temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the
evolving semantics of user queries in the dialogue setting. The retrieved
visual cues are used as contextual information to construct relevant responses
to the users. Our empirical results and comprehensive qualitative analysis show
that BiST achieves competitive performance and generates reasonable responses
on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA
setting and substantially outperform prior approaches on the TGIF-QA
benchmark.
",0,1,0
Extreme Extraction: Only One Hour per Relation,"  Information Extraction (IE) aims to automatically generate a large knowledge
base from natural language text but progress remains slow. Supervised learning
requires copious human annotation while unsupervised and weakly supervised
approaches do not deliver competitive accuracy. As a result most fielded
applications of IE as well as the leading TAC-KBP systems rely on significant
amounts of manual engineering. Even ""Extreme"" methods such as those reported
in Freedman et al. 2011 require about 10 hours of expert labor per relation.
  This paper shows how to reduce that effort by an order of magnitude. We
present a novel system InstaRead that streamlines authoring with an ensemble
of methods: 1) encoding extraction rules in an expressive and compositional
representation 2) guiding the user to promising rules based on corpus
statistics and mined resources and 3) introducing a new interactive
development cycle that provides immediate feedback --- even on large datasets.
Experiments show that experts can create quality extractors in under an hour
and even NLP novices can author good extractors. These extractors equal or
outperform ones obtained by comparably supervised and state-of-the-art
distantly supervised approaches.
",0,1,0
"Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact
  Verification","  Fact verification is a challenging task that requires simultaneously
reasoning and aggregating over multiple retrieved pieces of evidence to
evaluate the truthfulness of a claim. Existing approaches typically (i) explore
the semantic interaction between the claim and evidence at different
granularity levels but fail to capture their topical consistency during the
reasoning process which we believe is crucial for verification; (ii) aggregate
multiple pieces of evidence equally without considering their implicit stances
to the claim thereby introducing spurious information. To alleviate the above
issues we propose a novel topic-aware evidence reasoning and stance-aware
aggregation model for more accurate fact verification with the following four
key properties: 1) checking topical consistency between the claim and evidence;
2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring
semantic similarity between the global topic information and the semantic
representation of evidence; 4) aggregating evidence based on their implicit
stances to the claim. Extensive experiments conducted on the two benchmark
datasets demonstrate the superiority of the proposed model over several
state-of-the-art approaches for fact verification. The source code can be
obtained from https://github.com/jasenchn/TARSA.
",0,1,0
On DC based Methods for Phase Retrieval,"  In this paper we develop a new computational approach which is based on
minimizing the difference of two convex functionals (DC) to solve a broader
class of phase retrieval problems. The approach splits a standard nonlinear
least squares minimizing function associated with the phase retrieval problem
into the difference of two convex functions and then solves a sequence of
convex minimization sub-problems. For each subproblem the Nesterov's
accelerated gradient descent algorithm or the Barzilai-Borwein (BB) algorithm
is used. In the setting of sparse phase retrieval a standard $ell_1$ norm
term is added into the minimization mentioned above. The subproblem is
approximated by a proximal gradient method which is solved by the
shrinkage-threshold technique directly without iterations. In addition a
modified Attouch-Peypouquet technique is used to accelerate the iterative
computation. These lead to more effective algorithms than the Wirtinger flow
(WF) algorithm and the Gauss-Newton (GN) algorithm and etc.. A convergence
analysis of both DC based algorithms shows that the iterative solutions is
convergent linearly to a critical point and will be closer to a global
minimizer than the given initial starting point. Our study is a deterministic
analysis while the study for the Wirtinger flow (WF) algorithm and its
variants the Gauss-Newton (GN) algorithm the trust region algorithm is based
on the probability analysis. In particular the DC based algorithms are able to
retrieve solutions using a number $m$ of measurements which is about twice of
the number $n$ of entries in the solution with high frequency of successes.
When $mapprox n$ the $ell_1$ DC based algorithm is able to retrieve sparse
signals.
",1,0,0
"'Just because you are right doesn't mean I am wrong': Overcoming a
  Bottleneck in the Development and Evaluation of Open-Ended Visual Question
  Answering (VQA) Tasks","  GQA (Hudson and Manning 2019) is a dataset for real-world visual reasoning
and compositional question answering. We found that many answers predicted by
the best visionlanguage models on the GQA dataset do not match the ground-truth
answer but still are semantically meaningful and correct in the given context.
In fact this is the case with most existing visual question answering (VQA)
datasets where they assume only one ground-truth answer for each question. We
propose Alternative Answer Sets (AAS) of ground-truth answers to address this
limitation which is created automatically using off-the-shelf NLP tools. We
introduce a semantic metric based on AAS and modify top VQA solvers to support
multiple plausible answers for a question. We implement this approach on the
GQA dataset and show the performance improvements.
",0,1,0
"ChannelNets: Compact and Efficient Convolutional Neural Networks via
  Channel-Wise Convolutions","  Convolutional neural networks (CNNs) have shown great capability of solving
various artificial intelligence tasks. However the increasing model size has
raised challenges in employing them in resource-limited applications. In this
work we propose to compress deep models by using channel-wise convolutions
which re- place dense connections among feature maps with sparse ones in CNNs.
Based on this novel operation we build light-weight CNNs known as ChannelNets.
Channel- Nets use three instances of channel-wise convolutions; namely group
channel-wise convolutions depth-wise separable channel-wise convolutions and
the convolu- tional classification layer. Compared to prior CNNs designed for
mobile devices ChannelNets achieve a significant reduction in terms of the
number of parameters and computational cost without loss in accuracy. Notably
our work represents the first attempt to compress the fully-connected
classification layer which usually accounts for about 25% of total parameters
in compact CNNs. Experimental results on the ImageNet dataset demonstrate that
ChannelNets achieve consistently better performance compared to prior methods.
",0,0,1
"Generalizing Natural Language Analysis through Span-relation
  Representations","  Natural language processing covers a wide variety of tasks predicting syntax
semantics and information content and usually each type of output is
generated with specially designed architectures. In this paper we provide the
simple insight that a great variety of tasks can be represented in a single
unified format consisting of labeling spans and relations between spans thus a
single task-independent model can be used across different tasks. We perform
extensive experiments to test this insight on 10 disparate tasks spanning
dependency parsing (syntax) semantic role labeling (semantics) relation
extraction (information content) aspect based sentiment analysis (sentiment)
and many others achieving performance comparable to state-of-the-art
specialized models. We further demonstrate benefits of multi-task learning and
also show that the proposed method makes it easy to analyze differences and
similarities in how the model handles different tasks. Finally we convert
these datasets into a unified format to build a benchmark which provides a
holistic testbed for evaluating future models for generalized natural language
analysis.
",0,1,0
A cookbook of translating English to Xapi,"  The Xapagy cognitive architecture had been designed to perform narrative
reasoning: to model and mimic the activities performed by humans when
witnessing reading recalling narrating and talking about stories. Xapagy
communicates with the outside world using Xapi a simplified ""pidgin"" language
which is strongly tied to the internal representation model (instances scenes
and verb instances) and reasoning techniques (shadows and headless shadows).
While not fully a semantic equivalent of natural language Xapi can represent a
wide range of complex stories. We illustrate the representation technique used
in Xapi through examples taken from folk physics folk psychology as well as
some more unusual literary examples. We argue that while the Xapi model
represents a conceptual shift from the English representation the mapping is
logical and consistent and a trained knowledge engineer can translate between
English and Xapi at near-native speed.
",0,1,0
"(AF)2-S3Net: Attentive Feature Fusion with Adaptive Feature Selection
  for Sparse Semantic Segmentation Network","  Autonomous robotic systems and self driving cars rely on accurate perception
of their surroundings as the safety of the passengers and pedestrians is the
top priority. Semantic segmentation is one the essential components of
environmental perception that provides semantic information of the scene.
Recently several methods have been introduced for 3D LiDAR semantic
segmentation. While they can lead to improved performance they are either
afflicted by high computational complexity therefore are inefficient or lack
fine details of smaller instances. To alleviate this problem we propose
AF2-S3Net an end-to-end encoder-decoder CNN network for 3D LiDAR semantic
segmentation. We present a novel multi-branch attentive feature fusion module
in the encoder and a unique adaptive feature selection module with feature map
re-weighting in the decoder. Our AF2-S3Net fuses the voxel based learning and
point-based learning into a single framework to effectively process the large
3D scene. Our experimental results show that the proposed method outperforms
the state-of-the-art approaches on the large-scale SemanticKITTI benchmark
ranking 1st on the competitive public leaderboard competition upon publication.
",0,0,1
"Fundamental Limits of Decentralized Caching in Fog-RANs with Wireless
  Fronthaul","  This paper aims to characterize the synergy of distributed caching and
wireless fronthaul in a fog radio access network (Fog-RAN) where all edge nodes
(ENs) and user equipments (UEs) have a local cache and store contents
independently at random. The network operates in two phases a file-splitting
based decentralized cache placement phase and a fronthaul-aided content
delivery phase. We adopt normalized delivery time (NDT) to characterize the
asymptotic latency performance with respect to cache size and fronthaul
capacity. Both an achievable upper bound and a theoretical lower bound of NDT
are obtained and their multiplicative gap is within 12. In the proposed
delivery scheme we utilize the fronthaul link by exploiting coded
multicasting to fetch both non-cached and cached contents to boost EN
cooperation in the access link. In particular to fetch contents already cached
at ENs an additional layer of coded multicasting is added on the coded
messages desired by UEs in the fronthaul link. Our analysis shows that the
proposed delivery scheme can balance the delivery latency between the fronthaul
link and access link and is approximately optimum under decentralized caching.
",1,0,0
"$ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous
  Update","  We develop a dictionary learning algorithm by minimizing the $ell_1$
distortion metric on the data term which is known to be robust for
non-Gaussian noise contamination. The proposed algorithm exploits the idea of
iterative minimization of weighted $ell_2$ error. We refer to this algorithm
as $ell_1$-K-SVD where the dictionary atoms and the corresponding sparse
coefficients are simultaneously updated to minimize the $ell_1$ objective
resulting in noise-robustness. We demonstrate through experiments that the
$ell_1$-K-SVD algorithm results in higher atom recovery rate compared with the
K-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.
both in Gaussian and non-Gaussian noise conditions. We also show that for
fixed values of sparsity number of dictionary atoms and data-dimension the
$ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when the
training set available is small. We apply the proposed algorithm for denoising
natural images corrupted by additive Gaussian and Laplacian noise. The images
denoised using $ell_1$-K-SVD are observed to have slightly higher peak
signal-to-noise ratio (PSNR) over K-SVD for Laplacian noise but the
improvement in structural similarity index (SSIM) is significant (approximately
$0.1$) for lower values of input PSNR indicating the efficacy of the $ell_1$
metric.
",0,0,1
"Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal
  Classification","  This paper introduces a Deep Scattering network that utilizes Dual-Tree
complex wavelets to extract translation invariant representations from an input
signal. The computationally efficient Dual-Tree wavelets decompose the input
signal into densely spaced representations over scales. Translation invariance
is introduced in the representations by applying a non-linearity over a region
followed by averaging. The discriminatory information in the densely spaced
locally smooth signal representations aids the learning of the classifier. The
proposed network is shown to outperform Mallat's ScatterNet on four datasets
with different modalities on classification accuracy.
",0,0,1
On Nearest Neighbors in Non Local Means Denoising,"  To denoise a reference patch the Non-Local-Means denoising filter processes
a set of neighbor patches. Few Nearest Neighbors (NN) are used to limit the
computational burden of the algorithm. Here here we show analytically that the
NN approach introduces a bias in the denoised patch and we propose a different
neighbors' collection criterion named Statistical NN (SNN) to alleviate this
issue. Our approach outperforms the traditional one in case of both white and
colored noise: fewer SNNs generate images of higher quality at a lower
computational cost.
",0,0,1
"Emora STDM: A Versatile Framework for Innovative Dialogue System
  Development","  This demo paper presents Emora STDM (State Transition Dialogue Manager) a
dialogue system development framework that provides novel workflows for rapid
prototyping of chat-based dialogue managers as well as collaborative
development of complex interactions. Our framework caters to a wide range of
expertise levels by supporting interoperability between two popular approaches
state machine and information state to dialogue management. Our Natural
Language Expression package allows seamless integration of pattern matching
custom NLP modules and database querying that makes the workflows much more
efficient. As a user study we adopt this framework to an interdisciplinary
undergraduate course where students with both technical and non-technical
backgrounds are able to develop creative dialogue managers in a short period of
time.
",0,1,0
"Solving Quadratic Equations via PhaseLift when There Are About As Many
  Equations As Unknowns","  This note shows that we can recover a complex vector x in C^n exactly from on
the order of n quadratic equations of the form |<a_i x>|^2 = b_i i = 1 ...
m by using a semidefinite program known as PhaseLift. This improves upon
earlier bounds in [3] which required the number of equations to be at least on
the order of n log n. We also demonstrate optimal recovery results from noisy
quadratic measurements; these results are much sharper than previously known
results.
",1,0,0
Vectorial Feedback with Carry Registers and Memory requirements,"  In citemarjane2010 we have introduced vectorial conception of FCSR's in
Fibonacci mode. This conception allows us to easily analyze FCSR's over binary
finite fields $mathbbF_2^n$ for $ngeq 2$. In citeallailou2010 we
describe and study the corresponding Galois mode and use it to design a new
stream cipher. In this paper we introduce the Ring mode for vectorial FCSR
explain the analysis of such Feedback registers and illustrate with a simple
example.
",1,0,0
"Power Control and Scheduling under Hard Deadline Constraints for On-Off
  Fading Channels","  We consider the joint scheduling-and-power-allocation problem of a downlink
cellular system. The system consists of two groups of users: real-time (RT) and
non-real-time (NRT) users. Given some average power constraint on the base
station the problem is to find an algorithm that satisfies the RT and NRT
quality-of-service (QoS) constraints. The RT QoS constraints guarantee the
portion of RT packets that miss their deadline are no more than a pre-specified
threshold. On the other hand the NRT QoS is only to guarantee the stability of
their queues. We propose a sum-rate-maximizing algorithm that satisfy all QoS
and average power constraints. The proposed power allocation policy has a
closed-form expression for the two groups of users. However the power policy
of the RT users differ in structure from the NRT users. The proposed algorithm
is optimal for the on-off channel model with a polynomial-time scheduling
complexity. Using extensive simulations the throughput of the proposed
algorithm is shown exceed existing approaches.
",1,0,0
"E-LPIPS: Robust Perceptual Image Similarity via Random Transformation
  Ensembles","  It has been recently shown that the hidden variables of convolutional neural
networks make for an efficient perceptual similarity metric that accurately
predicts human judgment on relative image similarity assessment. First we show
that such learned perceptual similarity metrics (LPIPS) are susceptible to
adversarial attacks that dramatically contradict human visual similarity
judgment. While this is not surprising in light of neural networks' well-known
weakness to adversarial perturbations we proceed to show that self-ensembling
with an infinite family of random transformations of the input --- a technique
known not to render classification networks robust --- is enough to turn the
metric robust against attack while retaining predictive power on human
judgments. Finally we study the geometry imposed by our our novel
self-ensembled metric (E-LPIPS) on the space of natural images. We find
evidence of ""perceptual convexity"" by showing that convex combinations of
similar-looking images retain appearance and that discrete geodesics yield
meaningful frame interpolation and texture morphing all without explicit
correspondences.
",0,0,1
MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs,"  Multilabel conditional image generation is a challenging problem in computer
vision. In this work we propose Multi-ingredient Pizza Generator (MPG) a
conditional Generative Neural Network (GAN) framework for synthesizing
multilabel images. We design MPG based on a state-of-the-art GAN structure
called StyleGAN2 in which we develop a new conditioning technique by enforcing
intermediate feature maps to learn scalewise label information. Because of the
complex nature of the multilabel image generation problem we also regularize
synthetic image by predicting the corresponding ingredients as well as
encourage the discriminator to distinguish between matched image and mismatched
image. To verify the efficacy of MPG we test it on Pizza10 which is a
carefully annotated multi-ingredient pizza image dataset. MPG can successfully
generate photo-realist pizza images with desired ingredients. The framework can
be easily extend to other multilabel image generation scenarios.
",0,0,1
Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core,"  We present a novel method to track 3D models in color and depth data. To this
end we introduce approximations that accelerate the state-of-the-art in
region-based tracking by an order of magnitude while retaining similar
accuracy. Furthermore we show how the method can be made more robust in the
presence of depth data and consequently formulate a new joint contour and ICP
tracking energy. We present better results than the state-of-the-art while
being much faster then most other methods and achieving all of the above on a
single CPU core.
",0,0,1
Loss Rate Estimators and the Properties for the Tree Topology,"  A large number of explicit estimators are proposed in this paper for loss
rate estimation in a network of the tree topology. All of the estimators are
proved to be unbiased and consistent instead of asymptotic unbiased as that
obtained in [1] for a specific estimator. In addition a set of formulae are
derived for the variances of various maximum likelihood estimators that unveil
the connection between the path of interest and the subtrees connecting the
path to observers. Using the formulae we are able to not only rank the
estimators proposed so far including those proposed in this paper but also
identify the errors made in previous works. More importantly using the
formulae we can easily identify the most efficient explicit estimator from a
pool that makes model selection feasible in loss tomography
",1,0,0
"Global Wheat Challenge 2020: Analysis of the competition design and
  winning models","  Data competitions have become a popular approach to crowdsource new data
analysis methods for general and specialized data science problems. In plant
phenotyping data competitions have a rich history and new outdoor field
datasets have potential for new data competitions. We developed the Global
Wheat Challenge as a generalization competition to see if solutions for wheat
head detection from field images would work in different regions around the
world. In this paper we analyze the winning challenge solutions in terms of
their robustness and the relative importance of model and data augmentation
design decisions. We found that the design of the competition influence the
selection of winning solutions and provide recommendations for future
competitions in an attempt to garner more robust winning solutions.
",0,0,1
"A Systematic Media Frame Analysis of 15 Million New York Times Articles
  from 2000 to 2017","  Framing is an indispensable narrative device for news media because even the
same facts may lead to conflicting understandings if deliberate framing is
employed. Therefore identifying media framing is a crucial step to
understanding how news media influence the public. Framing is however
difficult to operationalize and detect and thus traditional media framing
studies had to rely on manual annotation which is challenging to scale up to
massive news datasets. Here by developing a media frame classifier that
achieves state-of-the-art performance we systematically analyze the media
frames of 1.5 million New York Times articles published from 2000 to 2017. By
examining the ebb and flow of media frames over almost two decades we show
that short-term frame abundance fluctuation closely corresponds to major
events while there also exist several long-term trends such as the gradually
increasing prevalence of the ``Cultural identity'' frame. By examining specific
topics and sentiments we identify characteristics and dynamics of each frame.
Finally as a case study we delve into the framing of mass shootings
revealing three major framing patterns. Our scalable computational approach to
massive news datasets opens up new pathways for systematic media framing
studies.
",0,1,0
"Incorporating Reinforced Adversarial Learning in Autoregressive Image
  Generation","  Autoregressive models recently achieved comparable results versus
state-of-the-art Generative Adversarial Networks (GANs) with the help of Vector
Quantized Variational AutoEncoders (VQ-VAE). However autoregressive models
have several limitations such as exposure bias and their training objective
does not guarantee visual fidelity. To address these limitations we propose to
use Reinforced Adversarial Learning (RAL) based on policy gradient optimization
for autoregressive models. By applying RAL we enable a similar process for
training and testing to address the exposure bias issue. In addition visual
fidelity has been further optimized with adversarial loss inspired by their
strong counterparts: GANs. Due to the slow sampling speed of autoregressive
models we propose to use partial generation for faster training. RAL also
empowers the collaboration between different modules of the VQ-VAE framework.
To our best knowledge the proposed method is first to enable adversarial
learning in autoregressive models for image generation. Experiments on
synthetic and real-world datasets show improvements over the MLE trained
models. The proposed method improves both negative log-likelihood (NLL) and
Fr'echet Inception Distance (FID) which indicates improvements in terms of
visual quality and diversity. The proposed method achieves state-of-the-art
results on Celeba for 64 $times$ 64 image resolution showing promise for
large scale image generation.
",0,0,1
Multigrid Predictive Filter Flow for Unsupervised Learning on Videos,"  We introduce multigrid Predictive Filter Flow (mgPFF) a framework for
unsupervised learning on videos. The mgPFF takes as input a pair of frames and
outputs per-pixel filters to warp one frame to the other. Compared to optical
flow used for warping frames mgPFF is more powerful in modeling sub-pixel
movement and dealing with corruption (e.g. motion blur). We develop a
multigrid coarse-to-fine modeling strategy that avoids the requirement of
learning large filters to capture large displacement. This allows us to train
an extremely compact model (4.6MB) which operates in a progressive way over
multiple resolutions with shared weights. We train mgPFF on unsupervised
free-form videos and show that mgPFF is able to not only estimate long-range
flow for frame reconstruction and detect video shot transitions but also
readily amendable for video object segmentation and pose tracking where it
substantially outperforms the published state-of-the-art without bells and
whistles. Moreover owing to mgPFF's nature of per-pixel filter prediction we
have the unique opportunity to visualize how each pixel is evolving during
solving these tasks thus gaining better interpretability.
",0,0,1
Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection,"  We propose a convolution neural network based algorithm for simultaneously
diagnosing diabetic retinopathy and highlighting suspicious regions. Our
contributions are two folds: 1) a network termed Zoom-in-Net which mimics the
zoom-in process of a clinician to examine the retinal images. Trained with only
image-level supervisions Zoomin-Net can generate attention maps which
highlight suspicious regions and predicts the disease level accurately based
on both the whole image and its high resolution suspicious patches. 2) Only
four bounding boxes generated from the automatically learned attention maps are
enough to cover 80% of the lesions labeled by an experienced ophthalmologist
which shows good localization ability of the attention maps. By clustering
features at high response locations on the attention maps we discover
meaningful clusters which contain potential lesions in diabetic retinopathy.
Experiments show that our algorithm outperform the state-of-the-art methods on
two datasets EyePACS and Messidor.
",0,0,1
"Expansion of the Kullback-Leibler Divergence and a new class of
  information metrics","  Inferring and comparing complex multivariable probability density functions
is fundamental to problems in several fields including probabilistic learning
network theory and data analysis. Classification and prediction are the two
faces of this class of problem. We take an approach here that simplifies many
aspects of these problems by presenting a structured series expansion of the
Kullback-Leibler divergence - a function central to information theory - and
devise a distance metric based on this divergence. Using the M""obius inversion
duality between multivariable entropies and multivariable interaction
information we express the divergence as an additive series in the number of
interacting variables which provides a restricted and simplified set of
distributions to use as approximation and with which to model data. Truncations
of this series yield approximations based on the number of interacting
variables. The first few terms of the expansion-truncation are illustrated and
shown to lead naturally to familiar approximations including the well-known
Kirkwood superposition approximation. Truncation can also induce a simple
relation between the multi-information and the interaction information. A
measure of distance between distributions based on Kullback-Leibler
divergence is then described and shown to be a true metric if properly
restricted. The expansion is shown to generate a hierarchy of metrics and
connects this work to information geometry formalisms. We give an example of
the application of these metrics to a graph comparision problem that shows that
the formalism can be applied to a wide range of network problems provides a
general approach for systematic approximations in numbers of interactions or
connections and a related quantitative metric.
",1,0,0
"Graph-Based Classification of Self-Dual Additive Codes over Finite
  Fields","  Quantum stabilizer states over GF(m) can be represented as self-dual additive
codes over GF(m^2). These codes can be represented as weighted graphs and
orbits of graphs under the generalized local complementation operation
correspond to equivalence classes of codes. We have previously used this fact
to classify self-dual additive codes over GF(4). In this paper we classify
self-dual additive codes over GF(9) GF(16) and GF(25). Assuming that the
classical MDS conjecture holds we are able to classify all self-dual additive
MDS codes over GF(9) by using an extension technique. We prove that the minimum
distance of a self-dual additive code is related to the minimum vertex degree
in the associated graph orbit. Circulant graph codes are introduced and a
computer search reveals that this set contains many strong codes. We show that
some of these codes have highly regular graph representations.
",1,0,0
"Amodal 3D Reconstruction for Robotic Manipulation via Stability and
  Connectivity","  Learning-based 3D object reconstruction enables single- or few-shot
estimation of 3D object models. For robotics this holds the potential to allow
model-based methods to rapidly adapt to novel objects and scenes. Existing 3D
reconstruction techniques optimize for visual reconstruction fidelity
typically measured by chamfer distance or voxel IOU. We find that when applied
to realistic cluttered robotics environments these systems produce
reconstructions with low physical realism resulting in poor task performance
when used for model-based control. We propose ARM an amodal 3D reconstruction
system that introduces (1) a stability prior over object shapes (2) a
connectivity prior and (3) a multi-channel input representation that allows
for reasoning over relationships between groups of objects. By using these
priors over the physical properties of objects our system improves
reconstruction quality not just by standard visual metrics but also
performance of model-based control on a variety of robotics manipulation tasks
in challenging cluttered environments. Code is available at
github.com/wagnew3/ARM.
",0,0,1
"MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet
  without Tricks","  We introduce a simple yet effective distillation framework that is able to
boost the vanilla ResNet-50 to 80%+ Top-1 accuracy on ImageNet without tricks.
We construct such a framework through analyzing the problems in the existing
classification system and simplify the base method ensemble knowledge
distillation via discriminators by: (1) adopting the similarity loss and
discriminator only on the final outputs and (2) using the average of softmax
probabilities from all teacher ensembles as the stronger supervision.
Intriguingly three novel perspectives are presented for distillation: (1)
weight decay can be weakened or even completely removed since the soft label
also has a regularization effect; (2) using a good initialization for students
is critical; and (3) one-hot/hard label is not necessary in the distillation
process if the weights are well initialized. We show that such a
straight-forward framework can achieve state-of-the-art results without
involving any commonly-used techniques such as architecture modification;
outside training data beyond ImageNet; autoaug/randaug; cosine learning rate;
mixup/cutmix training; label smoothing; etc. Our method obtains 80.67% top-1
accuracy on ImageNet using a single crop-size of 224x224 with vanilla
ResNet-50 outperforming the previous state-of-the-arts by a significant margin
under the same network structure. Our result can be regarded as a strong
baseline using knowledge distillation and to our best knowledge this is also
the first method that is able to boost vanilla ResNet-50 to surpass 80% on
ImageNet without architecture modification or additional training data. On
smaller ResNet-18 our distillation framework consistently improves from 69.76%
to 73.19% which shows tremendous practical values in real-world applications.
Our code and models are available at: https://github.com/szq0214/MEAL-V2.
",0,0,1
Photonic Engineering for CV-QKD over Earth-Satellite Channels,"  Quantum Key Distribution (QKD) via satellite offers up the possibility of
unconditionally secure communications on a global scale. Increasing the secret
key rate in such systems via photonic engineering at the source is a topic of
much ongoing research. In this work we investigate the use of photon-added
states and photon-subtracted states derived from two mode squeezed vacuum
states as examples of such photonic engineering. Specifically we determine
which engineered-photonic state provides for better QKD performance when
implemented over channels connecting terrestrial receivers with Low-Earth-Orbit
satellites. We quantify the impact the number of photons that are added or
subtracted has and highlight the role played by the adopted model for
atmospheric turbulence and loss on the predicted key rates. Our results are
presented in terms of the complexity of deployment used with the simplest
deployments ignoring any estimate of the channel and the more sophisticated
deployments involving a feedback loop that is used to optimize the key rate for
each channel estimation. The optimal quantum state is identified for each
deployment scenario investigated.
",1,0,0
TOM-Net: Learning Transparent Object Matting from a Single Image,"  This paper addresses the problem of transparent object matting. Existing
image matting approaches for transparent objects often require tedious
capturing procedures and long processing time which limit their practical use.
In this paper we first formulate transparent object matting as a refractive
flow estimation problem. We then propose a deep learning framework called
TOM-Net for learning the refractive flow. Our framework comprises two parts
namely a multi-scale encoder-decoder network for producing a coarse prediction
and a residual network for refinement. At test time TOM-Net takes a single
image as input and outputs a matte (consisting of an object mask an
attenuation mask and a refractive flow field) in a fast feed-forward pass. As
no off-the-shelf dataset is available for transparent object matting we create
a large-scale synthetic dataset consisting of 158K images of transparent
objects rendered in front of images sampled from the Microsoft COCO dataset. We
also collect a real dataset consisting of 876 samples using 14 transparent
objects and 60 background images. Promising experimental results have been
achieved on both synthetic and real data which clearly demonstrate the
effectiveness of our approach.
",0,0,1
"Low Anisotropy Sense Retrofitting (LASeR) : Towards Isotropic and Sense
  Enriched Representations","  Contextual word representation models have shown massive improvements on a
multitude of NLP tasks yet their word sense disambiguation capabilities remain
poorly explained. To address this gap we assess whether contextual word
representations extracted from deep pretrained language models create
distinguishable representations for different senses of a given word. We
analyze the representation geometry and find that most layers of deep
pretrained language models create highly anisotropic representations pointing
towards the existence of representation degeneration problem in contextual word
representations. After accounting for anisotropy our study further reveals
that there is variability in sense learning capabilities across different
language models. Finally we propose LASeR a 'Low Anisotropy Sense
Retrofitting' approach that renders off-the-shelf representations isotropic and
semantically more meaningful resolving the representation degeneration problem
as a post-processing step and conducting sense-enrichment of contextualized
representations extracted from deep neural language models.
",0,1,0
"Weakly-supervised Video Anomaly Detection with Robust Temporal Feature
  Magnitude Learning","  Anomaly detection with weakly supervised video-level labels is typically
formulated as a multiple instance learning (MIL) problem in which we aim to
identify snippets containing abnormal events with each video represented as a
bag of video snippets. Although current methods show effective detection
performance their recognition of the positive instances i.e. rare abnormal
snippets in the abnormal videos is largely biased by the dominant negative
instances especially when the abnormal events are subtle anomalies that
exhibit only small differences compared with normal events. This issue is
exacerbated in many methods that ignore important video temporal dependencies.
To address this issue we introduce a novel and theoretically sound method
named Robust Temporal Feature Magnitude learning (RTFM) which trains a feature
magnitude learning function to effectively recognise the positive instances
substantially improving the robustness of the MIL approach to the negative
instances from abnormal videos. RTFM also adapts dilated convolutions and
self-attention mechanisms to capture long- and short-range temporal
dependencies to learn the feature magnitude more faithfully. Extensive
experiments show that the RTFM-enabled MIL model (i) outperforms several
state-of-the-art methods by a large margin on four benchmark data sets
(ShanghaiTech UCF-Crime XD-Violence and UCSD-Peds) and (ii) achieves
significantly improved subtle anomaly discriminability and sample efficiency.
Code is available at https://github.com/tianyu0207/RTFM.
",0,0,1
Automatic multi-objective based feature selection for classification,"  Objective: Accurately classifying the malignancy of lesions detected in a
screening scan is critical for reducing false positives. Radiomics holds great
potential to differentiate malignant from benign tumors by extracting and
analyzing a large number of quantitative image features. Since not all radiomic
features contribute to an effective classifying model selecting an optimal
feature subset is critical. Methods: This work proposes a new multi-objective
based feature selection (MO-FS) algorithm that considers sensitivity and
specificity simultaneously as the objective functions during feature selection.
For MO-FS we developed a modified entropy based termination criterion (METC)
that stops the algorithm automatically rather than relying on a preset number
of generations. We also designed a solution selection methodology for
multi-objective learning that uses the evidential reasoning approach (SMOLER)
to automatically select the optimal solution from the Pareto-optimal set.
Furthermore we developed an adaptive mutation operation to generate the
mutation probability in MO-FS automatically. Results: We evaluated the MO-FS
for classifying lung nodule malignancy in low-dose CT and breast lesion
malignancy in digital breast tomosynthesis. Conclusion: The experimental
results demonstrated that the feature set selected by MO-FS achieved better
classification performance than features selected by other commonly used
methods. Significance: The proposed method is general and more effective
radiomic feature selection strategy.
",0,0,1
End-to-end Full Projector Compensation,"  Full projector compensation aims to modify a projector input image to
compensate for both geometric and photometric disturbance of the projection
surface. Traditional methods usually solve the two parts separately and may
suffer from suboptimal solutions. In this paper we propose the first
end-to-end differentiable solution named CompenNeSt++ to solve the two
problems jointly. First we propose a novel geometric correction subnet named
WarpingNet which is designed with a cascaded coarse-to-fine structure to learn
the sampling grid directly from sampling images. Second we propose a novel
photometric compensation subnet named CompenNeSt which is designed with a
siamese architecture to capture the photometric interactions between the
projection surface and the projected images and to use such information to
compensate the geometrically corrected images. By concatenating WarpingNet with
CompenNeSt CompenNeSt++ accomplishes full projector compensation and is
end-to-end trainable. Third to improve practicability we propose a novel
synthetic data-based pre-training strategy to significantly reduce the number
of training images and training time. Moreover we construct the first
setup-independent full compensation benchmark to facilitate future studies. In
thorough experiments our method shows clear advantages over prior art with
promising compensation quality and meanwhile being practically convenient.
",0,0,1
"Dialogue Generation on Infrequent Sentence Functions via Structured
  Meta-Learning","  Sentence function is an important linguistic feature indicating the
communicative purpose in uttering a sentence. Incorporating sentence functions
into conversations has shown improvements in the quality of generated
responses. However the number of utterances for different types of
fine-grained sentence functions is extremely imbalanced. Besides a small number
of high-resource sentence functions a large portion of sentence functions is
infrequent. Consequently dialogue generation conditioned on these infrequent
sentence functions suffers from data deficiency. In this paper we investigate
a structured meta-learning (SML) approach for dialogue generation on infrequent
sentence functions. We treat dialogue generation conditioned on different
sentence functions as separate tasks and apply model-agnostic meta-learning to
high-resource sentence functions data. Furthermore SML enhances meta-learning
effectiveness by promoting knowledge customization among different sentence
functions but simultaneously preserving knowledge generalization for similar
sentence functions. Experimental results demonstrate that SML not only improves
the informativeness and relevance of generated responses but also can generate
responses consistent with the target sentence functions.
",0,1,0
"Why Interpretability in Machine Learning? An Answer Using Distributed
  Detection and Data Fusion Theory","  As artificial intelligence is increasingly affecting all parts of society and
life there is growing recognition that human interpretability of machine
learning models is important. It is often argued that accuracy or other similar
generalization performance metrics must be sacrificed in order to gain
interpretability. Such arguments however fail to acknowledge that the overall
decision-making system is composed of two entities: the learned model and a
human who fuses together model outputs with his or her own information. As
such the relevant performance criteria should be for the entire system not
just for the machine learning component. In this work we characterize the
performance of such two-node tandem data fusion systems using the theory of
distributed detection. In doing so we work in the population setting and model
interpretable learned models as multi-level quantizers. We prove that under our
abstraction the overall system of a human with an interpretable classifier
outperforms one with a black box classifier.
",1,0,0
"The performance evaluation of IEEE 80216 physical layer in the basis of
  bit error rate considering reference channel models","  Fixed Broadband Wireless Access is a promising technology which can offer
high speed data rate from transmitting end to customer end which can offer high
speed text voice and video data. IEEE 802.16 WirelessMAN is a standard that
specifies medium access control layer and a set of PHY layer to fixed and
mobile BWA in broad range of frequencies and it supports equipment
manufacturers due to its robust performance in multipath environment.
Consequently WiMAX forum has adopted this version to develop the network world
wide. In this paper the performance of IEEE 802.16 OFDM PHY Layer has been
investigated by using the simulation model in Matlab. The Stanford University
Interim (SUI) channel models are selected for the performance evaluation of
this standard. The Ideal Channel estimation is considered in this work and the
performance evaluation is observed in the basis of BER.
",1,0,0
"Treadmill Assisted Gait Spoofing (TAGS): An Emerging Threat to wearable
  Sensor-based Gait Authentication","  In this work we examine the impact of Treadmill Assisted Gait Spoofing
(TAGS) on Wearable Sensor-based Gait Authentication (WSGait). We consider more
realistic implementation and deployment scenarios than the previous study
which focused only on the accelerometer sensor and a fixed set of features.
Specifically we consider the situations in which the implementation of WSGait
could be using one or more sensors embedded into modern smartphones. Besides
it could be using different sets of features or different classification
algorithms or both. Despite the use of a variety of sensors feature sets
(ranked by mutual information) and six different classification algorithms
TAGS was able to increase the average False Accept Rate (FAR) from 4% to 26%.
Such a considerable increase in the average FAR especially under the stringent
implementation and deployment scenarios considered in this study calls for a
further investigation into the design of evaluations of WSGait before its
deployment for public use.
",0,0,1
Polar Subcodes,"  An extension of polar codes is proposed which allows some of the frozen
symbols called dynamic frozen symbols to be data-dependent. A construction of
polar codes with dynamic frozen symbols being subcodes of extended BCH codes
is proposed. The proposed codes have higher minimum distance than classical
polar codes but still can be efficiently decoded using the successive
cancellation algorithm and its extensions. The codes with Arikan extended BCH
and Reed-Solomon kernel are considered. The proposed codes are shown to
outperform LDPC and turbo codes as well as polar codes with CRC.
",1,0,0
"MDR Codes: A New Class of RAID-6 Codes with Optimal Rebuilding and
  Encoding","  As storage systems grow in size device failures happen more frequently than
ever before. Given the commodity nature of hard drives employed a storage
system needs to tolerate a certain number of disk failures while maintaining
data integrity and to recover lost data with minimal interference to normal
disk I/O operations. RAID-6 which can tolerate up to two disk failures with
the minimum redundancy is becoming widespread. However traditional RAID-6
codes suffer from high disk I/O overhead during recovery. In this paper we
propose a new family of RAID-6 codes the Minimum Disk I/O Repairable (MDR)
codes which achieve the optimal disk I/O overhead for single failure
recoveries. Moreover we show that MDR codes can be encoded with the minimum
number of bit-wise XOR operations. Simulation results show that MDR codes help
to save about half of disk read operations than traditional RAID-6 codes and
thus can reduce the recovery time by up to 40%.
",1,0,0
"The Stability of Low-Rank Matrix Reconstruction: a Constrained Singular
  Value View","  The stability of low-rank matrix reconstruction with respect to noise is
investigated in this paper. The $ell_*$-constrained minimal singular value
($ell_*$-CMSV) of the measurement operator is shown to determine the recovery
performance of nuclear norm minimization based algorithms. Compared with the
stability results using the matrix restricted isometry constant the
performance bounds established using $ell_*$-CMSV are more concise and their
derivations are less complex. Isotropic and subgaussian measurement operators
are shown to have $ell_*$-CMSVs bounded away from zero with high probability
as long as the number of measurements is relatively large. The $ell_*$-CMSV
for correlated Gaussian operators are also analyzed and used to illustrate the
advantage of $ell_*$-CMSV compared with the matrix restricted isometry
constant. We also provide a fixed point characterization of $ell_*$-CMSV that
is potentially useful for its computation.
",1,0,0
Revisiting Anchor Mechanisms for Temporal Action Localization,"  Most of the current action localization methods follow an anchor-based
pipeline: depicting action instances by pre-defined anchors learning to select
the anchors closest to the ground truth and predicting the confidence of
anchors with refinements. Pre-defined anchors set prior about the location and
duration for action instances which facilitates the localization for common
action instances but limits the flexibility for tackling action instances with
drastic varieties especially for extremely short or extremely long ones. To
address this problem this paper proposes a novel anchor-free action
localization module that assists action localization by temporal points.
Specifically this module represents an action instance as a point with its
distances to the starting boundary and ending boundary alleviating the
pre-defined anchor restrictions in terms of action localization and duration.
The proposed anchor-free module is capable of predicting the action instances
whose duration is either extremely short or extremely long. By combining the
proposed anchor-free module with a conventional anchor-based module we propose
a novel action localization framework called A2Net. The cooperation between
anchor-free and anchor-based modules achieves superior performance to the
state-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore comprehensive
experiments demonstrate the complementarity between the anchor-free and the
anchor-based module making A2Net simple but effective.
",0,0,1
"Deep Multi-Shot Network for modelling Appearance Similarity in
  Multi-Person Tracking applications","  The automatization of Multi-Object Tracking becomes a demanding task in real
unconstrained scenarios where the algorithms have to deal with crowds
crossing people occlusions disappearances and the presence of visually
similar individuals. In those circumstances the data association between the
incoming detections and their corresponding identities could miss some tracks
or produce identity switches. In order to reduce these tracking errors and
even their propagation in further frames this article presents a Deep
Multi-Shot neural model for measuring the Degree of Appearance Similarity
(MS-DoAS) between person observations. This model provides temporal consistency
to the individuals' appearance representation and provides an affinity metric
to perform frame-by-frame data association allowing online tracking. The model
has been deliberately trained to be able to manage the presence of previous
identity switches and missed observations in the handled tracks. With that
purpose a novel data generation tool has been designed to create training
tracklets that simulate such situations. The model has demonstrated a high
capacity to discern when a new observation corresponds to a certain track
achieving a classification accuracy of 97% in a hard test that simulates
tracks with previous mistakes. Moreover the tracking efficiency of the model
in a Surveillance application has been demonstrated by integrating that into
the frame-by-frame association of a Tracking-by-Detection algorithm.
",0,0,1
On the Relation Between the Common Labelling and the Median Graph,"  In structural pattern recognition given a set of graphs the computation of
a Generalized Median Graph is a well known problem. Some methods approach the
problem by assuming a relation between the Generalized Median Graph and the
Common Labelling problem. However this relation has still not been formally
proved. In this paper we analyse such relation between both problems. The main
result proves that the cost of the common labelling upper-bounds the cost of
the median with respect to the given set. In addition we show that the two
problems are equivalent in some cases.
",0,0,1
VIP: Finding Important People in Images,"  People preserve memories of events such as birthdays weddings or vacations
by capturing photos often depicting groups of people. Invariably some
individuals in the image are more important than others given the context of
the event. This paper analyzes the concept of the importance of individuals in
group photographs. We address two specific questions -- Given an image who are
the most important individuals in it? Given multiple images of a person which
image depicts the person in the most important role? We introduce a measure of
importance of people in images and investigate the correlation between
importance and visual saliency. We find that not only can we automatically
predict the importance of people from purely visual cues incorporating this
predicted importance results in significant improvement in applications such as
im2text (generating sentences that describe images of groups of people).
",0,0,1
Procedural Reasoning Networks for Understanding Multimodal Procedures,"  This paper addresses the problem of comprehending procedural commonsense
knowledge. This is a challenging task as it requires identifying key entities
keeping track of their state changes and understanding temporal and causal
relations. Contrary to most of the previous work in this study we do not rely
on strong inductive bias and explore the question of how multimodality can be
exploited to provide a complementary semantic signal. Towards this end we
introduce a new entity-aware neural comprehension model augmented with external
relational memory units. Our model learns to dynamically update entity states
in relation to each other while reading the text instructions. Our experimental
analysis on the visual reasoning tasks in the recently proposed RecipeQA
dataset reveals that our approach improves the accuracy of the previously
reported models by a large margin. Moreover we find that our model learns
effective dynamic representations of entities even though we do not use any
supervision at the level of entity states.
",0,1,0
Data Noising as Smoothing in Neural Network Language Models,"  Data noising is an effective technique for regularizing neural network
models. While noising is widely adopted in application domains such as vision
and speech commonly used noising primitives have not been developed for
discrete sequence-level settings such as language modeling. In this paper we
derive a connection between input noising in neural network language models and
smoothing in $n$-gram models. Using this connection we draw upon ideas from
smoothing to develop effective noising schemes. We demonstrate performance
gains when applying the proposed schemes to language modeling and machine
translation. Finally we provide empirical analysis validating the relationship
between noising and smoothing.
",0,1,0
OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning,"  Learning image representations without human supervision is an important and
active research field. Several recent approaches have successfully leveraged
the idea of making such a representation invariant under different types of
perturbations especially via contrastive-based instance discrimination
training. Although effective visual representations should indeed exhibit such
invariances there are other important characteristics such as encoding
contextual reasoning skills for which alternative reconstruction-based
approaches might be better suited.
  With this in mind we propose a teacher-student scheme to learn
representations by training a convolutional net to reconstruct a
bag-of-visual-words (BoW) representation of an image given as input a
perturbed version of that same image. Our strategy performs an online training
of both the teacher network (whose role is to generate the BoW targets) and the
student network (whose role is to learn representations) along with an online
update of the visual-words vocabulary (used for the BoW targets). This idea
effectively enables fully online BoW-guided unsupervised learning. Extensive
experiments demonstrate the interest of our BoW-based strategy which surpasses
previous state-of-the-art methods (including contrastive-based ones) in several
applications. For instance in downstream tasks such Pascal object detection
Pascal classification and Places205 classification our method improves over
all prior unsupervised approaches thus establishing new state-of-the-art
results that are also significantly better even than those of supervised
pre-training. We provide the implementation code at
https://github.com/valeoai/obow.
",0,0,1
Context-aware Natural Language Generation with Recurrent Neural Networks,"  This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding the
context information are attended through a gating mechanism addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data in which rich
contexts are available and two informative contexts sentiments and products
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50% of the fake reviews are misclassified as the
real reviews and more than 90% are misclassified by existing state-of-the-art
fake review detection algorithm.
",0,1,0
"Continuously heterogeneous hyper-objects in cryo-EM and 3-D movies of
  many temporal dimensions","  Single particle cryo-electron microscopy (EM) is an increasingly popular
method for determining the 3-D structure of macromolecules from noisy 2-D
images of single macromolecules whose orientations and positions are random and
unknown. One of the great opportunities in cryo-EM is to recover the structure
of macromolecules in heterogeneous samples where multiple types or multiple
conformations are mixed together. Indeed in recent years many tools have been
introduced for the analysis of multiple discrete classes of molecules mixed
together in a cryo-EM experiment. However many interesting structures have a
continuum of conformations which do not fit discrete models nicely; the
analysis of such continuously heterogeneous models has remained a more elusive
goal. In this manuscript we propose to represent heterogeneous molecules and
similar structures as higher dimensional objects. We generalize the basic
operations used in many existing reconstruction algorithms making our approach
generic in the sense that in principle existing algorithms can be adapted to
reconstruct those higher dimensional objects. As proof of concept we present a
prototype of a new algorithm which we use to solve simulated reconstruction
problems.
",0,0,1
Attention Correctness in Neural Image Captioning,"  Attention mechanisms have recently been introduced in deep learning for
various tasks in natural language processing and computer vision. But despite
their popularity the ""correctness"" of the implicitly-learned attention maps
has only been assessed qualitatively by visualization of several examples. In
this paper we focus on evaluating and improving the correctness of attention in
neural image captioning models. Specifically we propose a quantitative
evaluation metric for the consistency between the generated attention maps and
human annotations using recently released datasets with alignment between
regions in images and entities in captions. We then propose novel models with
different levels of explicit supervision for learning attention maps during
training. The supervision can be strong when alignment between regions and
caption entities are available or weak when only object segments and
categories are provided. We show on the popular Flickr30k and COCO datasets
that introducing supervision of attention maps during training solidly improves
both attention correctness and caption quality showing the promise of making
machine perception more human-like.
",0,1,0
Expectation of the Largest bet size in Labouchere System,"  For Labouchere system with winning probability $p$ at each coup we prove
that the expectation of the largest bet size under any initial list is finite
if $p>frac12$ and is infinite if $ple frac12$ solving the open
conjecture in Grimmett and Stirzaker (2001). The same result holds for a
general family of betting systems and the proof builds upon a recursive
representation of the optimal betting system in the larger family.
",1,0,0
"Temporal Autoencoder with U-Net Style Skip-Connections for Frame
  Prediction","  Finding sustainable and novel solutions to predict city-wide mobility
behaviour is an ever-growing problem given increased urban complexity and
growing populations. This paper seeks to address this by describing a traffic
frame prediction approach that uses Convolutional LSTMs to create a Temporal
Autoencoder with U-Net style skip-connections that marry together recurrent and
traditional computer vision techniques to capture spatio-temporal dependencies
at different scales without losing topological details of a given city.
Utilisation of Cyclical Learning Rates is also presented improving training
efficiency by achieving lower loss scores in fewer epochs than standard
approaches.
",0,0,1
Reframing Instructional Prompts to GPTk's Language,"  How can model designers turn task instructions into effective prompts for
language models? Backed by extensive empirical analysis on GPT3 we observe
important features for successful instructional prompts and propose several
reframing techniques for model designers to create such prompts. For example a
complex task can be decomposed into multiple simpler tasks. We experiment over
12 NLP tasks across 6 diverse categories (question generation classification
etc.). Our results show that reframing improves few-shot and zero-shot learning
performance by 14% and 17% respectively while reducing sample complexity over
other recent few-shot baselines. The performance gains are particularly
important on large language models such as GPT3 where tuning models or prompts
on large datasets is not feasible. Furthermore we observe that such gains are
not limited to GPT3; the reframed tasks remain superior over raw instructions
across different model architectures underscoring the cross-model generality
of these guidelines. We hope these empirical-driven techniques will pave way
for more effective ways to prompt LMs in the future.
",0,1,0
Motion-Augmented Self-Training for Video Recognition at Smaller Scale,"  The goal of this paper is to self-train a 3D convolutional neural network on
an unlabeled video collection for deployment on small-scale video collections.
As smaller video datasets benefit more from motion than appearance we strive
to train our network using optical flow but avoid its computation during
inference. We propose the first motion-augmented self-training regime we call
MotionFit. We start with supervised training of a motion model on a small and
labeled video collection. With the motion model we generate pseudo-labels for
a large unlabeled video collection which enables us to transfer knowledge by
learning to predict these pseudo-labels with an appearance model. Moreover we
introduce a multi-clip loss as a simple yet efficient way to improve the
quality of the pseudo-labeling even without additional auxiliary tasks. We
also take into consideration the temporal granularity of videos during
self-training of the appearance model which was missed in previous works. As a
result we obtain a strong motion-augmented representation model suited for
video downstream tasks like action recognition and clip retrieval. On
small-scale video datasets MotionFit outperforms alternatives for knowledge
transfer by 5%-8% video-only self-supervision by 1%-7% and semi-supervised
learning by 9%-18% using the same amount of class labels.
",0,0,1
"Learning High-level Image Representation for Image Retrieval via
  Multi-Task DNN using Clickthrough Data","  Image retrieval refers to finding relevant images from an image database for
a query which is considered difficult for the gap between low-level
representation of images and high-level representation of queries. Recently
further developed Deep Neural Network sheds light on automatically learning
high-level image representation from raw pixels. In this paper we proposed a
multi-task DNN learned for image retrieval which contains two parts i.e.
query-sharing layers for image representation computation and query-specific
layers for relevance estimation. The weights of multi-task DNN are learned on
clickthrough data by Ring Training. Experimental results on both simulated and
real dataset show the effectiveness of the proposed method.
",0,0,1
A bio-inspired image coder with temporal scalability,"  We present a novel bio-inspired and dynamic coding scheme for static images.
Our coder aims at reproducing the main steps of the visual stimulus processing
in the mammalian retina taking into account its time behavior. The main novelty
of this work is to show how to exploit the time behavior of the retina cells to
ensure in a simple way scalability and bit allocation. To do so our main
source of inspiration will be the biologically plausible retina model called
Virtual Retina. Following a similar structure our model has two stages. The
first stage is an image transform which is performed by the outer layers in the
retina. Here it is modelled by filtering the image with a bank of difference of
Gaussians with time-delays. The second stage is a time-dependent
analog-to-digital conversion which is performed by the inner layers in the
retina. Thanks to its conception our coder enables scalability and bit
allocation across time. Also our decoded images do not show annoying artefacts
such as ringing and block effects. As a whole this article shows how to
capture the main properties of a biological system here the retina in order
to design a new efficient coder.
",1,0,0
"A Real-Time Predictive Pedestrian Collision Warning Service for
  Cooperative Intelligent Transportation Systems Using 3D Pose Estimation","  Minimizing traffic accidents between vehicles and pedestrians is one of the
primary research goals in intelligent transportation systems. To achieve the
goal pedestrian orientation recognition and prediction of pedestrian's
crossing or not-crossing intention play a central role. Contemporary approaches
do not guarantee satisfactory performance due to limited field-of-view lack of
generalization and high computational complexity. To overcome these
limitations we propose a real-time predictive pedestrian collision warning
service (P2CWS) for two tasks: pedestrian orientation recognition (100.53 FPS)
and intention prediction (35.76 FPS). Our framework obtains satisfying
generalization over multiple sites because of the proposed site-independent
features. At the center of the feature extraction lies 3D pose estimation. The
3D pose analysis enables robust and accurate recognition of pedestrian
orientations and prediction of intentions over multiple sites. The proposed
vision framework realizes 89.3% accuracy in the behavior recognition task on
the TUD dataset without any training process and 91.28% accuracy in intention
prediction on our dataset achieving new state-of-the-art performance. To
contribute to the corresponding research community we make our source codes
public which are available at https://github.com/Uehwan/VisionForPedestrian
",0,0,1
"Quantization and Feedback of Spatial Covariance Matrix for Massive MIMO
  Systems with Cascaded Precoding","  In this paper we investigate the quantization and the feedback of downlink
spatial covariance matrix for massive multiple-input multiple-output (MIMO)
systems with cascaded precoding. Massive MIMO has gained a lot of attention
recently because of its ability to significantly improve the network
performance. To reduce the overhead of downlink channel estimation and uplink
feedback in frequency-division duplex massive MIMO systems cascaded precoding
has been proposed where the outer precoder is implemented using traditional
limited feedback while the inner precoder is determined by the spatial
covariance matrix of the channels. In massive MIMO systems it is difficult to
quantize the spatial covariance matrix because of its large size caused by the
huge number of antennas. In this paper we propose a spatial spectrum based
approach for the quantization and the feedback of the spatial covariance
matrix. The proposed inner precoder can be viewed as modulated discrete prolate
spheroidal sequences and thus achieve much smaller spatial leakage than the
traditional discrete Fourier transform submatrix based precoding. Practical
issues for the application of the proposed approach are also addressed in this
paper.
",1,0,0
"DeepCopy: Grounded Response Generation with Hierarchical Pointer
  Networks","  Recent advances in neural sequence-to-sequence models have led to promising
results for several language generation-based tasks including dialogue
response generation summarization and machine translation. However these
models are known to have several problems especially in the context of
chit-chat based dialogue systems: they tend to generate short and dull
responses that are often too generic. Furthermore these models do not ground
conversational responses on knowledge and facts resulting in turns that are
not accurate informative and engaging for the users. In this paper we propose
and experiment with a series of response generation models that aim to serve in
the general scenario where in addition to the dialogue context relevant
unstructured external knowledge in the form of text is also assumed to be
available for models to harness. Our proposed approach extends
pointer-generator networks (See et al. 2017) by allowing the decoder to
hierarchically attend and copy from external knowledge in addition to the
dialogue context. We empirically show the effectiveness of the proposed model
compared to several baselines including (Ghazvininejad et al. 2018; Zhang et
al. 2018) through both automatic evaluation metrics and human evaluation on
CONVAI2 dataset.
",0,1,0
A spatiotemporal model with visual attention for video classification,"  High level understanding of sequential visual input is important for safe and
stable autonomy especially in localization and object detection. While
traditional object classification and tracking approaches are specifically
designed to handle variations in rotation and scale current state-of-the-art
approaches based on deep learning achieve better performance. This paper
focuses on developing a spatiotemporal model to handle videos containing moving
objects with rotation and scale changes. Built on models that combine
Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to
classify sequential data this work investigates the effectiveness of
incorporating attention modules in the CNN stage for video classification. The
superiority of the proposed spatiotemporal model is demonstrated on the Moving
MNIST dataset augmented with rotation and scaling.
",0,0,1
"VolumeNet: A Lightweight Parallel Network for Super-Resolution of
  Medical Volumetric Data","  Deep learning-based super-resolution (SR) techniques have generally achieved
excellent performance in the computer vision field. Recently it has been
proven that three-dimensional (3D) SR for medical volumetric data delivers
better visual results than conventional two-dimensional (2D) processing.
However deepening and widening 3D networks increases training difficulty
significantly due to the large number of parameters and small number of
training samples. Thus we propose a 3D convolutional neural network (CNN) for
SR of medical volumetric data called ParallelNet using parallel connections. We
construct a parallel connection structure based on the group convolution and
feature aggregation to build a 3D CNN that is as wide as possible with few
parameters. As a result the model thoroughly learns more feature maps with
larger receptive fields. In addition to further improve accuracy we present
an efficient version of ParallelNet (called VolumeNet) which reduces the
number of parameters and deepens ParallelNet using a proposed lightweight
building block module called the Queue module. Unlike most lightweight CNNs
based on depthwise convolutions the Queue module is primarily constructed
using separable 2D cross-channel convolutions. As a result the number of
network parameters and computational complexity can be reduced significantly
while maintaining accuracy due to full channel fusion. Experimental results
demonstrate that the proposed VolumeNet significantly reduces the number of
model parameters and achieves high precision results compared to
state-of-the-art methods.
",0,0,1
Cross Subspace Alignment Codes for Coded Distributed Batch Computation,"  Coded distributed batch computation distributes a computation task such as
matrix multiplication $N$-linear computation or multivariate polynomial
evaluation across $S$ servers through a coding scheme such that the response
from any $R$ servers ($R$ is called the recovery threshold) is sufficient for
the user to recover the desired computed value. Current approaches are based on
either exclusively matrix-partitioning (Entangled Polynomial (EP) Codes for
matrix multiplication) or exclusively batch processing (Lagrange Coded
Computing (LCC)). We present three related classes of codes based on the idea
of Cross-Subspace Alignment (CSA) which was introduced originally in the
context of private information retrieval. CSA codes are characterized by a
Cauchy-Vandermonde matrix structure that facilitates interference alignment
along Vandermonde terms while the desired computations remain resolvable along
the Cauchy terms. These codes unify generalize and improve upon the
state-of-art codes for distributed computing. First we introduce CSA codes for
matrix multiplication which yield LCC codes as a special case and are shown
to outperform LCC codes in general over strictly download-limited settings.
Next we introduce Generalized CSA (GCSA) codes for matrix multiplication that
bridge the extremes of matrix-partitioning and batch processing approaches.
Finally we introduce $N$-CSA codes for $N$-linear distributed batch
computations and multivariate batch polynomial evaluations. $N$-CSA codes
include LCC codes as a special case and are in general capable of achieving
significantly lower downloads than LCC codes due to cross-subspace alignment.
Generalizations of $N$-CSA codes to include $X$-secure data and $B$-byzantine
servers are also obtained.
",1,0,0
"A Robust Secure Hybrid Analog and Digital Receive Beamforming Scheme for
  Efficient Interference Reduction","  Medium-scale or large-scale receive antenna array with digital beamforming
can be employed at receiver to make a significant interference reduction but
leads to expensive cost and high complexity of the RF-chain circuit. To deal
with this issue a classic analog-and-digital beamforming (ADB) structure was
proposed in the literature for greatly reducing the number of RF-chains. Based
on the ADB structure we in this paper propose a robust hybrid ADB scheme to
resist directions of arrival (DOAs) estimation errors. The key idea of our
scheme is to employ null space projection (NSP) in analog beamforming domain
and diagonal loading (DL) method in digital beamforming domain. Simulation
results show that the proposed scheme performs more robustly and moreover has
a significant improvement on the receive signal to interference plus noise
ratio (SINR) compared to NSP ADB scheme and DL method.
",1,0,0
Characterization of Industrial Smoke Plumes from Remote Sensing Data,"  The major driver of global warming has been identified as the anthropogenic
release of greenhouse gas (GHG) emissions from industrial activities. The
quantitative monitoring of these emissions is mandatory to fully understand
their effect on the Earth's climate and to enforce emission regulations on a
large scale. In this work we investigate the possibility to detect and
quantify industrial smoke plumes from globally and freely available multi-band
image data from ESA's Sentinel-2 satellites. Using a modified ResNet-50 we can
detect smoke plumes of different sizes with an accuracy of 94.3%. The model
correctly ignores natural clouds and focuses on those imaging channels that are
related to the spectral absorption from aerosols and water vapor enabling the
localization of smoke. We exploit this localization ability and train a U-Net
segmentation model on a labeled sub-sample of our data resulting in an
Intersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the
detection of any smoke plume of 94.0%; on average our model can reproduce the
area covered by smoke in an image to within 5.6%. The performance of our model
is mostly limited by occasional confusion with surface objects the inability
to identify semi-transparent smoke and human limitations to properly identify
smoke based on RGB-only images. Nevertheless our results enable us to reliably
detect and qualitatively estimate the level of smoke activity in order to
monitor activity in industrial plants across the globe. Our data set and code
base are publicly available.
",0,0,1
A locality-based approach for coded computation,"  Modern distributed computation infrastructures are often plagued by
unavailabilities such as failing or slow servers. These unavailabilities
adversely affect the tail latency of computation in distributed
infrastructures. The simple solution of replicating computation entails
significant resource overhead. Coded computation has emerged as a
resource-efficient alternative wherein multiple units of data are encoded to
create parity units and the function to be computed is applied to each of these
units on distinct servers. A decoder can use the available function outputs to
decode the unavailable ones. Existing coded computation approaches are resource
efficient only for simple variants of linear functions such as multilinear
with even the class of low degree polynomials requiring the same multiplicative
overhead as replication for practically relevant straggler tolerance.
  In this paper we present a new approach to model coded computation via the
lens of locality of codes. We introduce a generalized notion of locality
denoted computational locality building upon the locality of an appropriately
defined code. We show that computational locality is equivalent to the required
number of workers for coded computation and leverage results from the
well-studied locality of codes to design coded computation schemes. We show
that recent results on coded computation of multivariate polynomials can be
derived using local recovering schemes for Reed-Muller codes. We present coded
computation schemes for multivariate polynomials that adaptively exploit
locality properties of input data-- an inadmissible technique under existing
frameworks. These schemes require fewer workers than the lower bound under
existing coded computation frameworks showing that the existing multiplicative
overhead on the number of servers is not fundamental for coded computation of
nonlinear functions.
",1,0,0
Fixing the Teacher-Student Knowledge Discrepancy in Distillation,"  Training a small student network with the guidance of a larger teacher
network is an effective way to promote the performance of the student. Despite
the different types the guided knowledge used to distill is always kept
unchanged for different teacher and student pairs in previous knowledge
distillation methods. However we find that teacher and student models with
different networks or trained from different initialization could have distinct
feature representations among different channels. (e.g. the high activated
channel for different categories). We name this incongruous representation of
channels as teacher-student knowledge discrepancy in the distillation process.
Ignoring the knowledge discrepancy problem of teacher and student models will
make the learning of student from teacher more difficult. To solve this
problem in this paper we propose a novel student-dependent distillation
method knowledge consistent distillation which makes teacher's knowledge more
consistent with the student and provides the best suitable knowledge to
different student networks for distillation. Extensive experiments on different
datasets (CIFAR100 ImageNet COCO) and tasks (image classification object
detection) reveal the widely existing knowledge discrepancy problem between
teachers and students and demonstrate the effectiveness of our proposed method.
Our method is very flexible that can be easily combined with other
state-of-the-art approaches.
",0,0,1
FaceOff: Anonymizing Videos in the Operating Rooms,"  Video capture in the surgical operating room (OR) is increasingly possible
and has potential for use with computer assisted interventions (CAI) surgical
data science and within smart OR integration. Captured video innately carries
sensitive information that should not be completely visible in order to
preserve the patient's and the clinical teams' identities. When surgical video
streams are stored on a server the videos must be anonymized prior to storage
if taken outside of the hospital. In this article we describe how a deep
learning model Faster R-CNN can be used for this purpose and help to
anonymize video data captured in the OR. The model detects and blurs faces in
an effort to preserve anonymity. After testing an existing face detection
trained model a new dataset tailored to the surgical environment with faces
obstructed by surgical masks and caps was collected for fine-tuning to achieve
higher face-detection rates in the OR. We also propose a temporal
regularisation kernel to improve recall rates. The fine-tuned model achieves a
face detection recall of 88.05 % and 93.45 % before and after applying
temporal-smoothing respectively.
",0,0,1
Radiomic Synthesis Using Deep Convolutional Neural Networks,"  Radiomics is a rapidly growing field that deals with modeling the textural
information present in the different tissues of interest for clinical decision
support. However the process of generating radiomic images is computationally
very expensive and could take substantial time per radiological image for
certain higher order features such as gray-level co-occurrence matrix(GLCM)
even with high-end GPUs. To that end we developed RadSynth a deep
convolutional neural network(CNN) model to efficiently generate radiomic
images. RadSynth was tested on a breast cancer patient cohort of twenty-four
patients(ten benign ten malignant and four normal) for computation of GLCM
entropy images from post-contrast DCE-MRI. RadSynth produced excellent
synthetic entropy images compared to traditional GLCM entropy images. The
average percentage difference and correlation between the two techniques were
0.07 $pm$ 0.06 and 0.97 respectively. In conclusion RadSynth presents a new
powerful tool for fast computation and visualization of the textural
information present in the radiological images.
",0,0,1
"Differentiating Objects by Motion: Joint Detection and Tracking of Small
  Flying Objects","  While generic object detection has achieved large improvements with rich
feature hierarchies from deep nets detecting small objects with poor visual
cues remains challenging. Motion cues from multiple frames may be more
informative for detecting such hard-to-distinguish objects in each frame.
However how to encode discriminative motion patterns such as deformations and
pose changes that characterize objects has remained an open question. To learn
them and thereby realize small object detection we present a neural model
called the Recurrent Correlational Network where detection and tracking are
jointly performed over a multi-frame representation learned through a single
trainable and end-to-end network. A convolutional long short-term memory
network is utilized for learning informative appearance change for detection
while learned representation is shared in tracking for enhancing its
performance. In experiments with datasets containing images of scenes with
small flying objects such as birds and unmanned aerial vehicles the proposed
method yielded consistent improvements in detection performance over deep
single-frame detectors and existing motion-based detectors. Furthermore our
network performs as well as state-of-the-art generic object trackers when it
was evaluated as a tracker on the bird dataset.
",0,0,1
RoboNet: Large-Scale Multi-Robot Learning,"  Robot learning has emerged as a promising tool for taming the complexity and
diversity of the real world. Methods based on high-capacity models such as
deep networks hold the promise of providing effective generalization to a wide
range of open-world environments. However these same methods typically require
large amounts of diverse training data to generalize effectively. In contrast
most robotic learning experiments are small-scale single-domain and
single-robot. This leads to a frequent tension in robotic learning: how can we
learn generalizable robotic controllers without having to collect impractically
large amounts of data for each separate experiment? In this paper we propose
RoboNet an open database for sharing robotic experience which provides an
initial pool of 15 million video frames from 7 different robot platforms and
study how it can be used to learn generalizable models for vision-based robotic
manipulation. We combine the dataset with two different learning algorithms:
visual foresight which uses forward video prediction models and supervised
inverse models. Our experiments test the learned algorithms' ability to work
across new objects new tasks new scenes new camera viewpoints new grippers
or even entirely new robots. In our final experiment we find that by
pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka
robot we can exceed the performance of a robot-specific training approach that
uses 4x-20x more data. For videos and data see the project webpage:
https://www.robonet.wiki/
",0,0,1
Coding for Network-Coded Slotted ALOHA,"  Slotted ALOHA can benefit from physical-layer network coding (PNC) by
decoding one or multiple linear combinations of the packets simultaneously
transmitted in a timeslot forming a system of linear equations. Different
systems of linear equations are recovered in different timeslots. A message
decoder then recovers the original packets of all the users by jointly solving
multiple systems of linear equations obtained over different timeslots. We
propose the batched BP decoding algorithm that combines belief propagation (BP)
and local Gaussian elimination. Compared with pure Gaussian elimination
decoding our algorithm reduces the decoding complexity from cubic to linear
function of the number of users. Compared with the ordinary BP decoding
algorithm for low-density generator-matrix codes our algorithm has better
performance and the same order of computational complexity. We analyze the
performance of the batched BP decoding algorithm by generalizing the tree-based
approach and provide an approach to optimize the system performance.
",1,0,0
"Bridging Maximum Likelihood and Adversarial Learning via
  $alpha$-Divergence","  Maximum likelihood (ML) and adversarial learning are two popular approaches
for training generative models and from many perspectives these techniques are
complementary. ML learning encourages the capture of all data modes and it is
typically characterized by stable training. However ML learning tends to
distribute probability mass diffusely over the data space $e.g.$ yielding
blurry synthetic images. Adversarial learning is well known to synthesize
highly realistic natural images despite practical challenges like mode
dropping and delicate training. We propose an $alpha$-Bridge to unify the
advantages of ML and adversarial learning enabling the smooth transfer from
one to the other via the $alpha$-divergence. We reveal that generalizations of
the $alpha$-Bridge are closely related to approaches developed recently to
regularize adversarial learning providing insights into that prior work and
further understanding of why the $alpha$-Bridge performs well in practice.
",0,0,1
A non-alternating graph hashing algorithm for large scale image search,"  In the era of big data methods for improving memory and computational
efficiency have become crucial for successful deployment of technologies.
Hashing is one of the most effective approaches to deal with computational
limitations that come with big data. One natural way for formulating this
problem is spectral hashing that directly incorporates affinity to learn binary
codes. However due to binary constraints the optimization becomes
intractable. To mitigate this challenge different relaxation approaches have
been proposed to reduce the computational load of obtaining binary codes and
still attain a good solution. The problem with all existing relaxation methods
is resorting to one or more additional auxiliary variables to attain high
quality binary codes while relaxing the problem. The existence of auxiliary
variables leads to coordinate descent approach which increases the
computational complexity. We argue that introducing these variables is
unnecessary. To this end we propose a novel relaxed formulation for spectral
hashing that adds no additional variables to the problem. Furthermore instead
of solving the problem in original space where number of variables is equal to
the data points we solve the problem in a much smaller space and retrieve the
binary codes from this solution. This trick reduces both the memory and
computational complexity at the same time. We apply two optimization
techniques namely projected gradient and optimization on manifold to obtain
the solution. Using comprehensive experiments on four public datasets we show
that the proposed efficient spectral hashing (ESH) algorithm achieves highly
competitive retrieval performance compared with state of the art at low
complexity.
",0,0,1
Making effective use of healthcare data using data-to-text technology,"  Healthcare organizations are in a continuous effort to improve health
outcomes reduce costs and enhance patient experience of care. Data is
essential to measure and help achieving these improvements in healthcare
delivery. Consequently a data influx from various clinical financial and
operational sources is now overtaking healthcare organizations and their
patients. The effective use of this data however is a major challenge.
Clearly text is an important medium to make data accessible. Financial reports
are produced to assess healthcare organizations on some key performance
indicators to steer their healthcare delivery. Similarly at a clinical level
data on patient status is conveyed by means of textual descriptions to
facilitate patient review shift handover and care transitions. Likewise
patients are informed about data on their health status and treatments via
text in the form of reports or via ehealth platforms by their doctors.
Unfortunately such text is the outcome of a highly labour-intensive process if
it is done by healthcare professionals. It is also prone to incompleteness
subjectivity and hard to scale up to different domains wider audiences and
varying communication purposes. Data-to-text is a recent breakthrough
technology in artificial intelligence which automatically generates natural
language in the form of text or speech from data. This chapter provides a
survey of data-to-text technology with a focus on how it can be deployed in a
healthcare setting. It will (1) give an up-to-date synthesis of data-to-text
approaches (2) give a categorized overview of use cases in healthcare (3)
seek to make a strong case for evaluating and implementing data-to-text in a
healthcare setting and (4) highlight recent research challenges.
",0,1,0
Safe Reinforcement Learning with Natural Language Constraints,"  While safe reinforcement learning (RL) holds great promise for many practical
applications like robotics or autonomous cars current approaches require
specifying constraints in mathematical form. Such specifications demand domain
expertise limiting the adoption of safe RL. In this paper we propose learning
to interpret natural language constraints for safe RL. To this end we first
introduce HazardWorld a new multi-task benchmark that requires an agent to
optimize reward while not violating constraints specified in free-form text. We
then develop an agent with a modular architecture that can interpret and adhere
to such textual constraints while learning new tasks. Our model consists of (1)
a constraint interpreter that encodes textual constraints into spatial and
temporal representations of forbidden states and (2) a policy network that
uses these representations to produce a policy achieving minimal constraint
violations during training. Across different domains in HazardWorld we show
that our method achieves higher rewards (up to11x) and fewer constraint
violations (by 1.8x) compared to existing approaches. However in terms of
absolute performance HazardWorld still poses significant challenges for agents
to learn efficiently motivating the need for future work.
",0,1,0
Serverless Straggler Mitigation using Local Error-Correcting Codes,"  Inexpensive cloud services such as serverless computing are often
vulnerable to straggling nodes that increase end-to-end latency for distributed
computation. We propose and implement simple yet principled approaches for
straggler mitigation in serverless systems for matrix multiplication and
evaluate them on several common applications from machine learning and
high-performance computing. The proposed schemes are inspired by
error-correcting codes and employ parallel encoding and decoding over the data
stored in the cloud using serverless workers. This creates a fully distributed
computing framework without using a master node to conduct encoding or
decoding which removes the computation communication and storage bottleneck
at the master. On the theory side we establish that our proposed scheme is
asymptotically optimal in terms of decoding time and provide a lower bound on
the number of stragglers it can tolerate with high probability. Through
extensive experiments we show that our scheme outperforms existing schemes
such as speculative execution and other coding theoretic methods by at least
25%.
",1,0,0
"Communication Computing and Caching for Mobile VR Delivery: Modeling
  and Trade-off","  Mobile virtual reality (VR) delivery is gaining increasing attention from
both industry and academia due to its ability to provide an immersive
experience. However achieving mobile VR delivery requires ultra-high
transmission rate deemed as a first killer application for 5G wireless
networks. In this paper in order to alleviate the traffic burden over wireless
networks we develop an implementation framework for mobile VR delivery by
utilizing caching and computing capabilities of mobile VR device. We then
jointly optimize the caching and computation offloading policy for minimizing
the required average transmission rate under the latency and local average
energy consumption constraints. In a symmetric scenario we obtain the optimal
joint policy and the closed-form expression of the minimum average transmission
rate. Accordingly we analyze the tradeoff among communication computing and
caching and then reveal analytically the fact that the communication overhead
can be traded by the computing and caching capabilities of mobile VR device
and also what conditions must be met for it to happen. Finally we discuss the
optimization problem in a heterogeneous scenario and propose an efficient
suboptimal algorithm with low computation complexity which is shown to achieve
good performance in the numerical results.
",1,0,0
Detect-and-Track: Efficient Pose Estimation in Videos,"  This paper addresses the problem of estimating and tracking human body
keypoints in complex multi-person video. We propose an extremely lightweight
yet highly effective approach that builds upon the latest advancements in human
detection and video understanding. Our method operates in two-stages: keypoint
estimation in frames or short clips followed by lightweight tracking to
generate keypoint predictions linked over the entire video. For frame-level
pose estimation we experiment with Mask R-CNN as well as our own proposed 3D
extension of this model which leverages temporal information over small clips
to generate more robust frame predictions. We conduct extensive ablative
experiments on the newly released multi-person video pose estimation benchmark
PoseTrack to validate various design choices of our model. Our approach
achieves an accuracy of 55.2% on the validation and 51.8% on the test set using
the Multi-Object Tracking Accuracy (MOTA) metric and achieves state of the art
performance on the ICCV 2017 PoseTrack keypoint tracking challenge.
",0,0,1
"Audiovisual speaker conversion: jointly and simultaneously transforming
  facial expression and acoustic characteristics","  An audiovisual speaker conversion method is presented for simultaneously
transforming the facial expressions and voice of a source speaker into those of
a target speaker. Transforming the facial and acoustic features together makes
it possible for the converted voice and facial expressions to be highly
correlated and for the generated target speaker to appear and sound natural. It
uses three neural networks: a conversion network that fuses and transforms the
facial and acoustic features a waveform generation network that produces the
waveform from both the converted facial and acoustic features and an image
reconstruction network that outputs an RGB facial image also based on both the
converted features. The results of experiments using an emotional audiovisual
database showed that the proposed method achieved significantly higher
naturalness compared with one that separately transformed acoustic and facial
features.
",0,1,0
CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data,"  Pre-training text representations have led to significant improvements in
many areas of natural language processing. The quality of these models benefits
greatly from the size of the pretraining corpora as long as its quality is
preserved. In this paper we describe an automatic pipeline to extract massive
high-quality monolingual datasets from Common Crawl for a variety of languages.
Our pipeline follows the data processing introduced in fastText (Mikolov et
al. 2017; Grave et al. 2018) that deduplicates documents and identifies
their language. We augment this pipeline with a filtering step to select
documents that are close to high quality corpora like Wikipedia.
",0,1,0
"Multi-message Authentication over Noisy Channel with Secure Channel
  Codes","  In this paper we investigate multi-message authentication to combat
adversaries with infinite computational capacity. An authentication framework
over a wiretap channel $(W_1W_2)$ is proposed to achieve information-theoretic
security with the same key. The proposed framework bridges the two research
areas in physical (PHY) layer security: secure transmission and message
authentication. Specifically the sender Alice first transmits message $M$ to
the receiver Bob over $(W_1W_2)$ with an error correction code; then Alice
employs a hash function (i.e. $varepsilon$-AWU$_2$ hash functions) to
generate a message tag $S$ of message $M$ using key $K$ and encodes $S$ to a
codeword $X^n$ by leveraging an existing strongly secure channel coding with
exponentially small (in code length $n$) average probability of error; finally
Alice sends $X^n$ over $(W_1W_2)$ to Bob who authenticates the received
messages. We develop a theorem regarding the requirements/conditions for the
authentication framework to be information-theoretic secure for authenticating
a polynomial number of messages in terms of $n$. Based on this theorem we
propose an authentication protocol that can guarantee the security
requirements and prove its authentication rate can approach infinity when $n$
goes to infinity. Furthermore we design and implement an efficient and
feasible authentication protocol over binary symmetric wiretap channel (BSWC)
by using emphLinear Feedback Shifting Register based (LFSR-based) hash
functions and strong secure polar code. Through extensive experiments it is
demonstrated that the proposed protocol can achieve low time cost high
authentication rate and low authentication error rate.
",1,0,0
"Robust Video Background Identification by Dominant Rigid Motion
  Estimation","  The ability to identify the static background in videos captured by a moving
camera is an important pre-requisite for many video applications (e.g. video
stabilization stitching and segmentation). Existing methods usually face
difficulties when the foreground objects occupy a larger area than the
background in the image. Many methods also cannot scale up to handle densely
sampled feature trajectories. In this paper we propose an efficient
local-to-global method to identify background based on the assumption that as
long as there is sufficient camera motion the cumulative background features
will have the largest amount of trajectories. Our motion model at the two-frame
level is based on the epipolar geometry so that there will be no
over-segmentation problem another issue that plagues the 2D motion
segmentation approach. Foreground objects erroneously labelled due to
intermittent motions are also taken care of by checking their global
consistency with the final estimated background motion. Lastly by virtue of
its efficiency our method can deal with densely sampled trajectories. It
outperforms several state-of-the-art motion segmentation methods on public
datasets both quantitatively and qualitatively.
",0,0,1
"""RAPID"" Regions-of-Interest Detection In Big Histopathological Images","  The sheer volume and size of histopathological images (e.g.10^6 MPixel)
underscores the need for faster and more accurate Regions-of-interest (ROI)
detection algorithms. In this paper we propose such an algorithm which has
four main components that help achieve greater accuracy and faster speed:
First while using coarse-to-fine topology preserving segmentation as the
baseline the proposed algorithm uses a superpixel regularity optimization
scheme for avoiding irregular and extremely small superpixels. Second the
proposed technique employs a prediction strategy to focus only on important
superpixels at finer image levels. Third the algorithm reuses the information
gained from the coarsest image level at other finer image levels. Both the
second and the third components drastically lower the complexity. Fourth the
algorithm employs a highly effective parallelization scheme using adap- tive
data partitioning which gains high speedup. Experimental results conducted on
the BSD500 [1] and 500 whole-slide histological images from the National Lung
Screening Trial (NLST)1 dataset confirm that the proposed algorithm gained 13
times speedup compared with the baseline and around 160 times compared with
SLIC [11] without losing accuracy.
",0,0,1
Anchor-Based Correction of Substitutions in Indexed Sets,"  Motivated by DNA-based data storage we investigate a system where digital
information is stored in an unordered set of several vectors over a finite
alphabet. Each vector begins with a unique index that represents its position
in the whole data set and does not contain data. This paper deals with the
design of error-correcting codes for such indexed sets in the presence of
substitution errors. We propose a construction that efficiently deals with the
challenges that arise when designing codes for unordered sets. Using a novel
mechanism called anchoring we show that it is possible to combat the ordering
loss of sequences with only a small amount of redundancy which allows to use
standard coding techniques such as tensor-product codes to correct errors
within the sequences. We finally derive upper and lower bounds on the
achievable redundancy of codes within the considered channel model and verify
that our construction yields a redundancy that is close to the best possible
achievable one. Our results surprisingly indicate that it requires less
redundancy to correct errors in the indices than in the data part of vectors.
",1,0,0
GFF: Gated Fully Fusion for Semantic Segmentation,"  Semantic segmentation generates comprehensive understanding of scenes through
densely predicting the category for each pixel. High-level features from Deep
Convolutional Neural Networks already demonstrate their effectiveness in
semantic segmentation tasks however the coarse resolution of high-level
features often leads to inferior results for small/thin objects where detailed
information is important. It is natural to consider importing low level
features to compensate for the lost detailed information in high-level
features.Unfortunately simply combining multi-level features suffers from the
semantic gap among them. In this paper we propose a new architecture named
Gated Fully Fusion (GFF) to selectively fuse features from multiple levels
using gates in a fully connected way. Specifically features at each level are
enhanced by higher-level features with stronger semantics and lower-level
features with more details and gates are used to control the propagation of
useful information which significantly reduces the noises during fusion. We
achieve the state of the art results on four challenging scene parsing datasets
including Cityscapes Pascal Context COCO-stuff and ADE20K.
",0,0,1
"Pardon the Interruption: An Analysis of Gender and Turn-Taking in US
  Supreme Court Oral Arguments","  This study presents a corpus of turn changes between speakers in U.S. Supreme
Court oral arguments. Each turn change is labeled on a spectrum of
""cooperative"" to ""competitive"" by a human annotator with legal experience in
the United States. We analyze the relationship between speech features the
nature of exchanges and the gender and legal role of the speakers. Finally we
demonstrate that the models can be used to predict the label of an exchange
with moderate success. The automatic classification of the nature of exchanges
indicates that future studies of turn-taking in oral arguments can rely on
larger unlabeled corpora.
",0,1,0
"ContextNet: Improving Convolutional Neural Networks for Automatic Speech
  Recognition with Global Context","  Convolutional neural networks (CNN) have shown promising results for
end-to-end speech recognition albeit still behind other state-of-the-art
methods in performance. In this paper we study how to bridge this gap and go
beyond with a novel CNN-RNN-transducer architecture which we call ContextNet.
ContextNet features a fully convolutional encoder that incorporates global
context information into convolution layers by adding squeeze-and-excitation
modules. In addition we propose a simple scaling method that scales the widths
of ContextNet that achieves good trade-off between computation and accuracy. We
demonstrate that on the widely used LibriSpeech benchmark ContextNet achieves
a word error rate (WER) of 2.1%/4.6% without external language model (LM)
1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy
LibriSpeech test sets. This compares to the previous best published system of
2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the
proposed ContextNet model is also verified on a much larger internal dataset.
",0,1,0
Weakly Supervised Object Localization and Detection: A Survey,"  As an emerging and challenging problem in the computer vision community
weakly supervised object localization and detection plays an important role for
developing new generation computer vision systems and has received significant
attention in the past decade. As methods have been proposed a comprehensive
survey of these topics is of great importance. In this work we review (1)
classic models (2) approaches with feature representations from off-the-shelf
deep networks (3) approaches solely based on deep learning and (4) publicly
available datasets and standard evaluation metrics that are widely used in this
field. We also discuss the key challenges in this field development history of
this field advantages/disadvantages of the methods in each category the
relationships between methods in different categories applications of the
weakly supervised object localization and detection methods and potential
future directions to further promote the development of this research field.
",0,0,1
"Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA
  Models","  Benefiting from large-scale pre-training we have witnessed significant
performance boost on the popular Visual Question Answering (VQA) task. Despite
rapid progress it remains unclear whether these state-of-the-art (SOTA) models
are robust when encountering examples in the wild. To study this we introduce
Adversarial VQA a new large-scale VQA benchmark collected iteratively via an
adversarial human-and-model-in-the-loop procedure. Through this new benchmark
we discover several interesting findings. (i) Surprisingly we find that during
dataset collection non-expert annotators can easily attack SOTA VQA models
successfully. (ii) Both large-scale pre-trained models and adversarial training
methods achieve far worse performance on the new benchmark than over standard
VQA v2 dataset revealing the fragility of these models while demonstrating the
effectiveness of our adversarial dataset. (iii) When used for data
augmentation our dataset can effectively boost model performance on other
robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light
on robustness study in the community and serve as a valuable benchmark for
future work.
",0,0,1
New Counting Codes for Distributed Video Coding,"  This paper introduces a new counting code. Its design was motivated by
distributed video coding where for decoding error correction methods are
applied to improve predictions. Those error corrections sometimes fail which
results in decoded values worse than the initial prediction. Our code exploits
the fact that bit errors are relatively unlikely events: more than a few bit
errors in a decoded pixel value are rare. With a carefully designed counting
code combined with a prediction those bit errors can be corrected and sometimes
the original pixel value recovered. The error correction improves
significantly. Our new code not only maximizes the Hamming distance between
adjacent (or ""near 1"") codewords but also between nearby (for example ""near 2"")
codewords. This is why our code is significantly different from the well-known
maximal counting sequences which have maximal average Hamming distance.
Fortunately the new counting code can be derived from Gray Codes for every
code word length (i.e. bit depth).
",1,0,0
"Combination of Unified Embedding Model and Observed Features for
  Knowledge Graph Completion","  Knowledge graphs are useful for many artificial intelligence tasks but often
have missing data. Hence a method for completing knowledge graphs is required.
Existing approaches include embedding models the Path Ranking Algorithm and
rule evaluation models. However these approaches have limitations. For
example all the information is mixed and difficult to interpret in embedding
models and traditional rule evaluation models are basically slow. In this
paper we provide an integrated view of various approaches and combine them to
compensate for their limitations. We first unify state-of-the-art embedding
models such as ComplEx and TorusE reinterpreting them as a variant of
translation-based models. Then we show that these models utilize paths for
link prediction and propose a method for evaluating rules based on this idea.
Finally we combine an embedding model and observed feature models to predict
missing triples. This is possible because all of these models utilize paths. We
also conduct experiments including link prediction tasks with standard
datasets to evaluate our method and framework. The experiments show that our
method can evaluate rules faster than traditional methods and that our
framework outperforms state-of-the-art models in terms of link prediction.
",0,1,0
"Balancing forward and feedback error correction for erasure channels
  with unreliable feedback","  The traditional information theoretic approach to studying feedback is to
consider ideal instantaneous high-rate feedback of the channel outputs to the
encoder. This was acceptable in classical work because the results were
negative: Shannon pointed out that even perfect feedback often does not improve
capacity and in the context of symmetric DMCs Dobrushin showed that it does
not improve the fixed block-coding error exponents in the interesting high rate
regime. However it has recently been shown that perfect feedback does allow
great improvements in the asymptotic tradeoff between end-to-end delay and
probability of error even for symmetric channels at high rate. Since gains are
claimed with ideal instantaneous feedback it is natural to wonder whether
these improvements remain if the feedback is unreliable or otherwise limited.
  Here packet-erasure channels are considered on both the forward and feedback
links. First the feedback channel is considered as a given and a strategy is
given to balance forward and feedback error correction in the suitable
information-theoretic limit of long end-to-end delays. At high enough rates
perfect-feedback performance is asymptotically attainable despite having only
unreliable feedback! Second the results are interpreted in the zero- sum case
of ""half-duplex"" nodes where the allocation of bandwidth or time to the
feedback channel comes at the direct expense of the forward channel. It turns
out that even here feedback is worthwhile since dramatically lower asymptotic
delays are possible by appropriately balancing forward and feedback error
correction.
  The results easily generalize to channels with strictly positive
zero-undeclared-error capacities.
",1,0,0
Team Applied Robotics: A closer look at our robotic picking system,"  This paper describes the vision based robotic picking system that was
developed by our team Team Applied Robotics for the Amazon Picking Challenge
2016. This competition challenged teams to develop a robotic system that is
able to pick a large variety of products from a shelve or a tote. We discuss
the design considerations and our strategy the high resolution 3D vision
system the use of a combination of texture and shape-based object detection
algorithms the robot path planning and object manipulators that were
developed.
",0,0,1
SweepNet: Wide-baseline Omnidirectional Depth Estimation,"  Omnidirectional depth sensing has its advantage over the conventional stereo
systems since it enables us to recognize the objects of interest in all
directions without any blind regions. In this paper we propose a novel
wide-baseline omnidirectional stereo algorithm which computes the dense depth
estimate from the fisheye images using a deep convolutional neural network. The
capture system consists of multiple cameras mounted on a wide-baseline rig with
ultrawide field of view (FOV) lenses and we present the calibration algorithm
for the extrinsic parameters based on the bundle adjustment. Instead of
estimating depth maps from multiple sets of rectified images and stitching
them our approach directly generates one dense omnidirectional depth map with
full 360-degree coverage at the rig global coordinate system. To this end the
proposed neural network is designed to output the cost volume from the warped
images in the sphere sweeping method and the final depth map is estimated by
taking the minimum cost indices of the aggregated cost volume by SGM. For
training the deep neural network and testing the entire system realistic
synthetic urban datasets are rendered using Blender. The experiments using the
synthetic and real-world datasets show that our algorithm outperforms the
conventional depth estimation methods and generate highly accurate depth maps.
",0,0,1
"Transfer Learning with Ensembles of Deep Neural Networks for Skin Cancer
  Detection in Imbalanced Data Sets","  Several machine learning techniques for accurate detection of skin cancer
from medical images have been reported. Many of these techniques are based on
pre-trained convolutional neural networks (CNNs) which enable training the
models based on limited amounts of training data. However the classification
accuracy of these models still tends to be severely limited by the scarcity of
representative images from malignant tumours. We propose a novel ensemble-based
CNN architecture where multiple CNN models some of which are pre-trained and
some are trained only on the data at hand along with auxiliary data in the
form of metadata associated with the input images are combined using a
meta-learner. The proposed approach improves the model's ability to handle
limited and imbalanced data. We demonstrate the benefits of the proposed
technique using a dataset with 33126 dermoscopic images from 2056 patients. We
evaluate the performance of the proposed technique in terms of the F1-measure
area under the ROC curve (AUC-ROC) and area under the PR-curve (AUC-PR) and
compare it with that of seven different benchmark methods including two recent
CNN-based techniques. The proposed technique compares favourably in terms of
all the evaluation metrics.
",0,0,1
"Select Good Regions for Deblurring based on Convolutional Neural
  Networks","  The goal of blind image deblurring is to recover sharp image from one input
blurred image with an unknown blur kernel. Most of image deblurring approaches
focus on developing image priors however there is not enough attention to the
influence of image details and structures on the blur kernel estimation. What
is the useful image structure and how to choose a good deblurring region? In
this work we propose a deep neural network model method for selecting good
regions to estimate blur kernel. First we construct image patches with labels
and train a deep neural networks then the learned model is applied to
determine which region of the image is most suitable to deblur. Experimental
results illustrate that the proposed approach is effective and could be able
to select good regions for image deblurring.
",0,0,1
"Fundamental Structure of Optimal Cache Placement for Coded Caching with
  Nonuniform Demands","  This paper studies the caching system of multiple cache-enabled users with
random demands. Under nonuniform file popularity we thoroughly characterize
the optimal uncoded cache placement structure for the coded caching scheme
(CCS). Formulating the cache placement as an optimization problem to minimize
the average delivery rate we identify the file group structure in the optimal
solution. We show that regardless of the file popularity distribution there
are emphat most three file groups in the optimal cache placement where
files within a group have the same cache placement. We further characterize
the complete structure of the optimal cache placement and obtain the
closed-form solution in each of the three file group structures. A simple
algorithm is developed to obtain the final optimal cache placement by comparing
a set of candidate closed-form solutions computed in parallel. We provide
insight into the file groups formed by the optimal cache placement. The optimal
placement solution also indicates that coding between file groups may be
explored during delivery in contrast to the existing suboptimal file grouping
schemes. Using the file group structure in the optimal cache placement for the
CCS we propose a new information-theoretic converse bound for coded caching
that is tighter than the existing best one. Moreover we characterize the file
subpacketization in the CCS with the optimal cache placement solution and show
that the maximum subpacketization level in the worst case scales as
$mathcalO(2^K/sqrtK)$ for $K$ users.
",1,0,0
"Invariants of multidimensional time series based on their
  iterated-integral signature","  We introduce a novel class of features for multidimensional time series that
are invariant with respect to transformations of the ambient space. The general
linear group the group of rotations and the group of permutations of the axes
are considered. The starting point for their construction is Chen's
iterated-integral signature.
",0,0,1
Uniqueness of Nonnegative Tensor Approximations,"  We show that for a nonnegative tensor a best nonnegative rank-r
approximation is almost always unique its best rank-one approximation may
always be chosen to be a best nonnegative rank-one approximation and that the
set of nonnegative tensors with non-unique best rank-one approximations form an
algebraic hypersurface. We show that the last part holds true more generally
for real tensors and thereby determine a polynomial equation so that a real or
nonnegative tensor which does not satisfy this equation is guaranteed to have a
unique best rank-one approximation. We also establish an analogue for real or
nonnegative symmetric tensors. In addition we prove a singular vector variant
of the Perron--Frobenius Theorem for positive tensors and apply it to show that
a best nonnegative rank-r approximation of a positive tensor can never be
obtained by deflation. As an aside we verify that the Euclidean distance (ED)
discriminants of the Segre variety and the Veronese variety are hypersurfaces
and give defining equations of these ED discriminants.
",1,0,0
Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On,"  We present a learning-based approach for virtual try-on applications based on
a fully convolutional graph neural network. In contrast to existing data-driven
models which are trained for a specific garment or mesh topology our fully
convolutional model can cope with a large family of garments represented as
parametric predefined 2D panels with arbitrary mesh topology including long
dresses shirts and tight tops. Under the hood our novel geometric deep
learning approach learns to drape 3D garments by decoupling the three different
sources of deformations that condition the fit of clothing: garment type
target body shape and material. Specifically we first learn a regressor that
predicts the 3D drape of the input parametric garment when worn by a mean body
shape. Then after a mesh topology optimization step where we generate a
sufficient level of detail for the input garment type we further deform the
mesh to reproduce deformations caused by the target body shape. Finally we
predict fine-scale details such as wrinkles that depend mostly on the garment
material. We qualitatively and quantitatively demonstrate that our fully
convolutional approach outperforms existing methods in terms of generalization
capabilities and memory requirements and therefore it opens the door to more
general learning-based models for virtual try-on applications.
",0,0,1
"Randomized Kaczmarz Algorithm for Inconsistent Linear Systems: An Exact
  MSE Analysis","  We provide a complete characterization of the randomized Kaczmarz algorithm
(RKA) for inconsistent linear systems. The Kaczmarz algorithm known in some
fields as the algebraic reconstruction technique is a classical method for
solving large-scale overdetermined linear systems through a sequence of
projection operators; the randomized Kaczmarz algorithm is a recent proposal by
Strohmer and Vershynin to randomize the sequence of projections in order to
guarantee exponential convergence (in mean square) to the solutions. A flurry
of work followed this development with renewed interest in the algorithm its
extensions and various bounds on their performance. Earlier we studied the
special case of consistent linear systems and provided an exact formula for the
mean squared error (MSE) in the value reconstructed by RKA as well as a simple
way to compute the exact decay rate of the error. In this work we consider the
case of inconsistent linear systems which is a more relevant scenario for most
applications. First by using a ""lifting trick"" we derive an exact formula for
the MSE given a fixed noise vector added to the measurements. Then we show how
to average over the noise when it is drawn from a distribution with known first
and second-order statistics. Finally we demonstrate the accuracy of our exact
MSE formulas through numerical simulations which also illustrate that previous
upper bounds in the literature may be several orders of magnitude too high.
",1,0,0
"A Central Limit Theorem for the SNR at the Wiener Filter Output for
  Large Dimensional Signals","  Consider the quadratic form $beta = bf y^* (bf YY^* + rho bf
I)^-1 bf y$ where $rho$ is a positive number where $bf y$ is a
random vector and $bf Y$ is a $N times K$ random matrix both having
independent elements with different variances and where $bf y$ and $bf
Y$ are independent. Such quadratic forms represent the Signal to Noise Ratio
at the output of the linear Wiener receiver for multi dimensional signals
frequently encountered in wireless communications and in array processing.
Using well known results of Random Matrix Theory the quadratic form $beta$
can be approximated with a known deterministic real number $barbeta_K$ in the
asymptotic regime where $Ktoinfty$ and $K/N to alpha > 0$. This paper
addresses the problem of convergence of $beta$. More specifically it is shown
here that $sqrtK(beta - barbeta_K)$ behaves for large $K$ like a Gaussian
random variable which variance is provided.
",1,0,0
Cooperation for interference management: A GDoF perspective,"  The impact of cooperation on interference management is investigated by
studying an elemental wireless network the so called symmetric interference
relay channel (IRC) from a generalized degrees of freedom (GDoF) perspective.
This is motivated by the fact that the deployment of relays is considered as a
remedy to overcome the bottleneck of current systems in terms of achievable
rates. The focus of this work is on the regime in which the interference link
is weaker than the source-relay link in the IRC. Our approach towards studying
the GDoF goes through the capacity analysis of the linear deterministic IRC
(LD-IRC). New upper bounds on the sum-capacity of the LD-IRC based on
genie-aided approaches are established. These upper bounds together with some
existing upper bounds are achieved by using four novel transmission schemes.
Extending the upper bounds and the transmission schemes to the Gaussian case
the GDoF of the Gaussian IRC is characterized for the aforementioned regime.
This completes the GDoF results available in the literature for the symmetric
GDoF. It is shown that in the strong interference regime in contrast to the
IC the GDoF is not a monotonically increasing function of the interference
level.
",1,0,0
"On the Universal Approximability and Complexity Bounds of Quantized ReLU
  Neural Networks","  Compression is a key step to deploy large neural networks on
resource-constrained platforms. As a popular compression technique
quantization constrains the number of distinct weight values and thus reducing
the number of bits required to represent and store each weight. In this paper
we study the representation power of quantized neural networks. First we prove
the universal approximability of quantized ReLU networks on a wide class of
functions. Then we provide upper bounds on the number of weights and the memory
size for a given approximation error bound and the bit-width of weights for
function-independent and function-dependent structures. Our results reveal
that to attain an approximation error bound of $epsilon$ the number of
weights needed by a quantized network is no more than
$mathcalOleft(log^5(1/epsilon)right)$ times that of an unquantized
network. This overhead is of much lower order than the lower bound of the
number of weights needed for the error bound supporting the empirical success
of various quantization techniques. To the best of our knowledge this is the
first in-depth study on the complexity bounds of quantized neural networks.
",0,0,1
Multi-object Monocular SLAM for Dynamic Environments,"  In this paper we tackle the problem of multibody SLAM from a monocular
camera. The term multibody implies that we track the motion of the camera as
well as that of other dynamic participants in the scene. The quintessential
challenge in dynamic scenes is unobservability: it is not possible to
unambiguously triangulate a moving object from a moving monocular camera.
Existing approaches solve restricted variants of the problem but the solutions
suffer relative scale ambiguity (i.e. a family of infinitely many solutions
exist for each pair of motions in the scene). We solve this rather intractable
problem by leveraging single-view metrology advances in deep learning and
category-level shape estimation. We propose a multi pose-graph optimization
formulation to resolve the relative and absolute scale factor ambiguities
involved. This optimization helps us reduce the average error in trajectories
of multiple bodies over real-world datasets such as KITTI. To the best of our
knowledge our method is the first practical monocular multi-body SLAM system
to perform dynamic multi-object and ego localization in a unified framework in
metric scale.
",0,0,1
Building a Lemmatizer and a Spell-checker for Sorani Kurdish,"  The present paper aims at presenting a lemmatization and a word-level error
correction system for Sorani Kurdish. We propose a hybrid approach based on the
morphological rules and a n-gram language model. We have called our
lemmatization and error correction systems Peyv and R^en^us respectively
which are the first tools presented for Sorani Kurdish to the best of our
knowledge. The Peyv lemmatizer has shown 86.7% accuracy. As for R^en^us
using a lexicon we have obtained 96.4% accuracy while without a lexicon the
correction system has 87% accuracy. As two fundamental text processing tools
these tools can pave the way for further researches on more natural language
processing applications for Sorani Kurdish.
",0,1,0
On robust width property for Lasso and Dantzig selector,"  Recently Cahill and Mixon completely characterized the sensing operators in
many compressed sensing instances with a robust width property. The proposed
property allows uniformly stable and robust reconstruction of certain solutions
from an underdetermined linear system via convex optimization. However their
theory does not cover the Lasso and Dantzig selector models both of which are
popular alternatives in the statistics community. In this letter we show that
the robust width property can be perfectly applied to these two models as well.
Our results solve an open problem left by Cahill and Mixon.
",1,0,0
Fountain Codes with Nonuniform Selection Distributions through Feedback,"  One key requirement for fountain (rateless) coding schemes is to achieve a
high intermediate symbol recovery rate. Recent coding schemes have incorporated
the use of a feedback channel to improve intermediate performance of
traditional rateless codes; however these codes with feedback are designed
based on uniformly at random selection of input symbols. In this paper on the
other hand we develop feedback-based fountain codes with dynamically-adjusted
nonuniform symbol selection distributions and show that this characteristic
can enhance the intermediate decoding rate. We provide an analysis of our
codes including bounds on computational complexity and failure probability for
a maximum likelihood decoder; the latter are tighter than bounds known for
classical rateless codes. Through numerical simulations we also show that
feedback information paired with a nonuniform selection distribution can highly
improve the symbol recovery rate and that the amount of feedback sent can be
tuned to the specific transmission properties of a given feedback channel.
",1,0,0
"Joint Millimeter Wave and Microwave Wave Resource Allocation Design for
  Dual-Mode Base Stations","  In this paper we consider the design of joint resource blocks (RBs) and
power allocation for dual-mode base stations operating over millimeter wave
(mmW) band and microwave ($mu$W) band. The resource allocation design aims to
minimize the system energy consumption while taking into account the channel
state information maximum delay load and different types of user
applications (UAs). To facilitate the design we first propose a group-based
algorithm to assign UAs to multiple groups. Within each group low-power UAs
which often appear in short distance and experience less obstacles are
inclined to be served over mmW band. The allocation problem over mmW band can
be solved by a greedy algorithm. Over $mu$W band we propose an
estimation-optimal-descent algorithm. The rate of each UA at all RBs is
estimated to initialize the allocation. Then we keep altering RB's ownership
until any altering makes power increases. Simulation results show that our
proposed algorithm offers an excellent tradeoff between low energy consumption
and fair transmission.
",1,0,0
Repairable Threshold Secret Sharing Schemes,"  In this paper we propose a class of threshold secret sharing schemes with
repairing function between shares without the help of the dealer that we
called repairable threshold secret sharing schemes. Specifically if a share
fails such as broken or lost it will be repaired just by some other shares. A
construction of such repairable threshold secret sharing schemes is designed by
applying linearized polynomials and regenerating codes in distributed storage
systems. In addition a new repairing rate is introduced to characterize the
performance and efficiency of the repairing function. Then an achievable upper
bound on the repairing rate is derived which implies the optimality of the
repair and describes the security between different shares. Under this
optimality of the repair we further discuss traditional information rate and
also indicate its optimality that can describe the efficiency of secret
sharing schemes in the aspect of storage. Finally by applying the minimum
bandwidth regenerating (MBR) codes our construction designs repairable
threshold secret sharing schemes achieving both optimal repairing and
information rates simultaneously.
",1,0,0
Towards Fully Interpretable Deep Neural Networks: Are We There Yet?,"  Despite the remarkable performance Deep Neural Networks (DNNs) behave as
black-boxes hindering user trust in Artificial Intelligence (AI) systems.
Research on opening black-box DNN can be broadly categorized into post-hoc
methods and inherently interpretable DNNs. While many surveys have been
conducted on post-hoc interpretation methods little effort is devoted to
inherently interpretable DNNs. This paper provides a review of existing methods
to develop DNNs with intrinsic interpretability with a focus on Convolutional
Neural Networks (CNNs). The aim is to understand the current progress towards
fully interpretable DNNs that can cater to different interpretation
requirements. Finally we identify gaps in current work and suggest potential
research directions.
",0,0,1
"Keep it Consistent: Topic-Aware Storytelling from an Image Stream via
  Iterative Multi-agent Communication","  Visual storytelling aims to generate a narrative paragraph from a sequence of
images automatically. Existing approaches construct text description
independently for each image and roughly concatenate them as a story which
leads to the problem of generating semantically incoherent content. In this
paper we propose a new way for visual storytelling by introducing a topic
description task to detect the global semantic context of an image stream. A
story is then constructed with the guidance of the topic description. In order
to combine the two generation tasks we propose a multi-agent communication
framework that regards the topic description generator and the story generator
as two agents and learn them simultaneously via iterative updating mechanism.
We validate our approach on VIST dataset where quantitative results
ablations and human evaluation demonstrate our method's good ability in
generating stories with higher quality compared to state-of-the-art methods.
",0,1,0
Single Model Ensemble using Pseudo-Tags and Distinct Vectors,"  Model ensemble techniques often increase task performance in neural networks;
however they require increased time memory and management effort. In this
study we propose a novel method that replicates the effects of a model
ensemble with a single model. Our approach creates K-virtual models within a
single parameter space using K-distinct pseudo-tags and K-distinct vectors.
Experiments on text classification and sequence labeling tasks on several
datasets demonstrate that our method emulates or outperforms a traditional
model ensemble with 1/K-times fewer parameters.
",0,1,0
"Speeding Up Distributed Gradient Descent by Utilizing Non-persistent
  Stragglers","  Distributed gradient descent (DGD) is an efficient way of implementing
gradient descent (GD) especially for large data sets by dividing the
computation tasks into smaller subtasks and assigning to different computing
servers (CSs) to be executed in parallel. In standard parallel execution
per-iteration waiting time is limited by the execution time of the straggling
servers. Coded DGD techniques have been introduced recently which can tolerate
straggling servers via assigning redundant computation tasks to the CSs. In
most of the existing DGD schemes either with coded computation or coded
communication the non-straggling CSs transmit one message per iteration once
they complete all their assigned computation tasks. However although the
straggling servers cannot complete all their assigned tasks they are often
able to complete a certain portion of them. In this paper we allow multiple
transmissions from each CS at each iteration in order to make sure a maximum
number of completed computations can be reported to the aggregating server
(AS) including the straggling servers. We numerically show that the average
completion time per iteration can be reduced significantly by slightly
increasing the communication load per server.
",1,0,0
Contextual Attention for Hand Detection in the Wild,"  We present Hand-CNN a novel convolutional network architecture for detecting
hand masks and predicting hand orientations in unconstrained images. Hand-CNN
extends MaskRCNN with a novel attention mechanism to incorporate contextual
cues in the detection process. This attention mechanism can be implemented as
an efficient network module that captures non-local dependencies between
features. This network module can be inserted at different stages of an object
detection network and the entire detector can be trained end-to-end.
  We also introduce a large-scale annotated hand dataset containing hands in
unconstrained images for training and evaluation. We show that Hand-CNN
outperforms existing methods on several datasets including our hand detection
benchmark and the publicly available PASCAL VOC human layout challenge. We also
conduct ablation studies on hand detection to show the effectiveness of the
proposed contextual attention module.
",0,0,1
Learning to mirror speaking styles incrementally,"  Mirroring is the behavior in which one person subconsciously imitates the
gesture speech pattern or attitude of another. In conversations mirroring
often signals the speakers enjoyment and engagement in their communication. In
chatbots methods have been proposed to add personas to the chatbots and to
train them to speak or to shift their dialogue style to that of the personas.
However they often require a large dataset consisting of dialogues of the
target personalities to train. In this work we explore a method that can learn
to mirror the speaking styles of a person incrementally. Our method extracts
ngrams that capture a persons speaking styles and uses the ngrams to create
patterns for transforming sentences to the persons speaking styles. Our
experiments show that our method is able to capture patterns of speaking style
that can be used to transform regular sentences into sentences with the target
style.
",0,1,0
Empirical Evaluation of Four Tensor Decomposition Algorithms,"  Higher-order tensor decompositions are analogous to the familiar Singular
Value Decomposition (SVD) but they transcend the limitations of matrices
(second-order tensors). SVD is a powerful tool that has achieved impressive
results in information retrieval collaborative filtering computational
linguistics computational vision and other fields. However SVD is limited to
two-dimensional arrays of data (two modes) and many potential applications
have three or more modes which require higher-order tensor decompositions.
This paper evaluates four algorithms for higher-order tensor decomposition:
Higher-Order Singular Value Decomposition (HO-SVD) Higher-Order Orthogonal
Iteration (HOOI) Slice Projection (SP) and Multislice Projection (MP). We
measure the time (elapsed run time) space (RAM and disk space requirements)
and fit (tensor reconstruction accuracy) of the four algorithms under a
variety of conditions. We find that standard implementations of HO-SVD and HOOI
do not scale up to larger tensors due to increasing RAM requirements. We
recommend HOOI for tensors that are small enough for the available RAM and MP
for larger tensors.
",0,1,0
Robust Benchmarking for Machine Learning of Clinical Entity Extraction,"  Clinical studies often require understanding elements of a patient's
narrative that exist only in free text clinical notes. To transform notes into
structured data for downstream use these elements are commonly extracted and
normalized to medical vocabularies. In this work we audit the performance of
and indicate areas of improvement for state-of-the-art systems. We find that
high task accuracies for clinical entity normalization systems on the 2019 n2c2
Shared Task are misleading and underlying performance is still brittle.
Normalization accuracy is high for common concepts (95.3%) but much lower for
concepts unseen in training data (69.3%). We demonstrate that current
approaches are hindered in part by inconsistencies in medical vocabularies
limitations of existing labeling schemas and narrow evaluation techniques. We
reformulate the annotation framework for clinical entity extraction to factor
in these issues to allow for robust end-to-end system benchmarking. We evaluate
concordance of annotations from our new framework between two annotators and
achieve a Jaccard similarity of 0.73 for entity recognition and an agreement of
0.83 for entity normalization. We propose a path forward to address the
demonstrated need for the creation of a reference standard to spur method
development in entity recognition and normalization.
",0,1,0
"Client Adaptation improves Federated Learning with Simulated Non-IID
  Clients","  We present a federated learning approach for learning a client adaptable
robust model when data is non-identically and non-independently distributed
(non-IID) across clients. By simulating heterogeneous clients we show that
adding learned client-specific conditioning improves model performance and the
approach is shown to work on balanced and imbalanced data set from both audio
and image domains. The client adaptation is implemented by a conditional gated
activation unit and is particularly beneficial when there are large differences
between the data distribution for each client a common scenario in federated
learning.
",0,0,1
Video Analytics on IoT devices,"  Deep Learning (DL) combined with advanced model optimization methods such as
RC-NN and Edge2Train has enabled offline execution of large networks on the IoT
devices. In this paper we compare the modern Deep Learning (DL) based video
analytics approaches with the standard Computer Vision (CV) based approaches
and finally discuss the best-suited approach for video analytics on IoT
devices.
",0,0,1
"GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based
  on Transformer Networks","  A key problem in multi-task learning (MTL) research is how to select
high-quality auxiliary tasks automatically. This paper presents GradTS an
automatic auxiliary task selection method based on gradient calculation in
Transformer-based models. Compared to AUTOSEM a strong baseline method GradTS
improves the performance of MT-DNN with a bert-base-cased backend model from
0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE
benchmarks. GradTS is also time-saving since (1) its gradient calculations are
based on single-task experiments and (2) the gradients are re-used without
additional experiments when the candidate task set changes. On the 8 GLUE
classification tasks for example GradTS costs on average 21.32% less time
than AUTOSEM with comparable GPU consumption. Further we show the robustness
of GradTS across various task settings and model selections e.g. mixed
objectives among candidate tasks. The efficiency and efficacy of GradTS in
these case studies illustrate its general applicability in MTL research without
requiring manual task filtering or costly parameter tuning.
",0,1,0
Data Discovery and Anomaly Detection Using Atypicality: Theory,"  A central question in the era of 'big data' is what to do with the enormous
amount of information. One possibility is to characterize it through
statistics e.g. averages or classify it using machine learning in order to
understand the general structure of the overall data. The perspective in this
paper is the opposite namely that most of the value in the information in some
applications is in the parts that deviate from the average that are unusual
atypical. We define what we mean by 'atypical' in an axiomatic way as data that
can be encoded with fewer bits in itself rather than using the code for the
typical data. We show that this definition has good theoretical properties. We
then develop an implementation based on universal source coding and apply this
to a number of real world data sets.
",1,0,0
"Combining Optimal Control and Learning for Visual Navigation in Novel
  Environments","  Model-based control is a popular paradigm for robot navigation because it can
leverage a known dynamics model to efficiently plan robust robot trajectories.
However it is challenging to use model-based methods in settings where the
environment is a priori unknown and can only be observed partially through
on-board sensors on the robot. In this work we address this short-coming by
coupling model-based control with learning-based perception. The learning-based
perception module produces a series of waypoints that guide the robot to the
goal via a collision-free path. These waypoints are used by a model-based
planner to generate a smooth and dynamically feasible trajectory that is
executed on the physical system using feedback control. Our experiments in
simulated real-world cluttered environments and on an actual ground vehicle
demonstrate that the proposed approach can reach goal locations more reliably
and efficiently in novel environments as compared to purely geometric
mapping-based or end-to-end learning-based alternatives. Our approach does not
rely on detailed explicit 3D maps of the environment works well with low frame
rates and generalizes well from simulation to the real world. Videos
describing our approach and experiments are available on the project website.
",0,0,1
"Rethinking Recurrent Neural Networks and Other Improvements for Image
  Classification","  Over the long history of machine learning which dates back several decades
recurrent neural networks (RNNs) have been used mainly for sequential data and
time series and generally with 1D information. Even in some rare studies on 2D
images these networks are used merely to learn and generate data sequentially
rather than for image recognition tasks. In this study we propose integrating
an RNN as an additional layer when designing image recognition models. We also
develop end-to-end multimodel ensembles that produce expert predictions using
several models. In addition we extend the training strategy so that our model
performs comparably to leading models and can even match the state-of-the-art
models on several challenging datasets (e.g. SVHN (0.99) Cifar-100 (0.9027)
and Cifar-10 (0.9852)). Moreover our model sets a new record on the Surrey
dataset (0.949). The source code of the methods provided in this article is
available at https://github.com/leonlha/e2e-3m and http://nguyenhuuphong.me.
",0,0,1
Arabic Opinion Mining Using a Hybrid Recommender System Approach,"  Recommender systems nowadays are playing an important role in the delivery of
services and information to users. Sentiment analysis (also known as opinion
mining) is the process of determining the attitude of textual opinions whether
they are positive negative or neutral. Data sparsity is representing a big
issue for recommender systems because of the insufficiency of user rating or
absence of data about users or items. This research proposed a hybrid approach
combining sentiment analysis and recommender systems to tackle the problem of
data sparsity problems by predicting the rating of products from users reviews
using text mining and NLP techniques. This research focuses especially on
Arabic reviews where the model is evaluated using Opinion Corpus for Arabic
(OCA) dataset. Our system was efficient and it showed a good accuracy of
nearly 85 percent in predicting rating from reviews
",0,1,0
"Simple Semi-Grant-Free Transmission Strategies Assisted by
  Non-Orthogonal Multiple Access","  Grant-free transmission is an important feature to be supported by future
wireless networks since it reduces the signalling overhead caused by
conventional grant-based schemes. However for grant-free transmission the
number of users admitted to the same channel is not caped which can lead to a
failure of multi-user detection. This paper proposes non-orthogonal
multiple-access (NOMA) assisted semi-grant-free (SGF) transmission which is a
compromise between grant-free and grant-based schemes. In particular instead
of reserving channels either for grant-based users or grant-free users we
focus on an SGF communication scenario where users are admitted to the same
channel via a combination of grant-based and grant-free protocols. As a result
a channel reserved by a grant-based user can be shared by grant-free users
which improves both connectivity and spectral efficiency. Two NOMA assisted SGF
contention control mechanisms are developed to ensure that with a small amount
of signalling overhead the number of admitted grant-free users is carefully
controlled and the interference from the grant-free users to the grant-based
users is effectively suppressed. Analytical results are provided to demonstrate
that the two proposed SGF mechanisms employing different successive
interference cancelation decoding orders are applicable to different practical
network scenarios.
",1,0,0
"End-to-End Data Visualization by Metric Learning and Coordinate
  Transformation","  This paper presents a deep nonlinear metric learning framework for data
visualization on an image dataset. We propose the Triangular Similarity and
prove its equivalence to the Cosine Similarity in measuring a data pair. Based
on this novel similarity a geometrically motivated loss function - the
triangular loss - is then developed for optimizing a metric learning system
comprising two identical CNNs. It is shown that this deep nonlinear system can
be efficiently trained by a hybrid algorithm based on the conventional
backpropagation algorithm. More interestingly benefiting from classical
manifold learning theories the proposed system offers two different views to
visualize the outputs the second of which provides better classification
results than the state-of-the-art methods in the visualizable spaces.
",0,0,1
"Erasure coding for distributed matrix multiplication for matrices with
  bounded entries","  Distributed matrix multiplication is widely used in several scientific
domains. It is well recognized that computation times on distributed clusters
are often dominated by the slowest workers (called stragglers). Recent work has
demonstrated that straggler mitigation can be viewed as a problem of designing
erasure codes. For matrices $mathbf A$ and $mathbf B$ the technique
essentially maps the computation of $mathbf A^T mathbf B$ into the
multiplication of smaller (coded) submatrices. The stragglers are treated as
erasures in this process. The computation can be completed as long as a certain
number of workers (called the recovery threshold) complete their assigned
tasks.
  We present a novel coding strategy for this problem when the absolute values
of the matrix entries are sufficiently small. We demonstrate a tradeoff between
the assumed absolute value bounds on the matrix entries and the recovery
threshold. At one extreme we are optimal with respect to the recovery
threshold and on the other extreme we match the threshold of prior work.
Experimental results on cloud-based clusters validate the benefits of our
method.
",1,0,0
"CFEA: Collaborative Feature Ensembling Adaptation for Domain Adaptation
  in Unsupervised Optic Disc and Cup Segmentation","  Recently deep neural networks have demonstrated comparable and even better
performance with board-certified ophthalmologists in well-annotated datasets.
However the diversity of retinal imaging devices poses a significant
challenge: domain shift which leads to performance degradation when applying
the deep learning models to new testing domains. In this paper we propose a
novel unsupervised domain adaptation framework called Collaborative Feature
Ensembling Adaptation (CFEA) to effectively overcome this challenge. Our
proposed CFEA is an interactive paradigm which presents an exquisite of
collaborative adaptation through both adversarial learning and ensembling
weights. In particular we simultaneously achieve domain-invariance and
maintain an exponential moving average of the historical predictions which
achieves a better prediction for the unlabeled data via ensembling weights
during training. Without annotating any sample from the target domain multiple
adversarial losses in encoder and decoder layers guide the extraction of
domain-invariant features to confuse the domain classifier and meanwhile
benefit the ensembling of smoothing weights. Comprehensive experimental results
demonstrate that our CFEA model can overcome performance degradation and
outperform the state-of-the-art methods in segmenting retinal optic disc and
cup from fundus images. textitCode is available at
urlhttps://github.com/cswin/AWC.
",0,0,1
"Language Model as an Annotator: Exploring DialoGPT for Dialogue
  Summarization","  Current dialogue summarization systems usually encode the text with a number
of general semantic features (e.g. keywords and topics) to gain more powerful
dialogue modeling capabilities. However these features are obtained via
open-domain toolkits that are dialog-agnostic or heavily relied on human
annotations. In this paper we show how DialoGPT a pre-trained model for
conversational response generation can be developed as an unsupervised
dialogue annotator which takes advantage of dialogue background knowledge
encoded in DialoGPT. We apply DialoGPT to label three types of features on two
dialogue summarization datasets SAMSum and AMI and employ pre-trained and non
pre-trained models as our summarizes. Experimental results show that our
proposed method can obtain remarkable improvements on both datasets and
achieves new state-of-the-art performance on the SAMSum dataset.
",0,1,0
Multi-Modal Deep Clustering: Unsupervised Partitioning of Images,"  The clustering of unlabeled raw images is a daunting task which has recently
been approached with some success by deep learning methods. Here we propose an
unsupervised clustering framework which learns a deep neural network in an
end-to-end fashion providing direct cluster assignments of images without
additional processing. Multi-Modal Deep Clustering (MMDC) trains a deep
network to align its image embeddings with target points sampled from a
Gaussian Mixture Model distribution. The cluster assignments are then
determined by mixture component association of image embeddings.
Simultaneously the same deep network is trained to solve an additional
self-supervised task of predicting image rotations. This pushes the network to
learn more meaningful image representations that facilitate a better
clustering. Experimental results show that MMDC achieves or exceeds
state-of-the-art performance on six challenging benchmarks. On natural image
datasets we improve on previous results with significant margins of up to 20%
absolute accuracy points yielding an accuracy of 82% on CIFAR-10 45% on
CIFAR-100 and 69% on STL-10.
",0,0,1
"Mini Lesions Detection on Diabetic Retinopathy Images via Large Scale
  CNN Features","  Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is
a primary cause of blindness in working-age people and it is estimated that 3
to 4 million people with diabetes are blinded by DR every year worldwide. Early
diagnosis have been considered an effective way to mitigate such problem. The
ultimate goal of our research is to develop novel machine learning techniques
to analyze the DR images generated by the fundus camera for automatically DR
diagnosis. In this paper we focus on identifying small lesions on DR fundus
images. The results from our analysis which include the lesion category and
their exact locations in the image can be used to facilitate the determination
of DR severity (indicated by DR stages). Different from traditional object
detection for natural images lesion detection for fundus images have unique
challenges. Specifically the size of a lesion instance is usually very small
compared with the original resolution of the fundus images making them
diffcult to be detected. We analyze the lesion-vs-image scale carefully and
propose a large-size feature pyramid network (LFPN) to preserve more image
details for mini lesion instance detection. Our method includes an effective
region proposal strategy to increase the sensitivity. The experimental results
show that our proposed method is superior to the original feature pyramid
network (FPN) method and Faster RCNN.
",0,0,1
VS-Net: Voting with Segmentation for Visual Localization,"  Visual localization is of great importance in robotics and computer vision.
Recently scene coordinate regression based methods have shown good performance
in visual localization in small static scenes. However it still estimates
camera poses from many inferior scene coordinates. To address this problem we
propose a novel visual localization framework that establishes 2D-to-3D
correspondences between the query image and the 3D map with a series of
learnable scene-specific landmarks. In the landmark generation stage the 3D
surfaces of the target scene are over-segmented into mosaic patches whose
centers are regarded as the scene-specific landmarks. To robustly and
accurately recover the scene-specific landmarks we propose the Voting with
Segmentation Network (VS-Net) to segment the pixels into different landmark
patches with a segmentation branch and estimate the landmark locations within
each patch with a landmark location voting branch. Since the number of
landmarks in a scene may reach up to 5000 training a segmentation network with
such a large number of classes is both computation and memory costly for the
commonly used cross-entropy loss. We propose a novel prototype-based triplet
loss with hard negative mining which is able to train semantic segmentation
networks with a large number of labels efficiently. Our proposed VS-Net is
extensively tested on multiple public benchmarks and can outperform
state-of-the-art visual localization methods. Code and models are available at
hrefhttps://github.com/zju3dv/VS-Nethttps://github.com/zju3dv/VS-Net.
",0,0,1
Upper and Lower Bounds on Black-Box Steganography,"  We study the limitations of steganography when the sender is not using any
properties of the underlying channel beyond its entropy and the ability to
sample from it. On the negative side we show that the number of samples the
sender must obtain from the channel is exponential in the rate of the
stegosystem. On the positive side we present the first secret-key stegosystem
that essentially matches this lower bound regardless of the entropy of the
underlying channel. Furthermore for high-entropy channels we present the
first secret-key stegosystem that matches this lower bound statelessly (i.e.
without requiring synchronized state between sender and receiver).
",1,0,0
Dataset Condensation with Distribution Matching,"  Computational cost to train state-of-the-art deep models in many learning
problems is rapidly increasing due to more sophisticated models and larger
datasets. A recent promising direction to reduce training time is dataset
condensation that aims to replace the original large training set with a
significantly smaller learned synthetic set while preserving its information.
While training deep models on the small set of condensed images can be
extremely fast their synthesis remains computationally expensive due to the
complex bi-level optimization and second-order derivative computation. In this
work we propose a simple yet effective dataset condensation technique that
requires significantly lower training cost with comparable performance by
matching feature distributions of the synthetic and original training images in
sampled embedding spaces. Thanks to its efficiency we apply our method to more
realistic and larger datasets with sophisticated neural architectures and
achieve a significant performance boost while using larger synthetic training
set. We also show various practical benefits of our method in continual
learning and neural architecture search.
",0,0,1
"Weakly Supervised Video Summarization by Hierarchical Reinforcement
  Learning","  Conventional video summarization approaches based on reinforcement learning
have the problem that the reward can only be received after the whole summary
is generated. Such kind of reward is sparse and it makes reinforcement learning
hard to converge. Another problem is that labelling each frame is tedious and
costly which usually prohibits the construction of large-scale datasets. To
solve these problems we propose a weakly supervised hierarchical reinforcement
learning framework which decomposes the whole task into several subtasks to
enhance the summarization quality. This framework consists of a manager network
and a worker network. For each subtask the manager is trained to set a subgoal
only by a task-level binary label which requires much fewer labels than
conventional approaches. With the guide of the subgoal the worker predicts the
importance scores for video frames in the subtask by policy gradient according
to both global reward and innovative defined sub-rewards to overcome the sparse
problem. Experiments on two benchmark datasets show that our proposal has
achieved the best performance even better than supervised approaches.
",0,0,1
"Supervised Opinion Aspect Extraction by Exploiting Past Extraction
  Results","  One of the key tasks of sentiment analysis of product reviews is to extract
product aspects or features that users have expressed opinions on. In this
work we focus on using supervised sequence labeling as the base approach to
performing the task. Although several extraction methods using sequence
labeling methods such as Conditional Random Fields (CRF) and Hidden Markov
Models (HMM) have been proposed we show that this supervised approach can be
significantly improved by exploiting the idea of concept sharing across
multiple domains. For example ""screen"" is an aspect in iPhone but not only
iPhone has a screen many electronic devices have screens too. When ""screen""
appears in a review of a new domain (or product) it is likely to be an aspect
too. Knowing this information enables us to do much better extraction in the
new domain. This paper proposes a novel extraction method exploiting this idea
in the context of supervised sequence labeling. Experimental results show that
it produces markedly better results than without using the past information.
",0,1,0
Joint Channel Parameter Estimation via Diffusive Molecular Communication,"  The design and analysis of diffusive molecular communication systems
generally requires knowledge of the environment's physical and chemical
properties. Furthermore prospective applications might rely on the timely
detection of changes in the local system parameters. This paper studies the
local estimation of channel parameters for diffusive molecular communication
when a transmitter releases molecules that are observed by a receiver. The
Fisher information matrix of the joint parameter estimation problem is derived
so that the Cramer-Rao lower bound on the variance of locally unbiased
estimation can be found. The joint estimation problem can be reduced to the
estimation of any subset of the channel parameters. Maximum likelihood
estimation leads to closed-form solutions for some single-parameter estimation
problems and can otherwise be determined numerically. Peak-based estimators are
proposed for low-complexity estimation of a single unknown parameter.
",1,0,0
"Echo State Transfer Learning for Data Correlation Aware Resource
  Allocation in Wireless Virtual Reality","  In this paper the problem of data correlation-aware resource management is
studied for a network of wireless virtual reality (VR) users communicating over
cloud-based small cell networks (SCNs). In the studied model small base
stations (SBSs) with limited computational resources act as VR control centers
that collect the tracking information from VR users over the cellular uplink
and send them to the VR users over the downlink. In such a setting VR users
may send or request correlated or similar data (panoramic images and tracking
data). This potential spatial data correlation can be factored into the
resource allocation problem to reduce the traffic load in both uplink and
downlink. This VR resource allocation problem is formulated as a noncooperative
game that allows jointly optimizing the computational and spectrum resources
while being cognizant of the data correlation. To solve this game a transfer
learning algorithm based on the machine learning framework of echo state
networks (ESNs) is proposed. Unlike conventional reinforcement learning
algorithms that must be executed each time the environment changes the
proposed algorithm can intelligently transfer information on the learned
utility across time to rapidly adapt to environmental dynamics due to factors
such as changes in the users' content or data correlation. Simulation results
show that the proposed algorithm achieves up to 16.7% and 18.2% gains in terms
of delay compared to the Q-learning with data correlation and Q-learning
without data correlation. The results also show that the proposed algorithm has
a faster convergence time than Q-learning and can guarantee low delays.
",1,0,0
Properties of phoneme N -grams across the world's language families,"  In this article we investigate the properties of phoneme N-grams across half
of the world's languages. We investigate if the sizes of three different N-gram
distributions of the world's language families obey a power law. Further the
N-gram distributions of language families parallel the sizes of the families
which seem to obey a power law distribution. The correlation between N-gram
distributions and language family sizes improves with increasing values of N.
We applied statistical tests originally given by physicists to test the
hypothesis of power law fit to twelve different datasets. The study also raises
some new questions about the use of N-gram distributions in linguistic
research which we answer by running a statistical test.
",0,1,0
Higher Weights of Affine Grassmann Codes and Their Duals,"  We consider the question of determining the higher weights or the generalized
Hamming weights of affine Grassmann codes and their duals. Several initial as
well as terminal higher weights of affine Grassmann codes of an arbitrary level
are determined explicitly. In the case of duals of these codes we give a
formula for many initial as well as terminal higher weights. As a special case
we obtain an alternative simpler proof of the formula of Beelen et al for the
minimum distance of the dual of an affine Grasmann code.
",1,0,0
"Deep Feature Consistent Deep Image Transformations: Downscaling
  Decolorization and HDR Tone Mapping","  Building on crucial insights into the determining factors of the visual
integrity of an image and the property of deep convolutional neural network
(CNN) we have developed the Deep Feature Consistent Deep Image Transformation
(DFC-DIT) framework which unifies challenging one-to-many mapping image
processing problems such as image downscaling decolorization (colour to
grayscale conversion) and high dynamic range (HDR) image tone mapping. We train
one CNN as a non-linear mapper to transform an input image to an output image
following what we term the deep feature consistency principle which is enforced
through another pretrained and fixed deep CNN. This is the first work that uses
deep learning to solve and unify these three common image processing tasks. We
present experimental results to demonstrate the effectiveness of the DFC-DIT
technique and its state of the art performances.
",0,0,1
Parameterizing Activation Functions for Adversarial Robustness,"  Deep neural networks are known to be vulnerable to adversarially perturbed
inputs. A commonly used defense is adversarial training whose performance is
influenced by model capacity. While previous works have studied the impact of
varying model width and depth on robustness the impact of increasing capacity
by using learnable parametric activation functions (PAFs) has not been studied.
We study how using learnable PAFs can improve robustness in conjunction with
adversarial training. We first ask the question: how should we incorporate
parameters into activation functions to improve robustness? To address this we
analyze the direct impact of activation shape on robustness through PAFs and
observe that activation shapes with positive outputs on negative inputs and
with high finite curvature can increase robustness. We combine these properties
to create a new PAF which we call Parametric Shifted Sigmoidal Linear Unit
(PSSiLU). We then combine PAFs (including PReLU PSoftplus and PSSiLU) with
adversarial training and analyze robust performance. We find that PAFs optimize
towards activation shape properties found to directly affect robustness.
Additionally we find that while introducing only 1-2 learnable parameters into
the network smooth PAFs can significantly increase robustness over ReLU. For
instance when trained on CIFAR-10 with additional synthetic data PSSiLU
improves robust accuracy by 4.54% over ReLU on ResNet-18 and 2.69% over ReLU on
WRN-28-10 in the $ell_infty$ threat model while adding only 2 additional
parameters into the network architecture. The PSSiLU WRN-28-10 model achieves
61.96% AutoAttack accuracy improving over the state-of-the-art robust accuracy
on RobustBench (Croce et al. 2020).
",0,0,1
An Enhanced Corpus for Arabic Newspapers Comments,"  In this paper we propose our enhanced approach to create a dedicated corpus
for Algerian Arabic newspapers comments. The developed approach has to enhance
an existing approach by the enrichment of the available corpus and the
inclusion of the annotation step by following the Model Annotate Train Test
Evaluate Revise (MATTER) approach. A corpus is created by collecting comments
from web sites of three well know Algerian newspapers. Three classifiers
support vector machines na""ive Bayes and k-nearest neighbors were used
for classification of comments into positive and negative classes. To identify
the influence of the stemming in the obtained results the classification was
tested with and without stemming. Obtained results show that stemming does not
enhance considerably the classification due to the nature of Algerian comments
tied to Algerian Arabic Dialect. The promising results constitute a motivation
for us to improve our approach especially in dealing with non Arabic sentences
especially Dialectal and French ones.
",0,1,0
Best-Buddies Tracking,"  Best-Buddies Tracking (BBT) applies the Best-Buddies Similarity measure (BBS)
to the problem of model-free online tracking. BBS was introduced as a
similarity measure between two point sets and was shown to be very effective
for template matching. Originally BBS was designed to work with point sets of
equal size and we propose a modification that lets it handle point sets of
different size. The modified BBS is better suited to handle scale changes in
the template size as well as support a variable number of template images. We
embed the modified BBS in a particle filter framework and obtain good results
on a number of standard benchmarks.
",0,0,1
Decoupling recognition and transcription in Mandarin ASR,"  Much of the recent literature on automatic speech recognition (ASR) is taking
an end-to-end approach. Unlike English where the writing system is closely
related to sound Chinese characters (Hanzi) represent meaning not sound. We
propose factoring audio -> Hanzi into two sub-tasks: (1) audio -> Pinyin and
(2) Pinyin -> Hanzi where Pinyin is a system of phonetic transcription of
standard Chinese. Factoring the audio -> Hanzi task in this way achieves 3.9%
CER (character error rate) on the Aishell-1 corpus the best result reported on
this dataset so far.
",0,1,0
VideoSET: Video Summary Evaluation through Text,"  In this paper we present VideoSET a method for Video Summary Evaluation
through Text that can evaluate how well a video summary is able to retain the
semantic information contained in its original video. We observe that semantics
is most easily expressed in words and develop a text-based approach for the
evaluation. Given a video summary a text representation of the video summary
is first generated and an NLP-based metric is then used to measure its
semantic distance to ground-truth text summaries written by humans. We show
that our technique has higher agreement with human judgment than pixel-based
distance metrics. We also release text annotations and ground-truth text
summaries for a number of publicly available video datasets for use by the
computer vision community.
",0,1,0
Inverse Compositional Spatial Transformer Networks,"  In this paper we establish a theoretical connection between the classical
Lucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer
Networks (STNs). STNs are of interest to the vision and learning communities
due to their natural ability to combine alignment and classification within the
same theoretical framework. Inspired by the Inverse Compositional (IC) variant
of the LK algorithm we present Inverse Compositional Spatial Transformer
Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance
than conventional STNs with less model capacity; in particular we show
superior performance in pure image alignment tasks as well as joint
alignment/classification problems on real-world problems.
",0,0,1
Sparse Graph Codes for Non-adaptive Quantitative Group Testing,"  This paper considers the problem of Quantitative Group Testing (QGT).
Consider a set of $N$ items among which $K$ items are defective. The QGT
problem is to identify (all or a sufficiently large fraction of) the defective
items where the result of a test reveals the number of defective items in the
tested group. In this work we propose a non-adaptive QGT algorithm using
sparse graph codes over bi-regular bipartite graphs with left-degree $ell$ and
right degree $r$ and binary $t$-error-correcting BCH codes. The proposed scheme
provides exact recovery with probabilistic guarantee i.e. recovers all the
defective items with high probability. In particular we show that for the
sub-linear regime where $fracKN$ vanishes as $KNrightarrowinfty$ the
proposed algorithm requires at most $m=c(t)Kleft(tlog_2left(fracell
Nc(t)K+1right)+1right)+1$ tests to recover all the defective items with
probability approaching one as $KNrightarrowinfty$ where $c(t)$ depends
only on $t$. The results of our theoretical analysis reveal that the minimum
number of required tests is achieved by $t=2$. The encoding and decoding of the
proposed algorithm for any $tleq 4$ have the computational complexity of
$mathcalO(Klog^2 fracNK)$ and $mathcalO(Klog fracNK)$
respectively. Our simulation results also show that the proposed algorithm
significantly outperforms a non-adaptive semi-quantitative group testing
algorithm recently proposed by Abdalla emphet al. in terms of the required
number of tests for identifying all the defective items with high probability.
",1,0,0
Separating Reflection and Transmission Images in the Wild,"  The reflections caused by common semi-reflectors such as glass windows can
impact the performance of computer vision algorithms. State-of-the-art methods
can remove reflections on synthetic data and in controlled scenarios. However
they are based on strong assumptions and do not generalize well to real-world
images. Contrary to a common misconception real-world images are challenging
even when polarization information is used. We present a deep learning approach
to separate the reflected and the transmitted components of the recorded
irradiance which explicitly uses the polarization properties of light. To
train it we introduce an accurate synthetic data generation pipeline which
simulates realistic reflections including those generated by curved and
non-ideal surfaces non-static scenes and high-dynamic-range scenes.
",0,0,1
A Greedy Algorithm for the Analysis Transform Domain,"  Many image processing applications benefited remarkably from the theory of
sparsity. One model of sparsity is the cosparse analysis one. It was shown that
using l_1-minimization one might stably recover a cosparse signal from a small
set of random linear measurements if the operator is a frame. Another effort
has provided guarantees for dictionaries that have a near optimal projection
procedure using greedy-like algorithms. However no claims have been given for
frames. A common drawback of all these existing techniques is their high
computational cost for large dimensional problems.
  In this work we propose a new greedy-like technique with theoretical recovery
guarantees for frames as the analysis operator closing the gap between greedy
and relaxation techniques. Our results cover both the case of bounded
adversarial noise where we show that the algorithm provides us with a stable
reconstruction and the one of random Gaussian noise for which we prove that
it has a denoising effect closing another gap in the analysis framework. Our
proposed program unlike the previous greedy-like ones that solely act in the
signal domain operates mainly in the analysis operator's transform domain.
Besides the theoretical benefit the main advantage of this strategy is its
computational efficiency that makes it easily applicable to visually big data.
We demonstrate its performance on several high dimensional images.
",1,0,0
I Find Your Lack of Uncertainty in Computer Vision Disturbing,"  Neural networks are used for many real world applications but often they
have problems estimating their own confidence. This is particularly problematic
for computer vision applications aimed at making high stakes decisions with
humans and their lives. In this paper we make a meta-analysis of the
literature showing that most if not all computer vision applications do not
use proper epistemic uncertainty quantification which means that these models
ignore their own limitations. We describe the consequences of using models
without proper uncertainty quantification and motivate the community to adopt
versions of the models they use that have proper calibrated epistemic
uncertainty in order to enable out of distribution detection. We close the
paper with a summary of challenges on estimating uncertainty for computer
vision applications and recommendations.
",0,0,1
3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset,"  In this work we present a novel publicly available stereo based 3D RGB
dataset for multi-object zebrafish tracking called 3D-ZeF. Zebrafish is an
increasingly popular model organism used for studying neurological disorders
drug addiction and more. Behavioral analysis is often a critical part of such
research. However visual similarity occlusion and erratic movement of the
zebrafish makes robust 3D tracking a challenging and unsolved problem. The
proposed dataset consists of eight sequences with a duration between 15-120
seconds and 1-10 free moving zebrafish. The videos have been annotated with a
total of 86400 points and bounding boxes. Furthermore we present a complexity
score and a novel open-source modular baseline system for 3D tracking of
zebrafish. The performance of the system is measured with respect to two
detectors: a naive approach and a Faster R-CNN based fish head detector. The
system reaches a MOTA of up to 77.6%. Links to the code and dataset is
available at the project page https://vap.aau.dk/3d-zef
",0,0,1
"On the Feedback Reduction of Relay Aided Multiuser Networks using
  Compressive Sensing","  In this paper we propose a feedback reduction scheme for full-duplex
relay-aided multiuser networks. The proposed scheme permits the base station
(BS) to obtain channel state information (CSI) from a subset of strong users
under substantially reduced feedback overhead. More specifically we cast the
problem of user identification and CSI estimation as a block sparse signal
recovery problem in compressive sensing (CS). Using existing CS block recovery
algorithms we first obtain the identity of the strong users and then estimate
their CSI using the best linear unbiased estimator (BLUE). To minimize the
effect of noise on the estimated CSI we introduce a back-off strategy that
optimally backs-off on the noisy estimated CSI and derive the error covariance
matrix of the post-detection noise. In addition to this we provide exact
closed form expressions for the average maximum equivalent SNR at the
destination user. Numerical results show that the proposed algorithm
drastically reduces the feedback air-time and achieves a rate close to that
obtained by scheduling schemes that require dedicated error-free feedback from
all the network users.
",1,0,0
Deep Transfer Learning for Person Re-identification,"  Person re-identification (Re-ID) poses a unique challenge to deep learning:
how to learn a deep model with millions of parameters on a small training set
of few or no labels. In this paper a number of deep transfer learning models
are proposed to address the data sparsity problem. First a deep network
architecture is designed which differs from existing deep Re-ID models in that
(a) it is more suitable for transferring representations learned from large
image classification datasets and (b) classification loss and verification
loss are combined each of which adopts a different dropout strategy. Second a
two-stepped fine-tuning strategy is developed to transfer knowledge from
auxiliary datasets. Third given an unlabelled Re-ID dataset a novel
unsupervised deep transfer learning model is developed based on co-training.
The proposed models outperform the state-of-the-art deep Re-ID models by large
margins: we achieve Rank-1 accuracy of 85.4% 83.7% and 56.3% on CUHK03
Market1501 and VIPeR respectively whilst on VIPeR our unsupervised model
(45.1%) beats most supervised models.
",0,0,1
AR-Net: Adaptive Frame Resolution for Efficient Action Recognition,"  Action recognition is an open and challenging problem in computer vision.
While current state-of-the-art models offer excellent recognition results
their computational expense limits their impact for many real-world
applications. In this paper we propose a novel approach called AR-Net
(Adaptive Resolution Network) that selects on-the-fly the optimal resolution
for each frame conditioned on the input for efficient action recognition in
long untrimmed videos. Specifically given a video frame a policy network is
used to decide what input resolution should be used for processing by the
action recognition model with the goal of improving both accuracy and
efficiency. We efficiently train the policy network jointly with the
recognition model using standard back-propagation. Extensive experiments on
several challenging action recognition benchmark datasets well demonstrate the
efficacy of our proposed approach over state-of-the-art methods. The project
page can be found at https://mengyuest.github.io/AR-Net
",0,0,1
"Design of Low-Artifact Interpolation Kernels by Means of Computer
  Algebra","  We present a number of new piecewise-polynomial kernels for image
interpolation. The kernels are constructed by optimizing a measure of
interpolation quality based on the magnitude of anisotropic artifacts. The
kernel design process is performed symbolically using Mathematica computer
algebra system. Experimental evaluation involving 14 image quality assessment
methods demonstrates that our results compare favorably with the existing
linear interpolators.
",0,0,1
"Characterizing and Improving Generalized Belief Propagation Algorithms
  on the 2D Edwards-Anderson Model","  We study the performance of different message passing algorithms in the two
dimensional Edwards Anderson model. We show that the standard Belief
Propagation (BP) algorithm converges only at high temperature to a paramagnetic
solution. Then we test a Generalized Belief Propagation (GBP) algorithm
derived from a Cluster Variational Method (CVM) at the plaquette level. We
compare its performance with BP and with other algorithms derived under the
same approximation: Double Loop (DL) and a two-ways message passing algorithm
(HAK). The plaquette-CVM approximation improves BP in at least three ways: the
quality of the paramagnetic solution at high temperatures a better estimate
(lower) for the critical temperature and the fact that the GBP message passing
algorithm converges also to non paramagnetic solutions. The lack of convergence
of the standard GBP message passing algorithm at low temperatures seems to be
related to the implementation details and not to the appearance of long range
order. In fact we prove that a gauge invariance of the constrained CVM free
energy can be exploited to derive a new message passing algorithm which
converges at even lower temperatures. In all its region of convergence this new
algorithm is faster than HAK and DL by some orders of magnitude.
",1,0,0
Two Local Models for Neural Constituent Parsing,"  Non-local features have been exploited by syntactic parsers for capturing
dependencies between sub output structures. Such features have been a key to
the success of state-of-the-art statistical parsers. With the rise of deep
learning however it has been shown that local output decisions can give
highly competitive accuracies thanks to the power of dense neural input
representations that embody global syntactic information. We investigate two
conceptually simple local neural models for constituent parsing which make
local decisions to constituent spans and CFG rules respectively. Consistent
with previous findings along the line our best model gives highly competitive
results achieving the labeled bracketing F1 scores of 92.4% on PTB and 87.3%
on CTB 5.1.
",0,1,0
"Algebraic Distributed Differential Space-Time Codes with Low Decoding
  Complexity","  The differential encoding/decoding setup introduced by Kiran et al
Oggier-Hassibi and Jing-Jafarkhani for wireless relay networks that use
codebooks consisting of unitary matrices is extended to allow codebooks
consisting of scaled unitary matrices. For such codebooks to be usable in the
Jing-Hassibi protocol for cooperative diversity the conditions involving the
relay matrices and the codebook that need to be satisfied are identified. Using
the algebraic framework of extended Clifford algebras a new class of
Distributed Differential Space-Time Codes satisfying these conditions for power
of two number of relays and also achieving full cooperative diversity with a
low complexity sub-optimal receiver is proposed. Simulation results indicate
that the proposed codes outperform both the cyclic codes as well as the
circulant codes. Furthermore these codes can also be applied as Differential
Space-Time codes for non-coherent communication in classical point to point
multiple antenna systems.
",1,0,0
"An Upper Bound on Multi-hop Transmission Capacity with Dynamic Routing
  Selection","  This paper develops upper bounds on the end-to-end transmission capacity of
multi-hop wireless networks. Potential source-destination paths are dynamically
selected from a pool of randomly located relays from which a closed-form lower
bound on the outage probability is derived in terms of the expected number of
potential paths. This is in turn used to provide an upper bound on the number
of successful transmissions that can occur per unit area which is known as the
transmission capacity. The upper bound results from assuming independence among
the potential paths and can be viewed as the maximum diversity case. A useful
aspect of the upper bound is its simple form for an arbitrary-sized network
which allows insights into how the number of hops and other network parameters
affect spatial throughput in the non-asymptotic regime. The outage probability
analysis is then extended to account for retransmissions with a maximum number
of allowed attempts. In contrast to prevailing wisdom we show that
predetermined routing (such as nearest-neighbor) is suboptimal since more hops
are not useful once the network is interference-limited. Our results also make
clear that randomness in the location of relay sets and dynamically varying
channel states is helpful in obtaining higher aggregate throughput and that
dynamic route selection should be used to exploit path diversity.
",1,0,0
An Efficient Transformer Decoder with Compressed Sub-layers,"  The large attention-based encoder-decoder network (Transformer) has become
prevailing recently due to its effectiveness. But the high computation
complexity of its decoder raises the inefficiency issue. By examining the
mathematic formulation of the decoder we show that under some mild conditions
the architecture could be simplified by compressing its sub-layers the basic
building block of Transformer and achieves a higher parallelism. We thereby
propose Compressed Attention Network whose decoder layer consists of only one
sub-layer instead of three. Extensive experiments on 14 WMT machine translation
tasks show that our model is 1.42x faster with performance on par with a strong
baseline. This strong baseline is already 2x faster than the widely used
standard baseline without loss in performance.
",0,1,0
Pedestrian Collision Avoidance System (PeCAS): a Deep Learning Framework,"  We propose a new deep learning based framework to identify pedestrians and
caution distracted drivers in an effort to prevent the loss of life and
property. This framework uses two Convolutional Neural Networks (CNN) one
which detects pedestrians and the second which predicts the onset of drowsiness
in a driver is implemented on a Raspberry Pi 3 Model B+ shows great promise.
The algorithm for implementing such a low-cost low-compute model is presented
and the results discussed.
",0,0,1
"A Comparison of Deep Learning Object Detection Models for Satellite
  Imagery","  In this work we compare the detection accuracy and speed of several
state-of-the-art models for the task of detecting oil and gas fracking wells
and small cars in commercial electro-optical satellite imagery. Several models
are studied from the single-stage two-stage and multi-stage object detection
families of techniques. For the detection of fracking well pads (50m - 250m)
we find single-stage detectors provide superior prediction speed while also
matching detection performance of their two and multi-stage counterparts.
However for detecting small cars two-stage and multi-stage models provide
substantially higher accuracies at the cost of some speed. We also measure
timing results of the sliding window object detection algorithm to provide a
baseline for comparison. Some of these models have been incorporated into the
Lockheed Martin Globally-Scalable Automated Target Recognition (GATR)
framework.
",0,0,1
"Delay Minimization in Real-time Communications with Joint Buffering and
  Coding","  We present a closed-form expression for the minimal delay that is achievable
in a setting that combines a buffer and an erasure code used to mitigate the
packet delay variance. The erasure code is modeled according to the recent
information-theoretic results on finite block length codes. Evaluations reveal
that accurate knowledge of the network parameters is essential for optimal
operation. Moreover it is shown that when the network packet delay variance
is large the buffer delay becomes negligible. Therefore in this case the
delay budget should be spent mainly on the erasure code.
",1,0,0
Graphical Representation for Heterogeneous Face Recognition,"  Heterogeneous face recognition (HFR) refers to matching face images acquired
from different sources (i.e. different sensors or different wavelengths) for
identification. HFR plays an important role in both biometrics research and
industry. In spite of promising progresses achieved in recent years HFR is
still a challenging problem due to the difficulty to represent two
heterogeneous images in a homogeneous manner. Existing HFR methods either
represent an image ignoring the spatial information or rely on a
transformation procedure which complicates the recognition task. Considering
these problems we propose a novel graphical representation based HFR method
(G-HFR) in this paper. Markov networks are employed to represent heterogeneous
image patches separately which takes the spatial compatibility between
neighboring image patches into consideration. A coupled representation
similarity metric (CRSM) is designed to measure the similarity between obtained
graphical representations. Extensive experiments conducted on multiple HFR
scenarios (viewed sketch forensic sketch near infrared image and thermal
infrared image) show that the proposed method outperforms state-of-the-art
methods.
",0,0,1
"Formalization of semantic network of image constructions in electronic
  content","  A formal theory based on a binary operator of directional associative
relation is constructed in the article and an understanding of an associative
normal form of image constructions is introduced. A model of a commutative
semigroup which provides a presentation of a sentence as three components of
an interrogative linguistic image construction is considered.
",0,1,0
"Comments on ""Overdemodulation for High-Performance Receivers with
  Low-Resolution ADC""","  This submission has been withdrawn by arXiv administrators due to
misrepresentation of authorship.
",1,0,0
"SemRe-Rank: Improving Automatic Term Extraction By Incorporating
  Semantic Relatedness With Personalised PageRank","  Automatic Term Extraction deals with the extraction of terminology from a
domain specific corpus and has long been an established research area in data
and knowledge acquisition. ATE remains a challenging task as it is known that
there is no existing ATE methods that can consistently outperform others in any
domain. This work adopts a refreshed perspective to this problem: instead of
searching for such a 'one-size-fit-all' solution that may never exist we
propose to develop generic methods to 'enhance' existing ATE methods. We
introduce SemRe-Rank the first method based on this principle to incorporate
semantic relatedness - an often overlooked venue - into an existing ATE method
to further improve its performance. SemRe-Rank incorporates word embeddings
into a personalised PageRank process to compute 'semantic importance' scores
for candidate terms from a graph of semantically related words (nodes) which
are then used to revise the scores of candidate terms computed by a base ATE
algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on
four datasets of diverse nature it is shown to have achieved widespread
improvement over all base methods and across all datasets with up to 15
percentage points when measured by the Precision in the top ranked K candidate
terms (the average for a set of K's) or up to 28 percentage points in F1
measured at a K that equals to the expected real terms in the candidates (F1 in
short). Compared to an alternative approach built on the well-known TextRank
algorithm SemRe-Rank can potentially outperform by up to 8 points in Precision
at top K or up to 17 points in F1.
",0,1,0
"Assessing the Use of Prosody in Constituency Parsing of Imperfect
  Transcripts","  This work explores constituency parsing on automatically recognized
transcripts of conversational speech. The neural parser is based on a sentence
encoder that leverages word vectors contextualized with prosodic features
jointly learning prosodic feature extraction with parsing. We assess the
utility of the prosody in parsing on imperfect transcripts i.e. transcripts
with automatic speech recognition (ASR) errors by applying the parser in an
N-best reranking framework. In experiments on Switchboard we obtain 13-15% of
the oracle N-best gain relative to parsing the 1-best ASR output with
insignificant impact on word recognition error rate. Prosody provides a
significant part of the gain and analyses suggest that it leads to more
grammatical utterances via recovering function words.
",0,1,0
"Buildings Detection in VHR SAR Images Using Fully Convolution Neural
  Networks","  This paper addresses the highly challenging problem of automatically
detecting man-made structures especially buildings in very high resolution
(VHR) synthetic aperture radar (SAR) images. In this context the paper has two
major contributions: Firstly it presents a novel and generic workflow that
initially classifies the spaceborne TomoSAR point clouds $ - $ generated by
processing VHR SAR image stacks using advanced interferometric techniques known
as SAR tomography (TomoSAR) $ - $ into buildings and non-buildings with the aid
of auxiliary information (i.e. either using openly available 2-D building
footprints or adopting an optical image classification scheme) and later back
project the extracted building points onto the SAR imaging coordinates to
produce automatic large-scale benchmark labelled (buildings/non-buildings) SAR
datasets. Secondly these labelled datasets (i.e. building masks) have been
utilized to construct and train the state-of-the-art deep Fully Convolution
Neural Networks with an additional Conditional Random Field represented as a
Recurrent Neural Network to detect building regions in a single VHR SAR image.
Such a cascaded formation has been successfully employed in computer vision and
remote sensing fields for optical image classification but to our knowledge
has not been applied to SAR images. The results of the building detection are
illustrated and validated over a TerraSAR-X VHR spotlight SAR image covering
approximately 39 km$ ^2 $ $ - $ almost the whole city of Berlin $ - $ with mean
pixel accuracies of around 93.84%
",0,0,1
"Integrating Source-channel and Attention-based Sequence-to-sequence
  Models for Speech Recognition","  This paper proposes a novel automatic speech recognition (ASR) framework
called Integrated Source-Channel and Attention (ISCA) that combines the
advantages of traditional systems based on the noisy source-channel model (SC)
and end-to-end style systems using attention-based sequence-to-sequence models.
The traditional SC system framework includes hidden Markov models and
connectionist temporal classification (CTC) based acoustic models language
models (LMs) and a decoding procedure based on a lexicon whereas the
end-to-end style attention-based system jointly models the whole process with a
single model. By rescoring the hypotheses produced by traditional systems using
end-to-end style systems based on an extended noisy source-channel model ISCA
allows structured knowledge to be easily incorporated via the SC-based model
while exploiting the complementarity of the attention-based model. Experiments
on the AMI meeting corpus show that ISCA is able to give a relative word error
rate reduction up to 21% over an individual system and by 13% over an
alternative method which also involves combining CTC and attention-based
models.
",0,1,0
Intimate Partner Violence and Injury Prediction From Radiology Reports,"  Intimate partner violence (IPV) is an urgent prevalent and under-detected
public health issue. We present machine learning models to assess patients for
IPV and injury. We train the predictive algorithms on radiology reports with 1)
IPV labels based on entry to a violence prevention program and 2) injury labels
provided by emergency radiology fellowship-trained physicians. Our dataset
includes 34642 radiology reports and 1479 patients of IPV victims and control
patients. Our best model predicts IPV a median of 3.08 years before violence
prevention program entry with a sensitivity of 64% and a specificity of 95%. We
conduct error analysis to determine for which patients our model has especially
high or low performance and discuss next steps for a deployed clinical risk
model.
",0,1,0
"A Mutual Information Maximization Perspective of Language Representation
  Learning","  We show state-of-the-art word representation learning methods maximize an
objective function that is a lower bound on the mutual information between
different parts of a word sequence (i.e. a sentence). Our formulation provides
an alternative perspective that unifies classical word embedding models (e.g.
Skip-gram) and modern contextual embeddings (e.g. BERT XLNet). In addition to
enhancing our theoretical understanding of these methods our derivation leads
to a principled framework that can be used to construct new self-supervised
tasks. We provide an example by drawing inspirations from related methods based
on mutual information maximization that have been successful in computer
vision and introduce a simple self-supervised objective that maximizes the
mutual information between a global sentence representation and n-grams in the
sentence. Our analysis offers a holistic view of representation learning
methods to transfer knowledge and translate progress across multiple domains
(e.g. natural language processing computer vision audio processing).
",0,1,0
"Dynaboard: An Evaluation-As-A-Service Platform for Holistic
  Next-Generation Benchmarking","  We introduce Dynaboard an evaluation-as-a-service framework for hosting
benchmarks and conducting holistic model comparison integrated with the
Dynabench platform. Our platform evaluates NLP models directly instead of
relying on self-reported metrics or predictions on a single dataset. Under this
paradigm models are submitted to be evaluated in the cloud circumventing the
issues of reproducibility accessibility and backwards compatibility that
often hinder benchmarking in NLP. This allows users to interact with uploaded
models in real time to assess their quality and permits the collection of
additional metrics such as memory use throughput and robustness which --
despite their importance to practitioners -- have traditionally been absent
from leaderboards. On each task models are ranked according to the Dynascore
a novel utility-based aggregation of these statistics which users can
customize to better reflect their preferences placing more/less weight on a
particular axis of evaluation or dataset. As state-of-the-art NLP models push
the limits of traditional benchmarks Dynaboard offers a standardized solution
for a more diverse and comprehensive evaluation of model quality.
",0,1,0
Recover Canonical-View Faces in the Wild with Deep Neural Networks,"  Face images in the wild undergo large intra-personal variations such as
poses illuminations occlusions and low resolutions which cause great
challenges to face-related applications. This paper addresses this challenge by
proposing a new deep learning framework that can recover the canonical view of
face images. It dramatically reduces the intra-person variances while
maintaining the inter-person discriminativeness. Unlike the existing face
reconstruction methods that were either evaluated in controlled 2D environment
or employed 3D information our approach directly learns the transformation
from the face images with a complex set of variations to their canonical views.
At the training stage to avoid the costly process of labeling canonical-view
images from the training set by hand we have devised a new measurement to
automatically select or synthesize a canonical-view image for each identity. As
an application this face recovery approach is used for face verification.
Facial features are learned from the recovered canonical-view face images by
using a facial component-based convolutional neural network. Our approach
achieves the state-of-the-art performance on the LFW dataset.
",0,0,1
Fine-Grained Head Pose Estimation Without Keypoints,"  Estimating the head pose of a person is a crucial problem that has a large
amount of applications such as aiding in gaze estimation modeling attention
fitting 3D models to video and performing face alignment. Traditionally head
pose is computed by estimating some keypoints from the target face and solving
the 2D to 3D correspondence problem with a mean human head model. We argue that
this is a fragile method because it relies entirely on landmark detection
performance the extraneous head model and an ad-hoc fitting step. We present
an elegant and robust way to determine pose by training a multi-loss
convolutional neural network on 300W-LP a large synthetically expanded
dataset to predict intrinsic Euler angles (yaw pitch and roll) directly from
image intensities through joint binned pose classification and regression. We
present empirical tests on common in-the-wild pose benchmark datasets which
show state-of-the-art results. Additionally we test our method on a dataset
usually used for pose estimation using depth and start to close the gap with
state-of-the-art depth pose methods. We open-source our training and testing
code as well as release our pre-trained models.
",0,0,1
"Pronunciation recognition of English phonemes /textipa@/ /ae/
  /textipaA:/ and /textipa2/ using Formants and Mel Frequency Cepstral
  Coefficients","  The Vocal Joystick Vowel Corpus by Washington University was used to study
monophthongs pronounced by native English speakers. The objective of this study
was to quantitatively measure the extent at which speech recognition methods
can distinguish between similar sounding vowels. In particular the phonemes
/textipa@/ /ae/ /textipaA:/ and /textipa2/ were analysed. 748
sound files from the corpus were used and subjected to Linear Predictive Coding
(LPC) to compute their formants and to Mel Frequency Cepstral Coefficients
(MFCC) algorithm to compute the cepstral coefficients. A Decision Tree
Classifier was used to build a predictive model that learnt the patterns of the
two first formants measured in the data set as well as the patterns of the 13
cepstral coefficients. An accuracy of 70% was achieved using formants for the
mentioned phonemes. For the MFCC analysis an accuracy of 52 % was achieved and
an accuracy of 71% when /textipa@/ was ignored. The results obtained show
that the studied algorithms are far from mimicking the ability of
distinguishing subtle differences in sounds like human hearing does.
",0,1,0
Structure Tensor Based Image Interpolation Method,"  Feature preserving image interpolation is an active area in image processing
field. In this paper a new direct edge directed image super-resolution
algorithm based on structure tensors is proposed. Using an isotropic Gaussian
filter the structure tensor at each pixel of the input image is computed and
the pixels are classified to three distinct classes; uniform region corners
and edges according to the eigenvalues of the structure tensor. Due to
application of the isotropic Gaussian filter the classification is robust to
noise presented in image. Based on the tangent eigenvector of the structure
tensor the edge direction is determined and used for interpolation along the
edges. In comparison to some previous edge directed image interpolation
methods the proposed method achieves higher quality in both subjective and
objective aspects. Also the proposed method outperforms previous methods in
case of noisy and JPEG compressed images. Furthermore without the need for
optimization in the process the algorithm can achieve higher speed.
",0,0,1
"Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance
  based High Dimensional Two Sample Testing","  Nonparametric two sample testing is a decision theoretic problem that
involves identifying differences between two random variables without making
parametric assumptions about their underlying distributions. We refer to the
most common settings as mean difference alternatives (MDA) for testing
differences only in first moments and general difference alternatives (GDA)
which is about testing for any difference in distributions. A large number of
test statistics have been proposed for both these settings. This paper connects
three classes of statistics - high dimensional variants of Hotelling's t-test
statistics based on Reproducing Kernel Hilbert Spaces and energy statistics
based on pairwise distances. We ask the question: how much statistical power do
popular kernel and distance based tests for GDA have when the unknown
distributions differ in their means compared to specialized tests for MDA?
  We formally characterize the power of popular tests for GDA like the Maximum
Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent
variants of the Energy Distance with the Euclidean norm (eED) in the
high-dimensional MDA regime. Some practically important properties include (a)
eED and gMMD have asymptotically equal power; furthermore they enjoy a free
lunch because while they are additionally consistent for GDA they also have
the same power as specialized high-dimensional t-test variants for MDA. All
these tests are asymptotically optimal (including matching constants) under MDA
for spherical covariances according to simple lower bounds (b) The power of
gMMD is independent of the kernel bandwidth as long as it is larger than the
choice made by the median heuristic (c) There is a clear and smooth
computation-statistics tradeoff for linear-time subquadratic-time and
quadratic-time versions of these tests with more computation resulting in
higher power.
",1,0,0
Zero-shot Entity Linking with Efficient Long Range Sequence Modeling,"  This paper considers the problem of zero-shot entity linking in which a link
in the test time may not present in training. Following the prevailing
BERT-based research efforts we find a simple yet effective way is to expand
the long-range sequence modeling. Unlike many previous methods our method does
not require expensive pre-training of BERT with long position embedding.
Instead we propose an efficient position embeddings initialization method
called Embedding-repeat which initializes larger position embeddings based on
BERT-Base. On Wikia's zero-shot EL dataset our method improves the SOTA from
76.06% to 79.08% and for its long data the corresponding improvement is from
74.57% to 82.14%. Our experiments suggest the effectiveness of long-range
sequence modeling without retraining the BERT model.
",0,1,0
Guetzli: Perceptually Guided JPEG Encoder,"  Guetzli is a new JPEG encoder that aims to produce visually indistinguishable
images at a lower bit-rate than other common JPEG encoders. It optimizes both
the JPEG global quantization tables and the DCT coefficient values in each JPEG
block using a closed-loop optimizer. Guetzli uses Butteraugli our perceptual
distance metric as the source of feedback in its optimization process. We
reach a 29-45% reduction in data size for a given perceptual distance
according to Butteraugli in comparison to other compressors we tried.
Guetzli's computation is currently extremely slow which limits its
applicability to compressing static content and serving as a proof- of-concept
that we can achieve significant reductions in size by combining advanced
psychovisual models with lossy compression techniques.
",0,0,1
Tracking Dynamic Point Processes on Networks,"  Cascading chains of events are a salient feature of many real-world social
biological and financial networks. In social networks social reciprocity
accounts for retaliations in gang interactions proxy wars in nation-state
conflicts or Internet memes shared via social media. Neuron spikes stimulate
or inhibit spike activity in other neurons. Stock market shocks can trigger a
contagion of volatility throughout a financial network. In these and other
examples only individual events associated with network nodes are observed
usually without knowledge of the underlying dynamic relationships between
nodes. This paper addresses the challenge of tracking how events within such
networks stimulate or influence future events. The proposed approach is an
online learning framework well-suited to streaming data using a multivariate
Hawkes point process model to encapsulate autoregressive features of observed
events within the social network. Recent work on online learning in dynamic
environments is leveraged not only to exploit the dynamics within the
underlying network but also to track the network structure as it evolves.
Regret bounds and experimental results demonstrate that the proposed method
performs nearly as well as an oracle or batch algorithm.
",1,0,0
"Compute-and-Forward on a Multiaccess Relay Channel: Coding and
  Symmetric-Rate Optimization","  We consider a system in which two users communicate with a destination with
the help of a half-duplex relay. Based on the compute-and-forward scheme we
develop and evaluate the performance of coding strategies that are of network
coding spirit. In this framework instead of decoding the users' information
messages the destination decodes two integer-valued linear combinations that
relate the transmitted codewords. Two decoding schemes are considered. In the
first one the relay computes one of the linear combinations and then forwards
it to the destination. The destination computes the other linear combination
based on the direct transmissions. In the second one accounting for the side
information available at the destination through the direct links the relay
compresses what it gets using Wyner-Ziv compression and conveys it to the
destination. The destination then computes the two linear combinations
locally. For both coding schemes we discuss the design criteria and derive
the allowed symmetric-rate. Next we address the power allocation and the
selection of the integer-valued coefficients to maximize the offered
symmetric-rate; an iterative coordinate descent method is proposed. The
analysis shows that the first scheme can outperform standard relaying
techniques in certain regimes and the second scheme while relying on feasible
structured lattice codes can at best achieve the same performance as regular
compress-and-forward for the multiaccess relay network model that we study. The
results are illustrated through some numerical examples.
",1,0,0
A Neural Multi-sequence Alignment TeCHnique (NeuMATCH),"  The alignment of heterogeneous sequential data (video to text) is an
important and challenging problem. Standard techniques for this task including
Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs) suffer from
inherent drawbacks. Mainly the Markov assumption implies that given the
immediate past future alignment decisions are independent of further history.
The separation between similarity computation and alignment decision also
prevents end-to-end training. In this paper we propose an end-to-end neural
architecture where alignment actions are implemented as moving data between
stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture
supports a large variety of alignment tasks including one-to-one one-to-many
skipping unmatched elements and (with extensions) non-monotonic alignment.
Extensive experiments on semi-synthetic and real datasets show that our
algorithm outperforms state-of-the-art baselines.
",0,1,0
"The cross-correlation distribution of a $p$-ary $m$-sequence of period
  $p^2m-1$ and its decimation by $frac(p^m+1)^22(p^e+1)$","  Let $n=2m$ $m$ odd $e|m$ and $p$ odd prime with $pequiv1 mathrmmod
4$. Let $d=frac(p^m+1)^22(p^e+1)$. In this paper we study the
cross-correlation between a $p$-ary $m$-sequence $s_t$ of period
$p^2m-1$ and its decimation $s_dt$. Our result shows that the
cross-correlation function is six-valued and that it takes the values in
$-1 pm p^m-1 frac1pm p^frace22p^m-1 frac(1-
p^e)2p^m-1$. Also the distribution of the cross-correlation is
completely determined.
",1,0,0
Stage-wise Fine-tuning for Graph-to-Text Generation,"  Graph-to-text generation has benefited from pre-trained language models
(PLMs) in achieving better performance than structured graph encoders. However
they fail to fully utilize the structure information of the input graph. In
this paper we aim to further improve the performance of the pre-trained
language model by proposing a structured graph-to-text model with a two-step
fine-tuning mechanism which first fine-tunes the model on Wikipedia before
adapting to the graph-to-text generation. In addition to using the traditional
token and position embeddings to encode the knowledge graph (KG) we propose a
novel tree-level embedding method to capture the inter-dependency structures of
the input graph. This new approach has significantly improved the performance
of all text generation metrics for the English WebNLG 2017 dataset.
",0,1,0
Abnormality Detection inside Blood Vessels with Mobile Nanomachines,"  Motivated by the numerous healthcare applications of molecular communication
within Internet of Bio-Nano Things (IoBNT) this work addresses the problem of
abnormality detection in a blood vessel using multiple biological embedded
computing devices called cooperative biological nanomachines (CNs) and a
common receiver called the fusion center (FC). Due to blood flow inside a
vessel each CN and the FC are assumed to be mobile. In this work each of the
CNs perform abnormality detection with certain probabilities of detection and
false alarm by counting the number of molecules received from a source e.g.
infected tissue. These CNs subsequently report their local decisions to a FC
over a diffusion-advection blood flow channel using different types of
molecules in the presence of inter-symbol interference multi-source
interference and counting errors. Due to limited computational capability at
the FC OR and AND logic based fusion rules are employed to make the final
decision after obtaining each local decision based on the optimal likelihood
ratio test. For the aforementioned system probabilities of detection and false
alarm at the FC are derived for OR and AND fusion rules. Finally simulation
results are presented to validate the derived analytical results which provide
important insights.
",1,0,0
"Mittens: An Extension of GloVe for Learning Domain-Specialized
  Representations","  We present a simple extension of the GloVe representation learning model that
begins with general-purpose representations and updates them based on data from
a specialized domain. We show that the resulting representations can lead to
faster learning and better results on a variety of tasks.
",0,1,0
"Quaternary Constant-Composition Codes with Weight Four and Distances
  Five or Six","  The sizes of optimal constant-composition codes of weight three have been
determined by Chee Ge and Ling with four cases in doubt. Group divisible codes
played an important role in their constructions. In this paper we study the
problem of constructing optimal quaternary constant-composition codes with
Hamming weight four and minimum distances five or six through group divisible
codes and Room square approaches. The problem is solved leaving only five
lengths undetermined. Previously the results on the sizes of such quaternary
constant-composition codes were scarce.
",1,0,0
"Spatial Fuzzy C Means PET Image Segmentation of Neurodegenerative
  Disorder","  Nuclear image has emerged as a promising research work in medical field.
Images from different modality meet its own challenge. Positron Emission
Tomography (PET) image may help to precisely localize disease to assist in
planning the right treatment for each case and saving valuable time. In this
paper a novel approach of Spatial Fuzzy C Means (PET SFCM) clustering
algorithm is introduced on PET scan image datasets. The proposed algorithm is
incorporated the spatial neighborhood information with traditional FCM and
updating the objective function of each cluster. This algorithm is implemented
and tested on huge data collection of patients with brain neuro degenerative
disorder such as Alzheimers disease. It has demonstrated its effectiveness by
testing it for real world patient data sets. Experimental results are compared
with conventional FCM and K Means clustering algorithm. The performance of the
PET SFCM provides satisfactory results compared with other two algorithms
",0,0,1
Real or Not Real that is the Question,"  While generative adversarial networks (GAN) have been widely adopted in
various topics in this paper we generalize the standard GAN to a new
perspective by treating realness as a random variable that can be estimated
from multiple angles. In this generalized framework referred to as
RealnessGAN the discriminator outputs a distribution as the measure of
realness. While RealnessGAN shares similar theoretical guarantees with the
standard GAN it provides more insights on adversarial learning. Compared to
multiple baselines RealnessGAN provides stronger guidance for the generator
achieving improvements on both synthetic and real-world datasets. Moreover it
enables the basic DCGAN architecture to generate realistic images at 1024*1024
resolution when trained from scratch.
",0,0,1
Multimodal One-Shot Learning of Speech and Images,"  Imagine a robot is shown new concepts visually together with spoken tags
e.g. ""milk"" ""eggs"" ""butter"". After seeing one paired audio-visual example per
class it is shown a new set of unseen instances of these objects and asked to
pick the ""milk"". Without receiving any hard labels could it learn to match the
new continuous speech input to the correct visual instance? Although unimodal
one-shot learning has been studied where one labelled example in a single
modality is given per class this example motivates multimodal one-shot
learning. Our main contribution is to formally define this task and to propose
several baseline and advanced models. We use a dataset of paired spoken and
visual digits to specifically investigate recent advances in Siamese
convolutional neural networks. Our best Siamese model achieves twice the
accuracy of a nearest neighbour model using pixel-distance over images and
dynamic time warping over speech in 11-way cross-modal matching.
",0,1,0
"Near-separable Non-negative Matrix Factorization with $ell_1$- and
  Bregman Loss Functions","  Recently a family of tractable NMF algorithms have been proposed under the
assumption that the data matrix satisfies a separability condition Donoho &
Stodden (2003); Arora et al. (2012). Geometrically this condition reformulates
the NMF problem as that of finding the extreme rays of the conical hull of a
finite set of vectors. In this paper we develop several extensions of the
conical hull procedures of Kumar et al. (2013) for robust ($ell_1$)
approximations and Bregman divergences. Our methods inherit all the advantages
of Kumar et al. (2013) including scalability and noise-tolerance. We show that
on foreground-background separation problems in computer vision robust
near-separable NMFs match the performance of Robust PCA considered state of
the art on these problems with an order of magnitude faster training time. We
also demonstrate applications in exemplar selection settings.
",0,0,1
Average Age of Information with Hybrid ARQ under a Resource Constraint,"  Scheduling of the transmission of status updates over an error-prone
communication channel is studied in order to minimize the long-term average age
of information (AoI) at the destination under an average resource constraint
at the source node which limits the average number of transmissions. After
each transmission the source receives an instantaneous ACK/NACK feedback and
decides on the next update without a priori knowledge on the success of the
future transmissions. The optimal scheduling policy is studied under different
feedback mechanisms; in particular standard automatic repeat request (ARQ) and
hybrid ARQ (HARQ) protocols are considered. Average-cost reinforcement learning
algorithms are proposed when the error probabilities for the HARQ system are
unknown.
",1,0,0
An optimal problem for relative entropy,"  Relative entropy is an essential tool in quantum information theory. There
are so many problems which are related to relative entropy. In this article
the optimal values which are defined by $displaystylemax_UinU(cX_d)
S(UrhoU^astparallelsigma)$ and $displaystylemin_UinU(cX_d)
S(UrhoU^astparallelsigma)$ for two positive definite operators
$rhosigmaintextmdPd(cX)$ are obtained. And the set of
$S(UrhoU^astparallelsigma)$ for every unitary operator $U$ is full of
the interval $[displaystylemin_UinU(cX_d)
S(UrhoU^astparallelsigma)displaystylemax_UinU(cX_d)
S(UrhoU^astparallelsigma)]$
",1,0,0
Bottom-Up Top-Down Cues for Weakly-Supervised Semantic Segmentation,"  We consider the task of learning a classifier for semantic segmentation using
weak supervision in the form of image labels which specify the object classes
present in the image. Our method uses deep convolutional neural networks (CNNs)
and adopts an Expectation-Maximization (EM) based approach. We focus on the
following three aspects of EM: (i) initialization; (ii) latent posterior
estimation (E-step) and (iii) the parameter update (M-step). We show that
saliency and attention maps our bottom-up and top-down cues respectively of
simple images provide very good cues to learn an initialization for the
EM-based algorithm. Intuitively we show that before trying to learn to segment
complex images it is much easier and highly effective to first learn to
segment a set of simple images and then move towards the complex ones. Next in
order to update the parameters we propose minimizing the combination of the
standard softmax loss and the KL divergence between the true latent posterior
and the likelihood given by the CNN. We argue that this combination is more
robust to wrong predictions made by the expectation step of the EM method. We
support this argument with empirical and visual results. Extensive experiments
and discussions show that: (i) our method is very simple and intuitive; (ii)
requires only image-level labels; and (iii) consistently outperforms other
weakly-supervised state-of-the-art methods with a very high margin on the
PASCAL VOC 2012 dataset.
",0,0,1
"Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential
  Fixations","  We consider the problem of fine-grained classification on an edge camera
device that has limited power. The edge device must sparingly interact with the
cloud to minimize communication bits to conserve power and the cloud upon
receiving the edge inputs returns a classification label. To deal with
fine-grained classification we adopt the perspective of sequential fixation
with a foveated field-of-view to model cloud-edge interactions. We propose a
novel deep reinforcement learning-based foveation model DRIFT that
sequentially generates and recognizes mixed-acuity images.Training of DRIFT
requires only image-level category labels and encourages fixations to contain
task-relevant information while maintaining data efficiency. Specifically
wetrain a foveation actor network with a novel Deep Deterministic Policy
Gradient by Conditioned Critic and Coaching (DDPGC3) algorithm. In addition we
propose to shape the reward to provide informative feedback after each fixation
to better guide RL training. We demonstrate the effectiveness of DRIFT on this
task by evaluating on five fine-grained classification benchmark datasets and
show that the proposed approach achieves state-of-the-art performance with over
3X reduction in transmitted pixels.
",0,0,1
Interpreting and Boosting Dropout from a Game-Theoretic View,"  This paper aims to understand and improve the utility of the dropout
operation from the perspective of game-theoretic interactions. We prove that
dropout can suppress the strength of interactions between input variables of
deep neural networks (DNNs). The theoretic proof is also verified by various
experiments. Furthermore we find that such interactions were strongly related
to the over-fitting problem in deep learning. Thus the utility of dropout can
be regarded as decreasing interactions to alleviate the significance of
over-fitting. Based on this understanding we propose an interaction loss to
further improve the utility of dropout. Experimental results have shown that
the interaction loss can effectively improve the utility of dropout and boost
the performance of DNNs.
",0,0,1
IRA codes derived from Gruenbaum graph,"  In this paper we consider coding of short data frames (192 bits) by IRA
codes. A new interleaver for the IRA codes based on a Gruenbaum graph is
proposed. The difference of the proposed algorithm from known methods consists
in the following: permutation is performed by using a match smaller interleaver
which is derived from the Gruenbaum graph by finding in this graph a
Hamiltonian path enumerating the passed vertices in ascending order and
passing them again in the permuted order through the edges which are not
included in the Hamiltonian path. For the IRA code the obtained interleaver
provides 0.7-0.8 db gain over a convolutional code decoded by Viterbi
algorithm.
",1,0,0
Hate and Offensive Speech Detection in Hindi and Marathi,"  Sentiment analysis is the most basic NLP task to determine the polarity of
text data. There has been a significant amount of work in the area of
multilingual text as well. Still hate and offensive speech detection faces a
challenge due to inadequate availability of data especially for Indian
languages like Hindi and Marathi. In this work we consider hate and offensive
speech detection in Hindi and Marathi texts. The problem is formulated as a
text classification task using the state of the art deep learning approaches.
We explore different deep learning architectures like CNN LSTM and variations
of BERT like multilingual BERT IndicBERT and monolingual RoBERTa. The basic
models based on CNN and LSTM are augmented with fast text word embeddings. We
use the HASOC 2021 Hindi and Marathi hate speech datasets to compare these
algorithms. The Marathi dataset consists of binary labels and the Hindi dataset
consists of binary as well as more-fine grained labels. We show that the
transformer-based models perform the best and even the basic models along with
FastText embeddings give a competitive performance. Moreover with normal
hyper-parameter tuning the basic models perform better than BERT-based models
on the fine-grained Hindi dataset.
",0,1,0
"On the Capacity of Symmetric Gaussian Interference Channels with
  Feedback","  In this paper we propose a new coding scheme for symmetric Gaussian
interference channels with feedback based on the ideas of time-varying coding
schemes. The proposed scheme improves the Suh-Tse and Kramer inner bounds of
the channel capacity for the cases of weak and not very strong interference.
This improvement is more significant when the signal-to-noise ratio (SNR) is
not very high. It is shown theoretically and numerically that our coding scheme
can outperform the Kramer code. In addition the generalized degrees-of-freedom
of our proposed coding scheme is equal to the Suh-Tse scheme in the strong
interference case. The numerical results show that our coding scheme can attain
better performance than the Suh-Tse coding scheme for all channel parameters.
Furthermore the simplicity of the encoding/decoding algorithms is another
strong point of our proposed coding scheme compared with the Suh-Tse coding
scheme. More importantly our results show that an optimal coding scheme for
the symmetric Gaussian interference channels with feedback can be achieved by
using only marginal posterior distributions under a better cooperation strategy
between transmitters.
",1,0,0
Collaborative Attention Network for Person Re-identification,"  Jointly utilizing global and local features to improve model accuracy is
becoming a popular approach for the person re-identification (ReID) problem
because previous works using global features alone have very limited capacity
at extracting discriminative local patterns in the obtained feature
representation. Existing works that attempt to collect local patterns either
explicitly slice the global feature into several local pieces in a handcrafted
way or apply the attention mechanism to implicitly infer the importance of
different local regions. In this paper we show that by explicitly learning the
importance of small local parts and part combinations we can further improve
the final feature representation for Re-ID. Specifically we first separate the
global feature into multiple local slices at different scale with a proposed
multi-branch structure. Then we introduce the Collaborative Attention Network
(CAN) to automatically learn the combination of features from adjacent slices.
In this way the combination keeps the intrinsic relation between adjacent
features across local regions and scales without losing information by
partitioning the global features. Experiment results on several widely-used
public datasets including Market-1501 DukeMTMC-ReID and CUHK03 prove that the
proposed method outperforms many existing state-of-the-art methods.
",0,0,1
Using Image Priors to Improve Scene Understanding,"  Semantic segmentation algorithms that can robustly segment objects across
multiple camera viewpoints are crucial for assuring navigation and safety in
emerging applications such as autonomous driving. Existing algorithms treat
each image in isolation but autonomous vehicles often revisit the same
locations or maintain information from the immediate past. We propose a simple
yet effective method for leveraging these image priors to improve semantic
segmentation of images from sequential driving datasets. We examine several
methods to fuse these temporal scene priors and introduce a prior fusion
network that is able to learn how to transfer this information. The prior
fusion model improves the accuracy over the non-prior baseline from 69.1% to
73.3% for dynamic classes and from 88.2% to 89.1% for static classes. Compared
to models such as FCN-8 our prior method achieves the same accuracy with 5
times fewer parameters. We used a simple encoder decoder backbone but this
general prior fusion method could be applied to more complex semantic
segmentation backbones. We also discuss how structured representations of
scenes in the form of a scene graph could be leveraged as priors to further
improve scene understanding.
",0,0,1
Temporal Shift GAN for Large Scale Video Generation,"  Video generation models have become increasingly popular in the last few
years however the standard 2D architectures used today lack natural
spatio-temporal modelling capabilities. In this paper we present a network
architecture for video generation that models spatio-temporal consistency
without resorting to costly 3D architectures. The architecture facilitates
information exchange between neighboring time points which improves the
temporal consistency of both the high level structure as well as the low-level
details of the generated frames. The approach achieves state-of-the-art
quantitative performance as measured by the inception score on the UCF-101
dataset as well as better qualitative results. We also introduce a new
quantitative measure (S3) that uses downstream tasks for evaluation. Moreover
we present a new multi-label dataset MaisToy which enables us to evaluate the
generalization of the model.
",0,0,1
"Learning Semantic Sentence Embeddings using Sequential Pair-wise
  Discriminator","  In this paper we propose a method for obtaining sentence-level embeddings.
While the problem of securing word-level embeddings is very well studied we
propose a novel method for obtaining sentence-level embeddings. This is
obtained by a simple method in the context of solving the paraphrase generation
task. If we use a sequential encoder-decoder model for generating paraphrase
we would like the generated paraphrase to be semantically close to the original
sentence. One way to ensure this is by adding constraints for true paraphrase
embeddings to be close and unrelated paraphrase candidate sentence embeddings
to be far. This is ensured by using a sequential pair-wise discriminator that
shares weights with the encoder that is trained with a suitable loss function.
Our loss function penalizes paraphrase sentence embedding distances from being
too large. This loss is used in combination with a sequential encoder-decoder
network. We also validated our method by evaluating the obtained embeddings for
a sentiment analysis task. The proposed method results in semantic embeddings
and outperforms the state-of-the-art on the paraphrase generation and sentiment
analysis task on standard datasets. These results are also shown to be
statistically significant.
",0,1,0
Domain-agnostic Question-Answering with Adversarial Training,"  Adapting models to new domain without finetuning is a challenging problem in
deep learning. In this paper we utilize an adversarial training framework for
domain generalization in Question Answering (QA) task. Our model consists of a
conventional QA model and a discriminator. The training is performed in the
adversarial manner where the two models constantly compete so that QA model
can learn domain-invariant features. We apply this approach in MRQA Shared Task
2019 and show better performance compared to the baseline model.
",0,1,0
Shallow Attention Network for Polyp Segmentation,"  Accurate polyp segmentation is of great importance for colorectal cancer
diagnosis. However even with a powerful deep neural network there still
exists three big challenges that impede the development of polyp segmentation.
(i) Samples collected under different conditions show inconsistent colors
causing the feature distribution gap and overfitting issue; (ii) Due to
repeated feature downsampling small polyps are easily degraded; (iii)
Foreground and background pixels are imbalanced leading to a biased training.
To address the above issues we propose the Shallow Attention Network (SANet)
for polyp segmentation. Specifically to eliminate the effects of color we
design the color exchange operation to decouple the image contents and colors
and force the model to focus more on the target shape and structure.
Furthermore to enhance the segmentation quality of small polyps we propose
the shallow attention module to filter out the background noise of shallow
features. Thanks to the high resolution of shallow features small polyps can
be preserved correctly. In addition to ease the severe pixel imbalance for
small polyps we propose a probability correction strategy (PCS) during the
inference phase. Note that even though PCS is not involved in the training
phase it can still work well on a biased model and consistently improve the
segmentation performance. Quantitative and qualitative experimental results on
five challenging benchmarks confirm that our proposed SANet outperforms
previous state-of-the-art methods by a large margin and achieves a speed about
72FPS.
",0,0,1
Response-based Distillation for Incremental Object Detection,"  Traditional object detection are ill-equipped for incremental learning.
However fine-tuning directly on a well-trained detection model with only new
data will leads to catastrophic forgetting. Knowledge distillation is a
straightforward way to mitigate catastrophic forgetting. In Incremental Object
Detection (IOD) previous work mainly focuses on feature-level knowledge
distillation but the different response of detector has not been fully
explored yet. In this paper we propose a fully response-based incremental
distillation method focusing on learning response from detection bounding boxes
and classification predictions. Firstly our method transferring category
knowledge while equipping student model with the ability to retain localization
knowledge during incremental learning. In addition we further evaluate the
qualities of all locations and provides valuable response by adaptive
pseudo-label selection (APS) strategies. Finally we elucidate that knowledge
from different responses should be assigned with different importance during
incremental distillation. Extensive experiments conducted on MS COCO
demonstrate significant advantages of our method which substantially narrow
the performance gap towards full training.
",0,0,1
MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software,"  Computer vision is playing an increasingly important role in automated
malware detection with to the rise of the image-based binary representation.
These binary images are fast to generate require no feature engineering and
are resilient to popular obfuscation methods. Significant research has been
conducted in this area however it has been restricted to small-scale or
private datasets that only a few industry labs and research teams have access
to. This lack of availability hinders examination of existing work development
of new research and dissemination of ideas. We introduce MalNet the largest
publicly available cybersecurity image database offering 133x more images and
27x more classes than the only other public binary-image database. MalNet
contains over 1.2 million images across a hierarchy of 47 types and 696
families. We provide extensive analysis of MalNet discussing its properties
and provenance. The scale and diversity of MalNet unlocks new and exciting
cybersecurity opportunities to the computer vision community--enabling
discoveries and research directions that were previously not possible. The
database is publicly available at www.mal-net.org.
",0,0,1
"3DFaceGAN: Adversarial Nets for 3D Face Representation Generation and
  Translation","  Over the past few years Generative Adversarial Networks (GANs) have garnered
increased interest among researchers in Computer Vision with applications
including but not limited to image generation translation imputation and
super-resolution. Nevertheless no GAN-based method has been proposed in the
literature that can successfully represent generate or translate 3D facial
shapes (meshes). This can be primarily attributed to two facts namely that (a)
publicly available 3D face databases are scarce as well as limited in terms of
sample size and variability (e.g. few subjects little diversity in race and
gender) and (b) mesh convolutions for deep networks present several challenges
that are not entirely tackled in the literature leading to operator
approximations and model instability often failing to preserve high-frequency
components of the distribution. As a result linear methods such as Principal
Component Analysis (PCA) have been mainly utilized towards 3D shape analysis
despite being unable to capture non-linearities and high frequency details of
the 3D face - such as eyelid and lip variations. In this work we present
3DFaceGAN the first GAN tailored towards modeling the distribution of 3D
facial surfaces while retaining the high frequency details of 3D face shapes.
We conduct an extensive series of both qualitative and quantitative
experiments where the merits of 3DFaceGAN are clearly demonstrated against
other state-of-the-art methods in tasks such as 3D shape representation
generation and translation.
",0,0,1
SAS: Self-Augmentation Strategy for Language Model Pre-training,"  The core of self-supervised learning for pre-training language models
includes pre-training task design as well as appropriate data augmentation.
Most data augmentations in language model pre-training are context-independent.
A seminal contextualized augmentation was recently proposed in ELECTRA and
achieved state-of-the-art performance by introducing an auxiliary generation
network (generator) to produce contextualized data augmentation for the
training of a main discrimination network (discriminator). This design
however introduces extra computation cost of the generator and a need to
adjust the relative capability between the generator and the discriminator. In
this paper we propose a self-augmentation strategy (SAS) where a single
network is utilized for both regular pre-training and contextualized data
augmentation for the training in later epochs. Essentially this strategy
eliminates a separate generator and uses the single network to jointly conduct
two pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced
Token Detection) heads. It avoids the challenge to search for an appropriate
size of the generator which is critical to the performance as evidenced in
ELECTRA and its subsequent variant models. In addition SAS is a general
strategy that can be seamlessly combined with many new techniques emerging
recently or in the future such as the disentangled attention mechanism from
DeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other
state-of-the-art models in the GLUE tasks with similar or less computation
cost.
",0,1,0
"A proposition of a robust system for historical document images
  indexation","  Characterizing noisy or ancient documents is a challenging problem up to now.
Many techniques have been done in order to effectuate feature extraction and
image indexation for such documents. Global approaches are in general less
robust and exact than local approaches. That's why we propose in this paper a
hybrid system based on global approach(fractal dimension) and a local one
based on SIFT descriptor. The Scale Invariant Feature Transform seems to do
well with our application since it's rotation invariant and relatively robust
to changing illumination.In the first step the calculation of fractal dimension
is applied to images in order to eliminate images which have distant features
than image request characteristics. Next the SIFT is applied to show which
images match well the request. However the average matching time using the
hybrid approach is better than ""fractal dimension"" and ""SIFT descriptor"" if
they are used alone.
",0,0,1
A Multimodal Sentiment Dataset for Video Recommendation,"  Recently multimodal sentiment analysis has seen remarkable advance and a lot
of datasets are proposed for its development. In general current multimodal
sentiment analysis datasets usually follow the traditional system of
sentiment/emotion such as positive negative and so on. However when applied
in the scenario of video recommendation the traditional sentiment/emotion
system is hard to be leveraged to represent different contents of videos in the
perspective of visual senses and language understanding. Based on this we
propose a multimodal sentiment analysis dataset named baiDu Video Sentiment
dataset (DuVideoSenti) and introduce a new sentiment system which is designed
to describe the sentimental style of a video on recommendation scenery.
Specifically DuVideoSenti consists of 5630 videos which displayed on Baidu
each video is manually annotated with a sentimental style label which describes
the user's real feeling of a video. Furthermore we propose UNIMO as our
baseline for DuVideoSenti. Experimental results show that DuVideoSenti brings
new challenges to multimodal sentiment analysis and could be used as a new
benchmark for evaluating approaches designed for video understanding and
multimodal fusion. We also expect our proposed DuVideoSenti could further
improve the development of multimodal sentiment analysis and its application to
video recommendations.
",0,1,0
Effective Strategies for Using Hashtags in Online Communication,"  The features of use of hashtags among students of Lviv were investigated. The
list of optimal strategies for using these communicative tools for personal
branding is determined. The effective strategy for using hashtags in online
communication for the personal and company branding is considered. The results
of calculation of effectiveness of hashtags related to #education is
calculated. The reports of using hashtag #education in social networks is
presented.
",0,1,0
"Relations between Information and Estimation in Discrete-Time L'evy
  Channels","  Fundamental relations between information and estimation have been
established in the literature for the discrete-time Gaussian and Poisson
channels. In this work we demonstrate that such relations hold for a much
larger class of observation models. We introduce the natural family of
discrete-time L'evy channels where the distribution of the output conditioned
on the input is infinitely divisible. For L'evy channels we establish new
representations relating the mutual information between the channel input and
output to an optimal expected estimation loss thereby unifying and
considerably extending results from the Gaussian and Poisson settings. We
demonstrate the richness of our results by working out two examples of L'evy
channels namely the gamma channel and the negative binomial channel with
corresponding relations between information and estimation. Extensions to the
setting of mismatched estimation are also presented.
",1,0,0
"Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of
  Sentence in Video","  In this paper we study the problem of weakly-supervised temporal grounding
of sentence in video. Specifically given an untrimmed video and a query
sentence our goal is to localize a temporal segment in the video that
semantically corresponds to the query sentence with no reliance on any
temporal annotation during training. We propose a two-stage model to tackle
this problem in a coarse-to-fine manner. In the coarse stage we first generate
a set of fixed-length temporal proposals using multi-scale sliding windows and
match their visual features against the sentence features to identify the
best-matched proposal as a coarse grounding result. In the fine stage we
perform a fine-grained matching between the visual features of the frames in
the best-matched proposal and the sentence features to locate the precise frame
boundary of the fine grounding result. Comprehensive experiments on the
ActivityNet Captions dataset and the Charades-STA dataset demonstrate that our
two-stage model achieves compelling performance.
",0,0,1
"Solutions to the Incomplete Toronto Function and Incomplete
  Lipschitz-Hankel Integrals","  This paper provides novel analytic expressions for the incomplete Toronto
function $T_B(mnr)$ and the incomplete Lipschitz-Hankel Integrals of the
modified Bessel function of the first kind $Ie_mn(az)$. These expressions
are expressed in closed-form and are valid for the case that $n$ is an odd
multiple of $1/2$ i.e. $n pm 0.5inmathbbN$. Capitalizing on these tight
upper and lower bounds are subsequently proposed for both $T_B(mnr)$
function and $Ie_mn(az)$ integrals. Importantly all new representations
are expressed in closed-form whilst the proposed bounds are shown to be rather
tight. To this effect they can be effectively exploited in various analytical
studies related to wireless communication theory. Indicative applications
include among others the performance evaluation of digital communications
over fading channels and the information-theoretic analysis of multiple-input
multiple-output systems.
",1,0,0
Part of speech tagging for code switched data,"  We address the problem of Part of Speech tagging (POS) in the context of
linguistic code switching (CS). CS is the phenomenon where a speaker switches
between two languages or variants of the same language within or across
utterances known as intra-sentential or inter-sentential CS respectively.
Processing CS data is especially challenging in intra-sentential data given
state of the art monolingual NLP technology since such technology is geared
toward the processing of one language at a time. In this paper we explore
multiple strategies of applying state of the art POS taggers to CS data. We
investigate the landscape in two CS language pairs Spanish-English and Modern
Standard Arabic-Arabic dialects. We compare the use of two POS taggers vs. a
unified tagger trained on CS data. Our results show that applying a machine
learning framework using two state of the art POS taggers achieves better
performance compared to all other approaches that we investigate.
",0,1,0
Bounds on the Information Divergence for Hypergeometric Distributions,"  The hypergeometric distributions have many important applications but they
have not had sufficient attention in information theory. Hypergeometric
distributions can be approximated by binomial distributions or Poisson
distributions. In this paper we present upper and lower bounds on information
divergence. These bounds are important for statistical testing and a better
understanding of the notion of exchange-ability.
",1,0,0
"Toward Ergonomic Risk Prediction via Segmentation of Indoor Object
  Manipulation Actions Using Spatiotemporal Convolutional Networks","  Automated real-time prediction of the ergonomic risks of manipulating objects
is a key unsolved challenge in developing effective human-robot collaboration
systems for logistics and manufacturing applications. We present a foundational
paradigm to address this challenge by formulating the problem as one of action
segmentation from RGB-D camera videos. Spatial features are first learned using
a deep convolutional model from the video frames which are then fed
sequentially to temporal convolutional networks to semantically segment the
frames into a hierarchy of actions which are either ergonomically safe
require monitoring or need immediate attention. For performance evaluation in
addition to an open-source kitchen dataset we collected a new dataset
comprising twenty individuals picking up and placing objects of varying weights
to and from cabinet and table locations at various heights. Results show very
high (87-94)% F1 overlap scores among the ground truth and predicted frame
labels for videos lasting over two minutes and consisting of a large number of
actions.
",0,0,1
Improving Human Activity Recognition Through Ranking and Re-ranking,"  We propose two well-motivated ranking-based methods to enhance the
performance of current state-of-the-art human activity recognition systems.
First as an improvement over the classic power normalization method we
propose a parameter-free ranking technique called rank normalization (RaN). RaN
normalizes each dimension of the video features to address the sparse and
bursty distribution problems of Fisher Vectors and VLAD. Second inspired by
curriculum learning we introduce a training-free re-ranking technique called
multi-class iterative re-ranking (MIR). MIR captures relationships among action
classes by separating easy and typical videos from difficult ones and
re-ranking the prediction scores of classifiers accordingly. We demonstrate
that our methods significantly improve the performance of state-of-the-art
motion features on six real-world datasets.
",0,0,1
Complete Multilingual Neural Machine Translation,"  Multilingual Neural Machine Translation (MNMT) models are commonly trained on
a joint set of bilingual corpora which is acutely English-centric (i.e. English
either as the source or target language). While direct data between two
languages that are non-English is explicitly available at times its use is not
common. In this paper we first take a step back and look at the commonly used
bilingual corpora (WMT) and resurface the existence and importance of implicit
structure that existed in it: multi-way alignment across examples (the same
sentence in more than two languages). We set out to study the use of multi-way
aligned examples to enrich the original English-centric parallel corpora. We
reintroduce this direct parallel data from multi-way aligned corpora between
all source and target languages. By doing so the English-centric graph expands
into a complete graph every language pair being connected. We call MNMT with
such connectivity pattern complete Multilingual Neural Machine Translation
(cMNMT) and demonstrate its utility and efficacy with a series of experiments
and analysis. In combination with a novel training data sampling strategy that
is conditioned on the target language only cMNMT yields competitive
translation quality for all language pairs. We further study the size effect of
multi-way aligned data its transfer learning capabilities and how it eases
adding a new language in MNMT. Finally we stress test cMNMT at scale and
demonstrate that we can train a cMNMT model with up to 111*112=12432 language
pairs that provides competitive translation quality for all language pairs.
",0,1,0
"Probing the Natural Language Inference Task with Automated Reasoning
  Tools","  The Natural Language Inference (NLI) task is an important task in modern NLP
as it asks a broad question to which many other tasks may be reducible: Given a
pair of sentences does the first entail the second? Although the
state-of-the-art on current benchmark datasets for NLI are deep learning-based
it is worthwhile to use other techniques to examine the logical structure of
the NLI task. We do so by testing how well a machine-oriented controlled
natural language (Attempto Controlled English) can be used to parse NLI
sentences and how well automated theorem provers can reason over the resulting
formulae. To improve performance we develop a set of syntactic and semantic
transformation rules. We report their performance and discuss implications for
NLI and logic-based NLP.
",0,1,0
"Grape detection segmentation and tracking using deep neural networks
  and three-dimensional association","  Agricultural applications such as yield prediction precision agriculture and
automated harvesting need systems able to infer the crop state from low-cost
sensing devices. Proximal sensing using affordable cameras combined with
computer vision has seen a promising alternative strengthened after the advent
of convolutional neural networks (CNNs) as an alternative for challenging
pattern recognition problems in natural images. Considering fruit growing
monitoring and automation a fundamental problem is the detection segmentation
and counting of individual fruits in orchards. Here we show that for wine
grapes a crop presenting large variability in shape color size and
compactness grape clusters can be successfully detected segmented and tracked
using state-of-the-art CNNs. In a test set containing 408 grape clusters from
images taken on a trellis-system based vineyard we have reached an F 1 -score
up to 0.91 for instance segmentation a fine separation of each cluster from
other structures in the image that allows a more accurate assessment of fruit
size and shape. We have also shown as clusters can be identified and tracked
along video sequences recording orchard rows. We also present a public dataset
containing grape clusters properly annotated in 300 images and a novel
annotation methodology for segmentation of complex objects in natural images.
The presented pipeline for annotation training evaluation and tracking of
agricultural patterns in images can be replicated for different crops and
production systems. It can be employed in the development of sensing components
for several agricultural and environmental applications.
",0,0,1
"Joint Generative and Contrastive Learning for Unsupervised Person
  Re-identification","  Recent self-supervised contrastive learning provides an effective approach
for unsupervised person re-identification (ReID) by learning invariance from
different views (transformed versions) of an input. In this paper we
incorporate a Generative Adversarial Network (GAN) and a contrastive learning
module into one joint training framework. While the GAN provides online data
augmentation for contrastive learning the contrastive module learns
view-invariant features for generation. In this context we propose a
mesh-based view generator. Specifically mesh projections serve as references
towards generating novel views of a person. In addition we propose a
view-invariant loss to facilitate contrastive learning between original and
generated views. Deviating from previous GAN-based unsupervised ReID methods
involving domain adaptation we do not rely on a labeled source dataset which
makes our method more flexible. Extensive experimental results show that our
method significantly outperforms state-of-the-art methods under both fully
unsupervised and unsupervised domain adaptive settings on several large scale
ReID datsets.
",0,0,1
"Scalable and Robust Sparse Subspace Clustering Using Randomized
  Clustering and Multilayer Graphs","  Sparse subspace clustering (SSC) is one of the current state-of-the-art
methods for partitioning data points into the union of subspaces with strong
theoretical guarantees. However it is not practical for large data sets as it
requires solving a LASSO problem for each data point where the number of
variables in each LASSO problem is the number of data points. To improve the
scalability of SSC we propose to select a few sets of anchor points using a
randomized hierarchical clustering method and for each set of anchor points
solve the LASSO problems for each data point allowing only anchor points to
have a non-zero weight (this reduces drastically the number of variables). This
generates a multilayer graph where each layer corresponds to a different set of
anchor points. Using the Grassmann manifold of orthogonal matrices the shared
connectivity among the layers is summarized within a single subspace. Finally
we use $k$-means clustering within that subspace to cluster the data points
similarly as done by spectral clustering in SSC. We show on both synthetic and
real-world data sets that the proposed method not only allows SSC to scale to
large-scale data sets but that it is also much more robust as it performs
significantly better on noisy data and on data with close susbspaces and
outliers while it is not prone to oversegmentation.
",0,0,1
CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue,"  Overconfident predictions on out-of-distribution (OOD) samples is a thorny
issue for deep neural networks. The key to resolve the OOD overconfidence issue
inherently is to build a subset of OOD samples and then suppress predictions on
them. This paper proposes the Chamfer OOD examples (CODEs) whose distribution
is close to that of in-distribution samples and thus could be utilized to
alleviate the OOD overconfidence issue effectively by suppressing predictions
on them. To obtain CODEs we first generate seed OOD examples via
slicing&splicing operations on in-distribution samples from different
categories and then feed them to the Chamfer generative adversarial network
for distribution transformation without accessing to any extra data. Training
with suppressing predictions on CODEs is validated to alleviate the OOD
overconfidence issue largely without hurting classification accuracy and
outperform the state-of-the-art methods. Besides we demonstrate CODEs are
useful for improving OOD detection and classification.
",0,0,1
Comparison of Channels: Criteria for Domination by a Symmetric Channel,"  This paper studies the basic question of whether a given channel $V$ can be
dominated (in the precise sense of being more noisy) by a $q$-ary symmetric
channel. The concept of ""less noisy"" relation between channels originated in
network information theory (broadcast channels) and is defined in terms of
mutual information or Kullback-Leibler divergence. We provide an equivalent
characterization in terms of $chi^2$-divergence. Furthermore we develop a
simple criterion for domination by a $q$-ary symmetric channel in terms of the
minimum entry of the stochastic matrix defining the channel $V$. The criterion
is strengthened for the special case of additive noise channels over finite
Abelian groups. Finally it is shown that domination by a symmetric channel
implies (via comparison of Dirichlet forms) a logarithmic Sobolev inequality
for the original channel.
",1,0,0
Fast camera focus estimation for gaze-based focus control,"  Many cameras implement auto-focus functionality. However they typically
require the user to manually identify the location to be focused on. While such
an approach works for temporally-sparse autofocusing functionality (e.g. photo
shooting) it presents extreme usability problems when the focus must be
quickly switched between multiple areas (and depths) of interest - e.g. in a
gaze-based autofocus approach. This work introduces a novel real-time
auto-focus approach based on eye-tracking which enables the user to shift the
camera focus plane swiftly based solely on the gaze information. Moreover the
proposed approach builds a graph representation of the image to estimate depth
plane surfaces and runs in real time (requiring ~20ms on a single i5 core)
thus allowing for the depth map estimation to be performed dynamically. We
evaluated our algorithm for gaze-based depth estimation against
state-of-the-art approaches based on eight new data sets with flat skewed and
round surfaces as well as publicly available datasets.
",0,0,1
"Multiple instance dense connected convolution neural network for aerial
  image scene classification","  With the development of deep learning many state-of-the-art natural image
scene classification methods have demonstrated impressive performance. While
the current convolution neural network tends to extract global features and
global semantic information in a scene the geo-spatial objects can be located
at anywhere in an aerial image scene and their spatial arrangement tends to be
more complicated. One possible solution is to preserve more local semantic
information and enhance feature propagation. In this paper an end to end
multiple instance dense connected convolution neural network (MIDCCNN) is
proposed for aerial image scene classification. First a 23 layer dense
connected convolution neural network (DCCNN) is built and served as a backbone
to extract convolution features. It is capable of preserving middle and low
level convolution features. Then an attention based multiple instance pooling
is proposed to highlight the local semantics in an aerial image scene. Finally
we minimize the loss between the bag-level predictions and the ground truth
labels so that the whole framework can be trained directly. Experiments on
three aerial image datasets demonstrate that our proposed methods can
outperform current baselines by a large margin.
",0,0,1
A partition of the hypercube into maximally nonparallel Hamming codes,"  By using the Gold map we construct a partition of the hypercube into cosets
of Hamming codes such that for every two cosets the corresponding Hamming codes
are maximally nonparallel that is their intersection cardinality is as small
as possible to admit nonintersecting cosets.
",1,0,0
"Towards Optimisation of Collaborative Question Answering over Knowledge
  Graphs","  Collaborative Question Answering (CQA) frameworks for knowledge graphs aim at
integrating existing question answering (QA) components for implementing
sequences of QA tasks (i.e. QA pipelines). The research community has paid
substantial attention to CQAs since they support reusability and scalability of
the available components in addition to the flexibility of pipelines. CQA
frameworks attempt to build such pipelines automatically by solving two
optimisation problems: 1) local collective performance of QA components per QA
task and 2) global performance of QA pipelines. In spite offering several
advantages over monolithic QA systems the effectiveness and efficiency of CQA
frameworks in answering questions is limited. In this paper we tackle the
problem of local optimisation of CQA frameworks and propose a three fold
approach which applies feature selection techniques with supervised machine
learning approaches in order to identify the best performing components
efficiently. We have empirically evaluated our approach over existing
benchmarks and compared to existing automatic CQA frameworks. The observed
results provide evidence that our approach answers a higher number of questions
than the state of the art while reducing: i) the number of used features by 50%
and ii) the number of components used by 76%.
",0,1,0
"Further Applications of Wireless Communication Systems over
  $alpha-eta-kappa-mu$ Fading Channels","  In this letter some applications of wireless communication systems over
$alpha-eta-kappa-mu$ fading channels are analysed. More specifically the
effective rate and the average of both the detection probability and area under
the receiver characteristics curve of energy detection based spectrum sensing
are studied. Furthermore highly accurate method to truncating the infinite
summation of the probability density function of $alpha-eta-kappa-mu$
fading conditions for a certain number of terms is provided. To this end novel
mathematically tractable exact expressions are derived in terms of the Fox's
$H$-function (FHF). The asymptotic behaviour is also explained to earn further
insights into the effect of the fading parameters on the system performance. A
comparison between the numerical results and Monte Carlo simulations is
presented to verify the validation of our analysis.
",1,0,0
On Scaling Laws of Diversity Schemes in Decentralized Estimation,"  This paper is concerned with decentralized estimation of a Gaussian source
using multiple sensors. We consider a diversity scheme where only the sensor
with the best channel sends their measurements over a fading channel to a
fusion center using the analog amplify and forwarding technique. The fusion
centre reconstructs an MMSE estimate of the source based on the received
measurements. A distributed version of the diversity scheme where sensors
decide whether to transmit based only on their local channel information is
also considered. We derive asymptotic expressions for the expected distortion
(of the MMSE estimate at the fusion centre) of these schemes as the number of
sensors becomes large. For comparison asymptotic expressions for the expected
distortion for a coherent multi-access scheme and an orthogonal access scheme
are derived. We also study for the diversity schemes the optimal power
allocation for minimizing the expected distortion subject to average total
power constraints. The effect of optimizing the probability of transmission on
the expected distortion in the distributed scenario is also studied. It is seen
that as opposed to the coherent multi-access scheme and the orthogonal scheme
(where the expected distortion decays as 1/M M being the number of sensors)
the expected distortion decays only as 1/ln(M) for the diversity schemes. This
reduction of the decay rate can be seen as a tradeoff between the simplicity of
the diversity schemes and the strict synchronization and large bandwidth
requirements for the coherent multi-access and the orthogonal schemes
respectively.
",1,0,0
Hierarchical Transformer for Task Oriented Dialog Systems,"  Generative models for dialog systems have gained much interest because of the
recent success of RNN and Transformer based models in tasks like question
answering and summarization. Although the task of dialog response generation is
generally seen as a sequence-to-sequence (Seq2Seq) problem researchers in the
past have found it challenging to train dialog systems using the standard
Seq2Seq models. Therefore to help the model learn meaningful utterance and
conversation level features Sordoni et al. (2015b); Serban et al. (2016)
proposed Hierarchical RNN architecture which was later adopted by several
other RNN based dialog systems. With the transformer-based models dominating
the seq2seq problems lately the natural question to ask is the applicability
of the notion of hierarchy in transformer based dialog systems. In this paper
we propose a generalized framework for Hierarchical Transformer Encoders and
show how a standard transformer can be morphed into any hierarchical encoder
including HRED and HIBERT like models by using specially designed attention
masks and positional encodings. We demonstrate that Hierarchical Encoding helps
achieve better natural language understanding of the contexts in
transformer-based models for task-oriented dialog systems through a wide range
of experiments.
",0,1,0
PANDA: Pose Aligned Networks for Deep Attribute Modeling,"  We propose a method for inferring human attributes (such as gender hair
style clothes style expression action) from images of people under large
variation of viewpoint pose appearance articulation and occlusion.
Convolutional Neural Nets (CNN) have been shown to perform very well on large
scale object recognition problems. In the context of attribute classification
however the signal is often subtle and it may cover only a small part of the
image while the image is dominated by the effects of pose and viewpoint.
Discounting for pose variation would require training on very large labeled
datasets which are not presently available. Part-based models such as poselets
and DPM have been shown to perform well for this problem but they are limited
by shallow low-level features. We propose a new method which combines
part-based models and deep learning by training pose-normalized CNNs. We show
substantial improvement vs. state-of-the-art methods on challenging attribute
classification tasks in unconstrained settings. Experiments confirm that our
method outperforms both the best part-based methods on this problem and
conventional CNNs trained on the full bounding box of the person.
",0,0,1
"Kaizen: Continuously improving teacher using Exponential Moving Average
  for semi-supervised speech recognition","  In this paper we introduce the Kaizen framework that uses a continuously
improving teacher to generate pseudo-labels for semi-supervised speech
recognition (ASR). The proposed approach uses a teacher model which is updated
as the exponential moving average (EMA) of the student model parameters. We
demonstrate that it is critical for EMA to be accumulated with full-precision
floating point. The Kaizen framework can be seen as a continuous version of the
iterative pseudo-labeling approach for semi-supervised training. It is
applicable for different training criteria and in this paper we demonstrate
its effectiveness for frame-level hybrid hidden Markov model-deep neural
network (HMM-DNN) systems as well as sequence-level Connectionist Temporal
Classification (CTC) based models.
  For large scale real-world unsupervised public videos in UK English and
Italian languages the proposed approach i) shows more than 10% relative word
error rate (WER) reduction over standard teacher-student training; ii) using
just 10 hours of supervised data and a large amount of unsupervised data closes
the gap to the upper-bound supervised ASR system that uses 650h or 2700h
respectively.
",0,1,0
Visual Question Answering on Image Sets,"  We introduce the task of Image-Set Visual Question Answering (ISVQA) which
generalizes the commonly studied single-image VQA problem to multi-image
settings. Taking a natural language question and a set of images as input it
aims to answer the question based on the content of the images. The questions
can be about objects and relationships in one or more images or about the
entire scene depicted by the image set. To enable research in this new topic
we introduce two ISVQA datasets - indoor and outdoor scenes. They simulate the
real-world scenarios of indoor image collections and multiple car-mounted
cameras respectively. The indoor-scene dataset contains 91479 human annotated
questions for 48138 image sets and the outdoor-scene dataset has 49617
questions for 12746 image sets. We analyze the properties of the two datasets
including question-and-answer distributions types of questions biases in
dataset and question-image dependencies. We also build new baseline models to
investigate new research challenges in ISVQA.
",0,0,1
"Video action detection by learning graph-based spatio-temporal
  interactions","  Action Detection is a complex task that aims to detect and classify human
actions in video clips. Typically it has been addressed by processing
fine-grained features extracted from a video classification backbone. Recently
thanks to the robustness of object and people detectors a deeper focus has
been added on relationship modelling. Following this line we propose a
graph-based framework to learn high-level interactions between people and
objects in both space and time. In our formulation spatio-temporal
relationships are learned through self-attention on a multi-layer graph
structure which can connect entities from consecutive clips thus considering
long-range spatial and temporal dependencies. The proposed module is backbone
independent by design and does not require end-to-end training. Extensive
experiments are conducted on the AVA dataset where our model demonstrates
state-of-the-art results and consistent improvements over baselines built with
different backbones. Code is publicly available at
https://github.com/aimagelab/STAGE_action_detection.
",0,0,1
XtremeDistil: Multi-stage Distillation for Massive Multilingual Models,"  Deep and large pre-trained language models are the state-of-the-art for
various natural language processing tasks. However the huge size of these
models could be a deterrent to use them in practice. Some recent and concurrent
works use knowledge distillation to compress these huge models into shallow
ones. In this work we study knowledge distillation with a focus on
multi-lingual Named Entity Recognition (NER). In particular we study several
distillation strategies and propose a stage-wise optimization scheme leveraging
teacher internal representations that is agnostic of teacher architecture and
show that it outperforms strategies employed in prior works. Additionally we
investigate the role of several factors like the amount of unlabeled data
annotation resources model architecture and inference latency to name a few.
We show that our approach leads to massive compression of MBERT-like teacher
models by upto 35x in terms of parameters and 51x in terms of latency for batch
inference while retaining 95% of its F1-score for NER over 41 languages.
",0,1,0
"Beyond Trade-off: Accelerate FCN-based Face Detector with Higher
  Accuracy","  Fully convolutional neural network (FCN) has been dominating the game of face
detection task for a few years with its congenital capability of
sliding-window-searching with shared kernels which boiled down all the
redundant calculation and most recent state-of-the-art methods such as
Faster-RCNN SSD YOLO and FPN use FCN as their backbone. So here comes one
question: Can we find a universal strategy to further accelerate FCN with
higher accuracy so could accelerate all the recent FCN-based methods? To
analyze this we decompose the face searching space into two orthogonal
directions `scale' and `spatial'. Only a few coordinates in the space expanded
by the two base vectors indicate foreground. So if FCN could ignore most of the
other points the searching space and false alarm should be significantly
boiled down. Based on this philosophy a novel method named scale estimation
and spatial attention proposal ($S^2AP$) is proposed to pay attention to some
specific scales and valid locations in the image pyramid. Furthermore we adopt
a masked-convolution operation based on the attention result to accelerate FCN
calculation. Experiments show that FCN-based method RPN can be accelerated by
about $4times$ with the help of $S^2AP$ and masked-FCN and at the same time it
can also achieve the state-of-the-art on FDDB AFW and MALF face detection
benchmarks as well.
",0,0,1
Dynamic Scene Deblurring using a Locally Adaptive Linear Blur Model,"  State-of-the-art video deblurring methods cannot handle blurry videos
recorded in dynamic scenes since they are built under a strong assumption that
the captured scenes are static. Contrary to the existing methods we propose a
video deblurring algorithm that can deal with general blurs inherent in dynamic
scenes. To handle general and locally varying blurs caused by various sources
such as moving objects camera shake depth variation and defocus we estimate
pixel-wise non-uniform blur kernels. We infer bidirectional optical flows to
handle motion blurs and also estimate Gaussian blur maps to remove optical
blur from defocus in our new blur model. Therefore we propose a single energy
model that jointly estimates optical flows defocus blur maps and latent
frames. We also provide a framework and efficient solvers to minimize the
proposed energy model. By optimizing the energy model we achieve significant
improvements in removing general blurs estimating optical flows and extending
depth-of-field in blurry frames. Moreover in this work to evaluate the
performance of non-uniform deblurring methods objectively we have constructed
a new realistic dataset with ground truths. In addition extensive experimental
on publicly available challenging video data demonstrate that the proposed
method produces qualitatively superior performance than the state-of-the-art
methods which often fail in either deblurring or optical flow estimation.
",0,0,1
"Second-order Co-occurrence Sensitivity of Skip-Gram with Negative
  Sampling","  We simulate first- and second-order context overlap and show that Skip-Gram
with Negative Sampling is similar to Singular Value Decomposition in capturing
second-order co-occurrence information while Pointwise Mutual Information is
agnostic to it. We support the results with an empirical study finding that the
models react differently when provided with additional second-order
information. Our findings reveal a basic property of Skip-Gram with Negative
Sampling and point towards an explanation of its success on a variety of tasks.
",0,1,0
Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework,"  Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years
where neural methods became capable of producing audios with high naturalness.
However these efforts still suffer from two types of latencies: (a) the em
computational latency (synthesizing time) which grows linearly with the
sentence length even with parallel approaches and (b) the em input latency
in scenarios where the input text is incrementally generated (such as in
simultaneous translation dialog generation and assistive technologies). To
reduce these latencies we devise the first neural incremental TTS approach
based on the recently proposed prefix-to-prefix framework. We synthesize speech
in an online fashion playing a segment of audio while generating the next
resulting in an $O(1)$ rather than $O(n)$ latency.
",0,1,0
Online Knowledge Distillation via Multi-branch Diversity Enhancement,"  Knowledge distillation is an effective method to transfer the knowledge from
the cumbersome teacher model to the lightweight student model. Online knowledge
distillation uses the ensembled prediction results of multiple student models
as soft targets to train each student model. However the homogenization
problem will lead to difficulty in further improving model performance. In this
work we propose a new distillation method to enhance the diversity among
multiple student models. We introduce Feature Fusion Module (FFM) which
improves the performance of the attention mechanism in the network by
integrating rich semantic information contained in the last block of multiple
student models. Furthermore we use the Classifier Diversification(CD) loss
function to strengthen the differences between the student models and deliver a
better ensemble result. Extensive experiments proved that our method
significantly enhances the diversity among student models and brings better
distillation performance. We evaluate our method on three image classification
datasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves
state-of-the-art performance on these datasets.
",0,0,1
"Flood severity mapping from Volunteered Geographic Information by
  interpreting water level from images containing people: a case study of
  Hurricane Harvey","  With increasing urbanization in recent years there has been a growing
interest and need in monitoring and analyzing urban flood events. Social media
as a new data source can provide real-time information for flood monitoring.
The social media posts with locations are often referred to as Volunteered
Geographic Information (VGI) which can reveal the spatial pattern of such
events. Since more images are shared on social media than ever before recent
research focused on the extraction of flood-related posts by analyzing images
in addition to texts. Apart from merely classifying posts as flood relevant or
not more detailed information e.g. the flood severity can also be extracted
based on image interpretation. However it has been less tackled and has not
yet been applied for flood severity mapping.
  In this paper we propose a novel three-step process to extract and map flood
severity information. First flood relevant images are retrieved with the help
of pre-trained convolutional neural networks as feature extractors. Second the
images containing people are further classified into four severity levels by
observing the relationship between body parts and their partial inundation
i.e. images are classified according to the water level with respect to
different body parts namely ankle knee hip and chest. Lastly locations of
the Tweets are used for generating a map of estimated flood extent and
severity. This process was applied to an image dataset collected during
Hurricane Harvey in 2017 as a proof of concept. The results show that VGI can
be used as a supplement to remote sensing observations for flood extent mapping
and is beneficial especially for urban areas where the infrastructure is
often occluding water. Based on the extracted water level information an
integrated overview of flood severity can be provided for the early stages of
emergency response.
",0,0,1
Binary Cyclic codes with two primitive nonzeros,"  In this paper we make some progress towards a well-known conjecture on the
minimum weights of binary cyclic codes with two primitive nonzeros. We also
determine the Walsh spectrum of $Tr(x^d)$ over $F_2^m$ in the case where
$m=2t$ $d=3+2^t+1$ and $gcd(d 2^m-1)=1$.
",1,0,0
MATE-KD: Masked Adversarial TExt a Companion to Knowledge Distillation,"  The advent of large pre-trained language models has given rise to rapid
progress in the field of Natural Language Processing (NLP). While the
performance of these models on standard benchmarks has scaled with size
compression techniques such as knowledge distillation have been key in making
them practical. We present MATE-KD a novel text-based adversarial training
algorithm which improves the performance of knowledge distillation. MATE-KD
first trains a masked language model based generator to perturb text by
maximizing the divergence between teacher and student logits. Then using
knowledge distillation a student is trained on both the original and the
perturbed training samples. We evaluate our algorithm using BERT-based models
on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive
adversarial learning and data augmentation baselines. On the GLUE test set our
6 layer RoBERTa based model outperforms BERT-Large.
",0,1,0
Few Shot Learning With No Labels,"  Few-shot learners aim to recognize new categories given only a small number
of training samples. The core challenge is to avoid overfitting to the limited
data while ensuring good generalization to novel classes. Existing literature
makes use of vast amounts of annotated data by simply shifting the label
requirement from novel classes to base classes. Since data annotation is
time-consuming and costly reducing the label requirement even further is an
important goal. To that end our paper presents a more challenging few-shot
setting where no label access is allowed during training or testing. By
leveraging self-supervision for learning image representations and image
similarity for classification at test time we achieve competitive baselines
while using textbfzero labels which is at least fewer labels than
state-of-the-art. We hope that this work is a step towards developing few-shot
learning methods which do not depend on annotated data at all. Our code will be
publicly released.
",0,0,1
"On the Non-existence of certain classes of perfect p-ary sequences and
  perfect almost p-ary sequences","  We obtain new non-existence results of perfect p-ary sequences with period n
(called type $[p n]$). The first case is a class with type [pequiv5pmod
8p^aqn']. The second case contains five types [pequiv3pmod 4p^aq^ln'] for
certain $p q$ and $l$. Moreover we also have similar non-existence results
for perfect almost p-ary sequences.
",1,0,0
A Tree Search Algorithm for Sequence Labeling,"  In this paper we propose a novel reinforcement learning based model for
sequence tagging referred to as MM-Tag. Inspired by the success and
methodology of the AlphaGo Zero MM-Tag formalizes the problem of sequence
tagging with a Monte Carlo tree search (MCTS) enhanced Markov decision process
(MDP) model in which the time steps correspond to the positions of words in a
sentence from left to right and each action corresponds to assign a tag to a
word. Two long short-term memory networks (LSTM) are used to summarize the past
tag assignments and words in the sentence. Based on the outputs of LSTMs the
policy for guiding the tag assignment and the value for predicting the whole
tagging accuracy of the whole sentence are produced. The policy and value are
then strengthened with MCTS which takes the produced raw policy and value as
inputs simulates and evaluates the possible tag assignments at the subsequent
positions and outputs a better search policy for assigning tags. A
reinforcement learning algorithm is proposed to train the model parameters. Our
work is the first to apply the MCTS enhanced MDP model to the sequence tagging
task. We show that MM-Tag can accurately predict the tags thanks to the
exploratory decision making mechanism introduced by MCTS. Experimental results
show based on a chunking benchmark showed that MM-Tag outperformed the
state-of-the-art sequence tagging baselines including CRF and CRF with LSTM.
",0,1,0
"Robust Online Video Super-Resolution Using an Efficient Alternating
  Projections Scheme","  Video super-resolution reconstruction (SRR) algorithms attempt to reconstruct
high-resolution (HR) video sequences from low-resolution observations. Although
recent progress in video SRR has significantly improved the quality of the
reconstructed HR sequences it remains challenging to design SRR algorithms
that achieve good quality and robustness at a small computational complexity
being thus suitable for online applications. In this paper we propose a new
adaptive video SRR algorithm that achieves state-of-the-art performance at a
very small computational cost. Using a nonlinear cost function constructed
considering characteristics of typical innovation outliers in natural image
sequences and an edge-preserving regularization strategy we achieve
state-of-the-art reconstructed image quality and robustness. This cost function
is optimized using a specific alternating projections strategy over non-convex
sets that is able to converge in a very few iterations. An accurate and very
efficient approximation for the projection operations is also obtained using
tools from multidimensional multirate signal processing. This solves the slow
convergence issue of stochastic gradient-based methods while keeping a small
computational complexity. Simulation results with both synthetic and real image
sequences show that the performance of the proposed algorithm is similar or
better than state-of-the-art SRR algorithms while requiring only a small
fraction of their computational cost.
",0,0,1
"Hypergraph-Based Analysis of Clustered Cooperative Beamforming with
  Application to Edge Caching","  The evaluation of the performance of clustered cooperative beamforming in
cellular networks generally requires the solution of complex non-convex
optimization problems. In this letter a framework based on a hypergraph
formalism is proposed that enables the derivation of a performance
characterization of clustered cooperative beamforming in terms of per-user
degrees of freedom (DoF) via the efficient solution of a coloring problem. An
emerging scenario in which clusters of cooperative base stations (BSs) arise is
given by cellular networks with edge caching. In fact clusters of BSs that
share the same requested files can jointly beamform the corresponding encoded
signals. Based on this observation the proposed framework is applied to obtain
quantitative insights into the optimal use of cache and backhaul resources in
cellular systems with edge caching. Numerical examples are provided to
illustrate the merits of the proposed framework.
",1,0,0
"PR2: A Language Independent Unsupervised Tool for Personality
  Recognition from Text","  We present PR2 a personality recognition system available online that
performs instance-based classification of Big5 personality types from
unstructured text using language-independent features. It has been tested on
English and Italian achieving performances up to f=.68.
",0,1,0
"Progressive Domain-Independent Feature Decomposition Network for
  Zero-Shot Sketch-Based Image Retrieval","  Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal
retrieval task for searching natural images given free-hand sketches under the
zero-shot scenario. Most existing methods solve this problem by simultaneously
projecting visual features and semantic supervision into a low-dimensional
common space for efficient retrieval. However such low-dimensional projection
destroys the completeness of semantic knowledge in original semantic space so
that it is unable to transfer useful knowledge well when learning semantic from
different modalities. Moreover the domain information and semantic information
are entangled in visual features which is not conducive for cross-modal
matching since it will hinder the reduction of domain gap between sketch and
image. In this paper we propose a Progressive Domain-independent Feature
Decomposition (PDFD) network for ZS-SBIR. Specifically with the supervision of
original semantic knowledge PDFD decomposes visual features into domain
features and semantic ones and then the semantic features are projected into
common space as retrieval features for ZS-SBIR. The progressive projection
strategy maintains strong semantic supervision. Besides to guarantee the
retrieval features to capture clean and complete semantic information the
cross-reconstruction loss is introduced to encourage that any combinations of
retrieval features and domain features can reconstruct the visual features.
Extensive experiments demonstrate the superiority of our PDFD over
state-of-the-art competitors.
",0,0,1
Compressing the Input for CNNs with the First-Order Scattering Transform,"  We study the first-order scattering transform as a candidate for reducing the
signal processed by a convolutional neural network (CNN). We show theoretical
and empirical evidence that in the case of natural images and sufficiently
small translation invariance this transform preserves most of the signal
information needed for classification while substantially reducing the spatial
resolution and total signal size. We demonstrate that cascading a CNN with this
representation performs on par with ImageNet classification models commonly
used in downstream tasks such as the ResNet-50. We subsequently apply our
trained hybrid ImageNet model as a base model on a detection system which has
typically larger image inputs. On Pascal VOC and COCO detection tasks we
demonstrate improvements in the inference speed and training memory consumption
compared to models trained directly on the input image.
",0,0,1
Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks,"  We present an approach that exploits hierarchical Recurrent Neural Networks
(RNNs) to tackle the video captioning problem i.e. generating one or multiple
sentences to describe a realistic video. Our hierarchical framework contains a
sentence generator and a paragraph generator. The sentence generator produces
one simple short sentence that describes a specific short video interval. It
exploits both temporal- and spatial-attention mechanisms to selectively focus
on visual elements during generation. The paragraph generator captures the
inter-sentence dependency by taking as input the sentential embedding produced
by the sentence generator combining it with the paragraph history and
outputting the new initial state for the sentence generator. We evaluate our
approach on two large-scale benchmark datasets: YouTubeClips and
TACoS-MultiLevel. The experiments demonstrate that our approach significantly
outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and
0.305 respectively.
",0,0,1
"A framework for quantitative analysis of Computed Tomography images of
  viral pneumonitis: radiomic features in COVID and non-COVID patients","  Purpose: to optimize a pipeline of clinical data gathering and CT images
processing implemented during the COVID-19 pandemic crisis and to develop
artificial intelligence model for different of viral pneumonia. Methods: 1028
chest CT image of patients with positive swab were segmented automatically for
lung extraction. A Gaussian model developed in Python language was applied to
calculate quantitative metrics (QM) describing well-aerated and ill portions of
the lungs from the histogram distribution of lung CT numbers in both lungs of
each image and in four geometrical subdivision. Furthermore radiomic features
(RF) of first and second order were extracted from bilateral lungs using
PyRadiomic tools. QM and RF were used to develop 4 different Multi-Layer
Perceptron (MLP) classifier to discriminate images of patients with COVID
(n=646) and non-COVID (n=382) viral pneumonia. Results: The Gaussian model
applied to lung CT histogram correctly described healthy parenchyma 94% of the
patients. The resulting accuracy of the models for COVID diagnosis were in the
range 0.76-0.87 as the integral of the receiver operating curve. The best
diagnostic performances were associated to the model based on RF of first and
second order with 21 relevant features after LASSO regression and an accuracy
of 0.81$pm$0.02 after 4-fold cross validation Conclusions: Despite these
results were obtained with CT images from a single center a platform for
extracting useful quantitative metrics from CT images was developed and
optimized. Four artificial intelligence-based models for classifying patients
with COVID and non-COVID viral pneumonia were developed and compared showing
overall good diagnostic performances
",0,0,1
"Augmenting the Pathology Lab: An Intelligent Whole Slide Image
  Classification System for the Real World","  Standard of care diagnostic procedure for suspected skin cancer is
microscopic examination of hematoxylin & eosin stained tissue by a
pathologist. Areas of high inter-pathologist discordance and rising biopsy
rates necessitate higher efficiency and diagnostic reproducibility. We present
and validate a deep learning system which classifies digitized dermatopathology
slides into 4 categories. The system is developed using 5070 images from a
single lab and tested on an uncurated set of 13537 images from 3 test labs
using whole slide scanners manufactured by 3 different vendors. The system's
use of deep-learning-based confidence scoring as a criterion to consider the
result as accurate yields an accuracy of up to 98% and makes it adoptable in
a real-world setting. Without confidence scoring the system achieved an
accuracy of 78%. We anticipate that our deep learning system will serve as a
foundation enabling faster diagnosis of skin cancer identification of cases
for specialist review and targeted diagnostic classifications.
",0,0,1
"Augmented Reality-based Feedback for Technician-in-the-loop C-arm
  Repositioning","  Interventional C-arm imaging is crucial to percutaneous orthopedic procedures
as it enables the surgeon to monitor the progress of surgery on the anatomy
level. Minimally invasive interventions require repeated acquisition of X-ray
images from different anatomical views to verify tool placement. Achieving and
reproducing these views often comes at the cost of increased surgical time and
radiation dose to both patient and staff. This work proposes a marker-free
""technician-in-the-loop"" Augmented Reality (AR) solution for C-arm
repositioning. The X-ray technician operating the C-arm interventionally is
equipped with a head-mounted display capable of recording desired C-arm poses
in 3D via an integrated infrared sensor. For C-arm repositioning to a
particular target view the recorded C-arm pose is restored as a virtual object
and visualized in an AR environment serving as a perceptual reference for the
technician. We conduct experiments in a setting simulating orthopedic trauma
surgery. Our proof-of-principle findings indicate that the proposed system can
decrease the 2.76 X-ray images required per desired view down to zero
suggesting substantial reductions of radiation dose during C-arm repositioning.
The proposed AR solution is a first step towards facilitating communication
between the surgeon and the surgical staff improving the quality of surgical
image acquisition and enabling context-aware guidance for surgery rooms of the
future. The concept of technician-in-the-loop design will become relevant to
various interventions considering the expected advancements of sensing and
wearable computing in the near future.
",0,0,1
"Distributed Optimization With Local Domains: Applications in MPC and
  Network Flows","  In this paper we consider a network with $P$ nodes where each node has
exclusive access to a local cost function. Our contribution is a
communication-efficient distributed algorithm that finds a vector $x^star$
minimizing the sum of all the functions. We make the additional assumption that
the functions have intersecting local domains i.e. each function depends only
on some components of the variable. Consequently each node is interested in
knowing only some components of $x^star$ not the entire vector. This allows
for improvement in communication-efficiency. We apply our algorithm to model
predictive control (MPC) and to network flow problems and show through
experiments on large networks that our proposed algorithm requires less
communications to converge than prior algorithms.
",1,0,0
"Post-processing procedure for industrial quantum key distribution
  systems","  We present algorithmic solutions aimed on post-processing for industrial
quantum key distribution systems with hardware sifting. The main steps of the
procedure are error correction parameter estimation and privacy
amplification. Authentication of a classical public communication channel is
also considered.
",1,0,0
"QuickNAT: A Fully Convolutional Network for Quick and Accurate
  Segmentation of Neuroanatomy","  Whole brain segmentation from structural magnetic resonance imaging (MRI) is
a prerequisite for most morphological analyses but is computationally intense
and can therefore delay the availability of image markers after scan
acquisition. We introduce QuickNAT a fully convolutional densely connected
neural network that segments a revisionMRI brain scan in 20 seconds. To
enable training of the complex network with millions of learnable parameters
using limited annotated data we propose to first pre-train on auxiliary labels
created from existing segmentation software. Subsequently the pre-trained
model is fine-tuned on manual labels to rectify errors in auxiliary labels.
With this learning strategy we are able to use large neuroimaging repositories
without manual annotations for training. In an extensive set of evaluations on
eight datasets that cover a wide age range pathology and different scanners
we demonstrate that QuickNAT achieves superior segmentation accuracy and
reliability in comparison to state-of-the-art methods while being orders of
magnitude faster. The speed up facilitates processing of large data
repositories and supports translation of imaging biomarkers by making them
available within seconds for fast clinical decision making.
",0,0,1
The Do's and Don'ts for CNN-based Face Verification,"  While the research community appears to have developed a consensus on the
methods of acquiring annotated data design and training of CNNs many
questions still remain to be answered. In this paper we explore the following
questions that are critical to face recognition research: (i) Can we train on
still images and expect the systems to work on videos? (ii) Are deeper datasets
better than wider datasets? (iii) Does adding label noise lead to improvement
in performance of deep networks? (iv) Is alignment needed for face recognition?
We address these questions by training CNNs using CASIA-WebFace UMDFaces and
a new video dataset and testing on YouTube- Faces IJB-A and a disjoint portion
of UMDFaces datasets. Our new data set which will be made publicly available
has 22075 videos and 3735476 human annotated frames extracted from them.
",0,0,1
"LiDAM: Semi-Supervised Learning with Localized Domain Adaptation and
  Iterative Matching","  Although data is abundant data labeling is expensive. Semi-supervised
learning methods combine a few labeled samples with a large corpus of unlabeled
data to effectively train models. This paper introduces our proposed method
LiDAM a semi-supervised learning approach rooted in both domain adaptation and
self-paced learning. LiDAM first performs localized domain shifts to extract
better domain-invariant features for the model that results in more accurate
clusters and pseudo-labels. These pseudo-labels are then aligned with real
class labels in a self-paced fashion using a novel iterative matching technique
that is based on majority consistency over high-confidence predictions.
Simultaneously a final classifier is trained to predict ground-truth labels
until convergence. LiDAM achieves state-of-the-art performance on the CIFAR-100
dataset outperforming FixMatch (73.50% vs. 71.82%) when using 2500 labels.
",0,0,1
Belief Propagation and Beyond for Particle Tracking,"  We describe a novel approach to statistical learning from particles tracked
while moving in a random environment. The problem consists in inferring
properties of the environment from recorded snapshots. We consider here the
case of a fluid seeded with identical passive particles that diffuse and are
advected by a flow. Our approach rests on efficient algorithms to estimate the
weighted number of possible matchings among particles in two consecutive
snapshots the partition function of the underlying graphical model. The
partition function is then maximized over the model parameters namely
diffusivity and velocity gradient. A Belief Propagation (BP) scheme is the
backbone of our algorithm providing accurate results for the flow parameters
we want to learn. The BP estimate is additionally improved by incorporating
Loop Series (LS) contributions. For the weighted matching problem LS is
compactly expressed as a Cauchy integral accurately estimated by a saddle
point approximation. Numerical experiments show that the quality of our
improved BP algorithm is comparable to the one of a fully polynomial randomized
approximation scheme based on the Markov Chain Monte Carlo (MCMC) method
while the BP-based scheme is substantially faster than the MCMC scheme.
",1,0,0
ROBO: Robust Fully Neural Object Detection for Robot Soccer,"  Deep Learning has become exceptionally popular in the last few years due to
its success in computer vision and other fields of AI. However deep neural
networks are computationally expensive which limits their application in low
power embedded systems such as mobile robots. In this paper an efficient
neural network architecture is proposed for the problem of detecting relevant
objects in robot soccer environments. The ROBO model's increase in efficiency
is achieved by exploiting the peculiarities of the environment. Compared to the
state-of-the-art Tiny YOLO model the proposed network provides approximately
35 times decrease in run time while achieving superior average precision
although at the cost of slightly worse localization accuracy.
",0,0,1
"Self-supervised Knowledge Distillation Using Singular Value
  Decomposition","  To solve deep neural network (DNN)'s huge training dataset and its high
computation issue so-called teacher-student (T-S) DNN which transfers the
knowledge of T-DNN to S-DNN has been proposed. However the existing T-S-DNN
has limited range of use and the knowledge of T-DNN is insufficiently
transferred to S-DNN. To improve the quality of the transferred knowledge from
T-DNN we propose a new knowledge distillation using singular value
decomposition (SVD). In addition we define a knowledge transfer as a
self-supervised task and suggest a way to continuously receive information from
T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of
the T-DNN can be up to 1.1% better than the T-DNN in terms of classification
accuracy. Also assuming the same computational cost our S-DNN outperforms the
S-DNN driven by the state-of-the-art distillation with a performance advantage
of 1.79%. code is available on https://github.com/sseung0703/SSKD_SVD.
",0,0,1
Linguistic Information Energy,"  In this treatment a text is considered to be a series of word impulses which
are read at a constant rate. The brain then assembles these units of
information into higher units of meaning. A classical systems approach is used
to model an initial part of this assembly process. The concepts of linguistic
system response information energy and ordering energy are defined and
analyzed. Finally as a demonstration information energy is used to estimate
the publication dates of a series of texts and the similarity of a set of
texts.
",0,1,0
Face Detection Using Adaboosted SVM-Based Component Classifier,"  Recently Adaboost has been widely used to improve the accuracy of any given
learning algorithm. In this paper we focus on designing an algorithm to employ
combination of Adaboost with Support Vector Machine as weak component
classifiers to be used in Face Detection Task. To obtain a set of effective
SVM-weaklearner Classifier this algorithm adaptively adjusts the kernel
parameter in SVM instead of using a fixed one. Proposed combination outperforms
in generalization in comparison with SVM on imbalanced classification problem.
The proposed here method is compared in terms of classification accuracy to
other commonly used Adaboost methods such as Decision Trees and Neural
Networks on CMU+MIT face database. Results indicate that the performance of
the proposed method is overall superior to previous Adaboost approaches.
",0,0,1
Adversarial Teacher-Student Learning for Unsupervised Domain Adaptation,"  The teacher-student (T/S) learning has been shown effective in unsupervised
domain adaptation [1]. It is a form of transfer learning not in terms of the
transfer of recognition decisions but the knowledge of posteriori
probabilities in the source domain as evaluated by the teacher model. It learns
to handle the speaker and environment variability inherent in and restricted to
the speech signal in the target domain without proactively addressing the
robustness to other likely conditions. Performance degradation may thus ensue.
In this work we advance T/S learning by proposing adversarial T/S learning to
explicitly achieve condition-robust unsupervised domain adaptation. In this
method a student acoustic model and a condition classifier are jointly
optimized to minimize the Kullback-Leibler divergence between the output
distributions of the teacher and student models and simultaneously to
min-maximize the condition classification loss. A condition-invariant deep
feature is learned in the adapted student model through this procedure. We
further propose multi-factorial adversarial T/S learning which suppresses
condition variabilities caused by multiple factors simultaneously. Evaluated
with the noisy CHiME-3 test set the proposed methods achieve relative word
error rate improvements of 44.60% and 5.38% respectively over a clean source
model and a strong T/S learning baseline model.
",0,1,0
"Near-Capacity Detection and Decoding: Code Design for Dynamic User Loads
  in Gaussian Multiple Access Channels","  This paper considers the forward error correction (FEC) code design for
approaching the capacity of a dynamic multiple access channel (MAC) where both
the number of users and their respective signal powers keep constantly
changing resembling the scenario of an actual wireless cellular system. To
obtain a low-complexity non-orthogonal multiple access (NOMA) scheme we
propose a serial concatenation of a low-density parity-check (LDPC) code and a
repetition code (REP) this way achieving near Gaussian MAC (GMAC) capacity
performance while coping with the dynamics of the MAC system. The joint
optimization of the LDPC and REP codes is addressed by matching the analytical
extrinsic information transfer (EXIT) functions of the sub-optimal multi-user
detector (MUD) and the channel code for a specific and static MAC system
achieving near-GMAC capacity. We show that the near-capacity performance can be
flexibly maintained with the same LDPC code regardless of the variations in the
number of users and power levels. This flexibility (or elasticity) is provided
by the REP code acting as ""user-load and power equalizer"" dramatically
simplifying the practical implementation of NOMA schemes as only a single LDPC
code is needed to cope with the dynamics of the MAC system.
",1,0,0
Unsupervised Deep Embedding for Clustering Analysis,"  Clustering is central to many data-driven application domains and has been
studied extensively in terms of distance functions and grouping algorithms.
Relatively little work has focused on learning representations for clustering.
In this paper we propose Deep Embedded Clustering (DEC) a method that
simultaneously learns feature representations and cluster assignments using
deep neural networks. DEC learns a mapping from the data space to a
lower-dimensional feature space in which it iteratively optimizes a clustering
objective. Our experimental evaluations on image and text corpora show
significant improvement over state-of-the-art methods.
",0,0,1
"On the Exact BER of Bit-Wise Demodulators for One-Dimensional
  Constellations","  The optimal bit-wise demodulator for M-ary pulse amplitude modulation (PAM)
over the additive white Gaussian noise channel is analyzed in terms of uncoded
bit-error rate (BER). New closed-form BER expressions for 4-PAM with any
labeling are developed. Moreover closed-form BER expressions for 11 out of 23
possible bit patterns for 8-PAM are presented which enable us to obtain the
BER for 8-PAM with some of the most popular labelings including the binary
reflected Gray code and the natural binary code. Numerical results show that
regardless of the labeling there is no difference between the optimal
demodulator and the symbol-wise demodulator for any BER of practical interest
(below 0.1).
",1,0,0
"Perception Consistency Ultrasound Image Super-resolution via
  Self-supervised CycleGAN","  Due to the limitations of sensors the transmission medium and the intrinsic
properties of ultrasound the quality of ultrasound imaging is always not
ideal especially its low spatial resolution. To remedy this situation deep
learning networks have been recently developed for ultrasound image
super-resolution (SR) because of the powerful approximation capability.
However most current supervised SR methods are not suitable for ultrasound
medical images because the medical image samples are always rare and usually
there are no low-resolution (LR) and high-resolution (HR) training pairs in
reality. In this work based on self-supervision and cycle generative
adversarial network (CycleGAN) we propose a new perception consistency
ultrasound image super-resolution (SR) method which only requires the LR
ultrasound data and can ensure the re-degenerated image of the generated SR one
to be consistent with the original LR image and vice versa. We first generate
the HR fathers and the LR sons of the test ultrasound LR image through image
enhancement and then make full use of the cycle loss of LR-SR-LR and HR-LR-SR
and the adversarial characteristics of the discriminator to promote the
generator to produce better perceptually consistent SR results. The evaluation
of PSNR/IFC/SSIM inference efficiency and visual effects under the benchmark
CCA-US and CCA-US datasets illustrate our proposed approach is effective and
superior to other state-of-the-art methods.
",0,0,1
Deep Learning for Identifying Metastatic Breast Cancer,"  The International Symposium on Biomedical Imaging (ISBI) held a grand
challenge to evaluate computational systems for the automated detection of
metastatic breast cancer in whole slide images of sentinel lymph node biopsies.
Our team won both competitions in the grand challenge obtaining an area under
the receiver operating curve (AUC) of 0.925 for the task of whole slide image
classification and a score of 0.7051 for the tumor localization task. A
pathologist independently reviewed the same images obtaining a whole slide
image classification AUC of 0.966 and a tumor localization score of 0.733.
Combining our deep learning system's predictions with the human pathologist's
diagnoses increased the pathologist's AUC to 0.995 representing an
approximately 85 percent reduction in human error rate. These results
demonstrate the power of using deep learning to produce significant
improvements in the accuracy of pathological diagnoses.
",0,0,1
Very Deep Convolutional Networks for Large-Scale Image Recognition,"  In this work we investigate the effect of the convolutional network depth on
its accuracy in the large-scale image recognition setting. Our main
contribution is a thorough evaluation of networks of increasing depth using an
architecture with very small (3x3) convolution filters which shows that a
significant improvement on the prior-art configurations can be achieved by
pushing the depth to 16-19 weight layers. These findings were the basis of our
ImageNet Challenge 2014 submission where our team secured the first and the
second places in the localisation and classification tracks respectively. We
also show that our representations generalise well to other datasets where
they achieve state-of-the-art results. We have made our two best-performing
ConvNet models publicly available to facilitate further research on the use of
deep visual representations in computer vision.
",0,0,1
Fundamental Limits of Training-Based Multiuser MIMO Systems,"  In this paper we endeavour to seek a fundamental understanding of the
potentials and limitations of training-based multiuser multiple-input
multiple-output (MIMO) systems. In a multiuser MIMO system users are
geographically separated. So the near-far effect plays an indispensable role
in channel fading. The existing optimal training design for conventional MIMO
does not take the near-far effect into account and thus is not applicable to a
multiuser MIMO system. In this work we use the majorization theory as a basic
tool to study the tradeoff between the channel estimation quality and the
information throughput. We establish tight upper and lower bounds of the
throughput and prove that the derived lower bound is asymptotically optimal
for throughput maximization at high signal-to-noise ratio. Our analysis shows
that the optimal training sequences for throughput maximization in a multiuser
MIMO system are in general not orthogonal to each other. Furthermore due to
the near-far effect the optimal training design for throughput maximization is
to deactivate a portion of users with the weakest channels in transmission.
These observations shed light on the practical design of training-based
multiuser MIMO systems.
",1,0,0
A Deep DUAL-PATH Network for Improved Mammogram Image Processing,"  We present for the first time a novel deep neural network architecture
called dcn with a dual-path connection between the input image and output
class label for mammogram image processing. This architecture is built upon
U-Net which non-linearly maps the input data into a deep latent space. One
path of the dcnn the locality preserving learner is devoted to
hierarchically extracting and exploiting intrinsic features of the input while
the other path called the conditional graph learner focuses on modeling the
input-mask correlations. The learned mask is further used to improve
classification results and the two learning paths complement each other. By
integrating the two learners our new architecture provides a simple but
effective way to jointly learn the segmentation and predict the class label.
Benefiting from the powerful expressive capacity of deep neural networks a more
discriminative representation can be learned in which both the semantics and
structure are well preserved. Experimental results show that dcn achieves the
best mammography segmentation and classification simultaneously outperforming
recent state-of-the-art models.
",0,0,1
"Batch Codes from Hamming and Reed-M""uller Codes","  Batch codes introduced by Ishai et al. encode a string $x in Sigma^k$
into an $m$-tuple of strings called buckets. In this paper we consider
multiset batch codes wherein a set of $t$-users wish to access one bit of
information each from the original string. We introduce a concept of optimal
batch codes. We first show that binary Hamming codes are optimal batch codes.
The main body of this work provides batch properties of Reed-M""uller codes. We
look at locality and availability properties of first order Reed-M""uller codes
over any finite field. We then show that binary first order Reed-M""uller codes
are optimal batch codes when the number of users is 4 and generalize our study
to the family of binary Reed-M""uller codes which have order less than half
their length.
",1,0,0
When are microcircuits well-modeled by maximum entropy methods?,"  Describing the collective activity of neural populations is a daunting task:
the number of possible patterns grows exponentially with the number of cells
resulting in practically unlimited complexity. Recent empirical studies
however suggest a vast simplification in how multi-neuron spiking occurs: the
activity patterns of some circuits are nearly completely captured by pairwise
interactions among neurons. Why are such pairwise models so successful in some
instances but insufficient in others? Here we study the emergence of
higher-order interactions in simple circuits with different architectures and
inputs. We quantify the impact of higher-order interactions by comparing the
responses of mechanistic circuit models vs. ""null"" descriptions in which all
higher-than-pairwise correlations have been accounted for by lower order
statistics known as pairwise maximum entropy models.
  We find that bimodal input signals produce larger deviations from pairwise
predictions than unimodal inputs for circuits with local and global
connectivity. Moreover recurrent coupling can accentuate these deviations if
coupling strengths are neither too weak nor too strong. A circuit model based
on intracellular recordings from ON parasol retinal ganglion cells shows that a
broad range of light signals induce unimodal inputs to spike generators and
that coupling strengths produce weak effects on higher-order interactions. This
provides a novel explanation for the success of pairwise models in this system.
Overall our findings identify circuit-level mechanisms that produce and fail
to produce higher-order spiking statistics in neural ensembles.
",1,0,0
Relevance-based Word Embedding,"  Learning a high-dimensional dense representation for vocabulary terms also
known as a word embedding has recently attracted much attention in natural
language processing and information retrieval tasks. The embedding vectors are
typically learned based on term proximity in a large corpus. This means that
the objective in well-known word embedding algorithms e.g. word2vec is to
accurately predict adjacent word(s) for a given word or context. However this
objective is not necessarily equivalent to the goal of many information
retrieval (IR) tasks. The primary objective in various IR tasks is to capture
relevance instead of term proximity syntactic or even semantic similarity.
This is the motivation for developing unsupervised relevance-based word
embedding models that learn word representations based on query-document
relevance information. In this paper we propose two learning models with
different objective functions; one learns a relevance distribution over the
vocabulary set for each query and the other classifies each term as belonging
to the relevant or non-relevant class for each query. To train our models we
used over six million unique queries and the top ranked documents retrieved in
response to each query which are assumed to be relevant to the query. We
extrinsically evaluate our learned word representation models using two IR
tasks: query expansion and query classification. Both query expansion
experiments on four TREC collections and query classification experiments on
the KDD Cup 2005 dataset suggest that the relevance-based word embedding models
significantly outperform state-of-the-art proximity-based embedding models
such as word2vec and GloVe.
",0,1,0
Learning Lightweight Lane Detection CNNs by Self Attention Distillation,"  Training deep models for lane detection is challenging due to the very subtle
and sparse supervisory signals inherent in lane annotations. Without learning
from much richer context these models often fail in challenging scenarios
e.g. severe occlusion ambiguous lanes and poor lighting conditions. In this
paper we present a novel knowledge distillation approach i.e. Self Attention
Distillation (SAD) which allows a model to learn from itself and gains
substantial improvement without any additional supervision or labels.
Specifically we observe that attention maps extracted from a model trained to
a reasonable level would encode rich contextual information. The valuable
contextual information can be used as a form of 'free' supervision for further
representation learning through performing topdown and layer-wise attention
distillation within the network itself. SAD can be easily incorporated in any
feedforward convolutional neural networks (CNN) and does not increase the
inference time. We validate SAD on three popular lane detection benchmarks
(TuSimple CULane and BDD100K) using lightweight models such as ENet ResNet-18
and ResNet-34. The lightest model ENet-SAD performs comparatively or even
surpasses existing algorithms. Notably ENet-SAD has 20 x fewer parameters and
runs 10 x faster compared to the state-of-the-art SCNN while still achieving
compelling performance in all benchmarks. Our code is available at
https://github.com/cardwing/Codes-for-Lane-Detection.
",0,0,1
"Reliable Communication under the Influence of a State-Constrained
  Jammer: A Novel Perspective on Receive Diversity","  The question of robust direct communication in vehicular networks is
discussed. In most state-of-the-art approaches there is no central entity
controlling channel access so there may be arbitrary interference from other
parties. Thus a suitable channel model for Vehicle-to-X (V2X) communication is
the Arbitrarily Varying Channel (AVC). Employing multiple antennas on a vehicle
or sending over multiple frequencies to make use of diversity are promising
approaches to combat interference. In this setup an important question about
diversity is how many antennas or orthogonal carrier frequencies are necessary
in order to avoid system breakdown due to unknown interference in AVCs. For
Binary Symmetric AVCs (AVBSC) and a physically meaningful identical
state-constrained jammer the deployment of a third uncorrelated receiving
antenna or the parallel transmission over three different orthogonal
frequencies avoids symmetrizability and thus ensures positivity of the capacity
of the overall communication channel. Furthermore the capacity of the
identical state-constrained composite AVBSC is continuous and shows
super-activation a phenomenon which was hitherto deemed impossible for
classical communication without secrecy constraints. Subsuming spatial and
frequency diversity are enablers for reliable communication over communication
channels with arbitrarily varying interference.
",1,0,0
"Degrees of Freedom of the MIMO Interference Channel with Cooperation and
  Cognition","  In this paper we explore the benefits in the sense of total (sum rate)
degrees of freedom (DOF) of cooperation and cognitive message sharing for a
two-user multiple-input-multiple-output (MIMO) Gaussian interference channel
with $M_1$ $M_2$ antennas at transmitters and $N_1$ $N_2$ antennas at
receivers. For the case of cooperation (including cooperation at transmitters
only at receivers only and at transmitters as well as receivers) the DOF is
$min M_1+M_2 N_1+N_2 max(M_1 N_2)) max(M_2 N_1)$ which is the same
as the DOF of the channel without cooperation. For the case of cognitive
message sharing the DOF is $min M_1+M_2 N_1+N_2 (1-1_T2)((1-1_R2)
max(M_1 N_2) + 1_R2 (M_1+N_2)) (1-1_T1)((1-1_R1) max(M_2 N_1) +
1_R1 (M_2+N_1)) $ where $1_Ti = 1$ $(0)$ when transmitter $i$ is (is not)
a cognitive transmitter and $1_Ri$ is defined in the same fashion. Our
results show that while both techniques may increase the sum rate capacity of
the MIMO interference channel only cognitive message sharing can increase the
DOF. We also find that it may be more beneficial for a user to have a cognitive
transmitter than to have a cognitive receiver.
",1,0,0
"EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery
  Generation","  Temporal grounding aims to predict a time interval of a video clip
corresponding to a natural language query input. In this work we present
EVOQUER a temporal grounding framework incorporating an existing text-to-video
grounding model and a video-assisted query generation network. Given a query
and an untrimmed video the temporal grounding model predicts the target
interval and the predicted video clip is fed into a video translation task by
generating a simplified version of the input query. EVOQUER forms closed-loop
learning by incorporating loss functions from both temporal grounding and query
generation serving as feedback. Our experiments on two widely used datasets
Charades-STA and ActivityNet show that EVOQUER achieves promising improvements
by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could
facilitate error analysis by explaining temporal grounding model behavior.
",0,0,1
The pursuit of beauty: Converting image labels to meaningful vectors,"  A challenge of the computer vision community is to understand the semantics
of an image in order to allow image reconstruction based on existing
high-level features or to better analyze (semi-)labelled datasets. Towards
addressing this challenge this paper introduces a method called
Occlusion-based Latent Representations (OLR) for converting image labels to
meaningful representations that capture a significant amount of data semantics.
Besides being informational rich these representations compose a disentangled
low-dimensional latent space where each image label is encoded into a separate
vector. We evaluate the quality of these representations in a series of
experiments whose results suggest that the proposed model can capture data
concepts and discover data interrelations.
",0,0,1
FingerNet: An Unified Deep Network for Fingerprint Minutiae Extraction,"  Minutiae extraction is of critical importance in automated fingerprint
recognition. Previous works on rolled/slap fingerprints failed on latent
fingerprints due to noisy ridge patterns and complex background noises. In this
paper we propose a new way to design deep convolutional network combining
domain knowledge and the representation ability of deep learning. In terms of
orientation estimation segmentation enhancement and minutiae extraction
several typical traditional methods performed well on rolled/slap fingerprints
are transformed into convolutional manners and integrated as an unified plain
network. We demonstrate that this pipeline is equivalent to a shallow network
with fixed weights. The network is then expanded to enhance its representation
ability and the weights are released to learn complex background variance from
data while preserving end-to-end differentiability. Experimental results on
NIST SD27 latent database and FVC 2004 slap database demonstrate that the
proposed algorithm outperforms the state-of-the-art minutiae extraction
algorithms. Code is made publicly available at:
https://github.com/felixTY/FingerNet.
",0,0,1
High-Fidelity Neural Human Motion Transfer from Monocular Video,"  Video-based human motion transfer creates video animations of humans
following a source motion. Current methods show remarkable results for
tightly-clad subjects. However the lack of temporally consistent handling of
plausible clothing dynamics including fine and high-frequency details
significantly limits the attainable visual quality. We address these
limitations for the first time in the literature and present a new framework
which performs high-fidelity and temporally-consistent human motion transfer
with natural pose-dependent non-rigid deformations for several types of loose
garments. In contrast to the previous techniques we perform image generation
in three subsequent stages synthesizing human shape structure and
appearance. Given a monocular RGB video of an actor we train a stack of
recurrent deep neural networks that generate these intermediate representations
from 2D poses and their temporal derivatives. Splitting the difficult motion
transfer problem into subtasks that are aware of the temporal motion context
helps us to synthesize results with plausible dynamics and pose-dependent
detail. It also allows artistic control of results by manipulation of
individual framework stages. In the experimental results we significantly
outperform the state-of-the-art in terms of video realism. Our code and data
will be made publicly available.
",0,0,1
"My Eyes Are Up Here: Promoting Focus on Uncovered Regions in Masked Face
  Recognition","  The recent Covid-19 pandemic and the fact that wearing masks in public is now
mandatory in several countries created challenges in the use of face
recognition systems (FRS). In this work we address the challenge of masked
face recognition (MFR) and focus on evaluating the verification performance in
FRS when verifying masked vs unmasked faces compared to verifying only unmasked
faces. We propose a methodology that combines the traditional triplet loss and
the mean squared error (MSE) intending to improve the robustness of an MFR
system in the masked-unmasked comparison mode. The results obtained by our
proposed method show improvements in a detailed step-wise ablation study. The
conducted study showed significant performance gains induced by our proposed
training paradigm and modified triplet loss on two evaluation databases.
",0,0,1
"Monocular Instance Motion Segmentation for Autonomous Driving: KITTI
  InstanceMotSeg Dataset and Multi-task Baseline","  Moving object segmentation is a crucial task for autonomous vehicles as it
can be used to segment objects in a class agnostic manner based on their motion
cues. It enables the detection of unseen objects during training (e.g. moose
or a construction truck) based on their motion and independent of their
appearance. Although pixel-wise motion segmentation has been studied in
autonomous driving literature it has been rarely addressed at the instance
level which would help separate connected segments of moving objects leading
to better trajectory planning. As the main issue is the lack of large public
datasets we create a new InstanceMotSeg dataset comprising of 12.9K samples
improving upon our KITTIMoSeg dataset. In addition to providing instance level
annotations we have added 4 additional classes which is crucial for studying
class agnostic motion segmentation. We adapt YOLACT and implement a
motion-based class agnostic instance segmentation model which would act as a
baseline for the dataset. We also extend it to an efficient multi-task model
which additionally provides semantic instance segmentation sharing the encoder.
The model then learns separate prototype coefficients within the class agnostic
and semantic heads providing two independent paths of object detection for
redundant safety. To obtain real-time performance we study different efficient
encoders and obtain 39 fps on a Titan Xp GPU using MobileNetV2 with an
improvement of 10% mAP relative to the baseline. Our model improves the
previous state of the art motion segmentation method by 3.3%. The dataset and
qualitative results video are shared in our website at
https://sites.google.com/view/instancemotseg/.
",0,0,1
The Porosity of Additive Noise Sequences,"  Consider a binary additive noise channel with noiseless feedback. When the
noise is a stationary and ergodic process $mathbfZ$ the capacity is
$1-mathbbH(mathbfZ)$ ($mathbbH(cdot)$ denoting the entropy rate). It
is shown analogously that when the noise is a deterministic sequence
$z^infty$ the capacity under finite-state encoding and decoding is
$1-barrho(z^infty)$ where $barrho(cdot)$ is Lempel and Ziv's
finite-state compressibility. This quantity is termed the emphporosity
$underlinesigma(cdot)$ of an individual noise sequence. A sequence of
schemes are presented that universally achieve porosity for any noise sequence.
These converse and achievability results may be interpreted both as a
channel-coding counterpart to Ziv and Lempel's work in universal source coding
as well as an extension to the work by Lomnitz and Feder and Shayevitz and
Feder on communication across modulo-additive channels. Additionally a
slightly more practical architecture is suggested that draws a connection with
finite-state predictability as introduced by Feder Gutman and Merhav.
",1,0,0
"Fully automated deep learning based segmentation of normal infarcted
  and edema regions from multiple cardiac MRI sequences","  Myocardial characterization is essential for patients with myocardial
infarction and other myocardial diseases and the assessment is often performed
using cardiac magnetic resonance (CMR) sequences. In this study we propose a
fully automated approach using deep convolutional neural networks (CNN) for
cardiac pathology segmentation including left ventricular (LV) blood pool
right ventricular blood pool LV normal myocardium LV myocardial edema (ME)
and LV myocardial scars (MS). The input to the network consists of three CMR
sequences namely late gadolinium enhancement (LGE) T2 and balanced steady
state free precession (bSSFP). The proposed approach utilized the data provided
by the MyoPS challenge hosted by MICCAI 2020 in conjunction with STACOM. The
training set for the CNN model consists of images acquired from 25 cases and
the gold standard labels are provided by trained raters and validated by
radiologists. The proposed approach introduces a data augmentation module
linear encoder and decoder module and a network module to increase the number
of training samples and improve the prediction accuracy for LV ME and MS. The
proposed approach is evaluated by the challenge organizers with a test set
including 20 cases and achieves a mean dice score of $46.8%$ for LV MS and
$55.7%$ for LV ME+MS
",0,0,1
"Closed-Form Expressions for Secrecy Capacity over Correlated Rayleigh
  Fading Channels","  We investigate the secure communications over correlated wiretap Rayleigh
fading channels assuming the full channel state information (CSI) available.
Based on the information theoretic formulation we derive closed-form
expressions for the average secrecy capacity and the outage probability.
Simulation results confirm our analytical expressions.
",1,0,0
"Probabilistic Multi-modal Trajectory Prediction with Lane Attention for
  Autonomous Vehicles","  Trajectory prediction is crucial for autonomous vehicles. The planning system
not only needs to know the current state of the surrounding objects but also
their possible states in the future. As for vehicles their trajectories are
significantly influenced by the lane geometry and how to effectively use the
lane information is of active interest. Most of the existing works use
rasterized maps to explore road information which does not distinguish
different lanes. In this paper we propose a novel instance-aware
representation for lane representation. By integrating the lane features and
trajectory features a goal-oriented lane attention module is proposed to
predict the future locations of the vehicle. We show that the proposed lane
representation together with the lane attention module can be integrated into
the widely used encoder-decoder framework to generate diverse predictions. Most
importantly each generated trajectory is associated with a probability to
handle the uncertainty. Our method does not suffer from collapsing to one
behavior modal and can cover diverse possibilities. Extensive experiments and
ablation studies on the benchmark datasets corroborate the effectiveness of our
proposed method. Notably our proposed method ranks third place in the
Argoverse motion forecasting competition at NeurIPS 2019.
",0,0,1
"OpenPifPaf: Composite Fields for Semantic Keypoint Detection and
  Spatio-Temporal Association","  Many image-based perception tasks can be formulated as detecting associating
and tracking semantic keypoints e.g. human body pose estimation and tracking.
In this work we present a general framework that jointly detects and forms
spatio-temporal keypoint associations in a single stage making this the first
real-time pose detection and tracking algorithm. We present a generic neural
network architecture that uses Composite Fields to detect and construct a
spatio-temporal pose which is a single connected graph whose nodes are the
semantic keypoints (e.g. a person's body joints) in multiple frames. For the
temporal associations we introduce the Temporal Composite Association Field
(TCAF) which requires an extended network architecture and training method
beyond previous Composite Fields. Our experiments show competitive accuracy
while being an order of magnitude faster on multiple publicly available
datasets such as COCO CrowdPose and the PoseTrack 2017 and 2018 datasets. We
also show that our method generalizes to any class of semantic keypoints such
as car and animal parts to provide a holistic perception framework that is well
suited for urban mobility such as self-driving cars and delivery robots.
",0,0,1
Coded Computation over Heterogeneous Clusters,"  In large-scale distributed computing clusters such as Amazon EC2 there are
several types of ""system noise"" that can result in major degradation of
performance: bottlenecks due to limited communication bandwidth latency due to
straggler nodes etc. On the other hand these systems enjoy abundance of
redundancy - a vast number of computing nodes and large storage capacity. There
have been recent results that demonstrate the impact of coding for efficient
utilization of computation and storage redundancy to alleviate the effect of
stragglers and communication bottlenecks in homogeneous clusters. In this
paper we focus on general heterogeneous distributed computing clusters
consisting of a variety of computing machines with different capabilities. We
propose a coding framework for speeding up distributed computing in
heterogeneous clusters by trading redundancy for reducing the latency of
computation. In particular we propose Heterogeneous Coded Matrix
Multiplication (HCMM) algorithm for performing distributed matrix
multiplication over heterogeneous clusters that is provably asymptotically
optimal for a broad class of processing time distributions. Moreover we show
that HCMM is unboundedly faster than any uncoded scheme. To demonstrate
practicality of HCMM we carry out experiments over Amazon EC2 clusters where
HCMM is found to be up to $61%$ $46%$ and $36%$ respectively faster than
three benchmark load allocation schemes - Uniform Uncoded Load-balanced
Uncoded and Uniform Coded. Additionally we provide a generalization to the
problem of optimal load allocation in heterogeneous settings where we take
into account the monetary costs associated with the clusters. We argue that
HCMM is asymptotically optimal for budget-constrained scenarios as well and we
develop a heuristic algorithm for (HCMM) load allocation for budget-limited
computation tasks.
",1,0,0
AffWild Net and Aff-Wild Database,"  Emotions recognition is the task of recognizing people's emotions. Usually it
is achieved by analyzing expression of peoples faces. There are two ways for
representing emotions: The categorical approach and the dimensional approach by
using valence and arousal values. Valence shows how negative or positive an
emotion is and arousal shows how much it is activated. Recent deep learning
models that have to do with emotions recognition are using the second
approach valence and arousal. Moreover a more interesting concept which is
useful in real life is the ""in the wild"" emotions recognition. ""In the wild""
means that the images analyzed for the recognition task come from from real
life sources(online videos online photos etc.) and not from staged
experiments. So they introduce unpredictable situations in the images that
have to be modeled. The purpose of this project is to study the previous work
that was done for the ""in the wild"" emotions recognition concept design a new
dataset which has as a standard the ""Aff-wild"" database implement new deep
learning models and evaluate the results. First already existing databases and
deep learning models are presented. Then inspired by them a new database is
created which includes 507.208 frames in total from 106 videos which were
gathered from online sources. Then the data are tested in a CNN model based on
CNN-M architecture in order to be sure about their usability. Next the main
model of this project is implemented. That is a Regression GAN which can
execute unsupervised and supervised learning at the same time. More
specifically it keeps the main functionality of GANs which is to produce fake
images that look as good as the real ones while it can also predict valence
and arousal values for both real and fake images. Finally the database created
earlier is applied to this model and the results are presented and evaluated.
",0,0,1
"PSF--NET: A Non-parametric Point Spread Function Model for Ground Based
  Optical Telescopes","  Ground based optical telescopes are seriously affected by atmospheric
turbulence induced aberrations. Understanding properties of these aberrations
is important both for instruments design and image restoration methods
development. Because the point spread function can reflect performance of the
whole optic system it is appropriate to use the point spread function to
describe atmospheric turbulence induced aberrations. Assuming point spread
functions induced by the atmospheric turbulence with the same profile belong to
the same manifold space we propose a non-parametric point spread function --
PSF-NET. The PSF-NET has a cycle convolutional neural network structure and is
a statistical representation of the manifold space of PSFs induced by the
atmospheric turbulence with the same profile. Testing the PSF-NET with
simulated and real observation data we find that a well trained PSF--NET can
restore any short exposure images blurred by atmospheric turbulence with the
same profile. Besides we further use the impulse response of the PSF-NET
which can be viewed as the statistical mean PSF to analyze interpretation
properties of the PSF-NET. We find that variations of statistical mean PSFs are
caused by variations of the atmospheric turbulence profile: as the difference
of the atmospheric turbulence profile increases the difference between
statistical mean PSFs also increases. The PSF-NET proposed in this paper
provides a new way to analyze atmospheric turbulence induced aberrations which
would be benefit to develop new observation methods for ground based optical
telescopes.
",0,0,1
"Superpixel-guided Discriminative Low-rank Representation of
  Hyperspectral Images for Classification","  In this paper we propose a novel classification scheme for the remotely
sensed hyperspectral image (HSI) namely SP-DLRR by comprehensively exploring
its unique characteristics including the local spatial information and
low-rankness. SP-DLRR is mainly composed of two modules i.e. the
classification-guided superpixel segmentation and the discriminative low-rank
representation which are iteratively conducted. Specifically by utilizing the
local spatial information and incorporating the predictions from a typical
classifier the first module segments pixels of an input HSI (or its
restoration generated by the second module) into superpixels. According to the
resulting superpixels the pixels of the input HSI are then grouped into
clusters and fed into our novel discriminative low-rank representation model
with an effective numerical solution. Such a model is capable of increasing the
intra-class similarity by suppressing the spectral variations locally while
promoting the inter-class discriminability globally leading to a restored HSI
with more discriminative pixels. Experimental results on three benchmark
datasets demonstrate the significant superiority of SP-DLRR over
state-of-the-art methods especially for the case with an extremely limited
number of training pixels.
",0,0,1
A Modified No Search Algorithm for Fractal Image Compression,"  Fractal image compression has some desirable properties like high quality at
high compression ratio fast decoding and resolution independence. Therefore
it can be used for many applications such as texture mapping and pattern
recognition and image watermarking. But it suffers from long encoding time due
to its need to find the best match between sub blocks. This time is related to
the approach that is used. In this paper we present a fast encoding Algorithm
based on no search method. Our goal is that more blocks are covered in initial
step of quad tree algorithm. Experimental result has been compared with other
new fast fractal coding methods showing it is better in term of bit rate in
same condition while the other parameters are fixed.
",0,0,1
"Multi-Layer Precoding: A Potential Solution for Full-Dimensional Massive
  MIMO Systems","  Massive multiple-input multiple-output (MIMO) systems achieve high sum
spectral efficiency by offering an order of magnitude increase in multiplexing
gains. In time division duplexing systems however the reuse of uplink
training pilots among cells results in additional channel estimation error
which causes downlink inter-cell interference even when large numbers of
antennas are employed. Handling this interference with conventional network
MIMO techniques is challenging due to the large channel dimensionality.
Further the implementation of large antenna precoding/combining matrices is
associated with high hardware complexity and power consumption. In this paper
we propose multi-layer precoding to enable efficient and low complexity
operation in full-dimensional massive MIMO where a large number of antennas is
used in two dimensions. In multi-layer precoding the precoding matrix of each
base station is written as a product of a number of precoding matrices each
one called a layer. Multi-layer precoding (i) leverages the directional
characteristics of large-scale MIMO channels to manage inter-cell interference
with low channel knowledge requirements and (ii) allows for an efficient
implementation using low-complexity hybrid analog/digital architectures. We
present a specific multi-layer precoding design for full-dimensional massive
MIMO systems. The performance of this precoding design is analyzed and the
per-user achievable rate is characterized for general channel models. The
asymptotic optimality of the proposed multi-layer precoding design is then
proved for some special yet important channels. Numerical simulations verify
the analytical results and illustrate the potential gains of multi-layer
precoding compared to traditional pilot-contaminated massive MIMO solutions.
",1,0,0
Learning Loss for Test-Time Augmentation,"  Data augmentation has been actively studied for robust neural networks. Most
of the recent data augmentation methods focus on augmenting datasets during the
training phase. At the testing phase simple transformations are still widely
used for test-time augmentation. This paper proposes a novel instance-level
test-time augmentation that efficiently selects suitable transformations for a
test input. Our proposed method involves an auxiliary module to predict the
loss of each possible transformation given the input. Then the transformations
having lower predicted losses are applied to the input. The network obtains the
results by averaging the prediction results of augmented inputs. Experimental
results on several image classification benchmarks show that the proposed
instance-aware test-time augmentation improves the model's robustness against
various corruptions.
",0,0,1
Reinterpreting CTC training as iterative fitting,"  The connectionist temporal classification (CTC) enables end-to-end sequence
learning by maximizing the probability of correctly recognizing sequences
during training. The outputs of a CTC-trained model tend to form a series of
spikes separated by strongly predicted blanks know as the spiky problem. To
figure out the reason for it we reinterpret the CTC training process as an
iterative fitting task that is based on frame-wise cross-entropy loss. It
offers us an intuitive way to compare target probabilities with model outputs
for each iteration and explain how the model outputs gradually turns spiky.
Inspired by it we put forward two ways to modify the CTC training. The
experiments demonstrate that our method can well solve the spiky problem and
moreover lead to faster convergence over various training settings. Beside
this the reinterpretation of CTC as a brand new perspective may be
potentially useful in other situations. The code is publicly available at
https://github.com/hzli-ucas/caffe/tree/ctc.
",0,0,1
TaxiNLI: Taking a Ride up the NLU Hill,"  Pre-trained Transformer-based neural architectures have consistently achieved
state-of-the-art performance in the Natural Language Inference (NLI) task.
Since NLI examples encompass a variety of linguistic logical and reasoning
phenomena it remains unclear as to which specific concepts are learnt by the
trained systems and where they can achieve strong generalization. To
investigate this question we propose a taxonomic hierarchy of categories that
are relevant for the NLI task. We introduce TAXINLI a new dataset that has
10k examples from the MNLI dataset (Williams et al. 2018) with these taxonomic
labels. Through various experiments on TAXINLI we observe that whereas for
certain taxonomic categories SOTA neural models have achieved near perfect
accuracies - a large jump over the previous models - some categories still
remain difficult. Our work adds to the growing body of literature that shows
the gaps in the current NLI systems and datasets through a systematic
presentation and analysis of reasoning categories.
",0,1,0
"Exploiting Inter-Frame Regional Correlation for Efficient Action
  Recognition","  Temporal feature extraction is an important issue in video-based action
recognition. Optical flow is a popular method to extract temporal feature
which produces excellent performance thanks to its capacity of capturing
pixel-level correlation information between consecutive frames. However such a
pixel-level correlation is extracted at the cost of high computational
complexity and large storage resource. In this paper we propose a novel
temporal feature extraction method named Attentive Correlated Temporal Feature
(ACTF) by exploring inter-frame correlation within a certain region. The
proposed ACTF exploits both bilinear and linear correlation between successive
frames on the regional level. Our method has the advantage of achieving
performance comparable to or better than optical flow-based methods while
avoiding the introduction of optical flow. Experimental results demonstrate our
proposed method achieves the state-of-the-art performances of 96.3% on UCF101
and 76.3% on HMDB51 benchmark datasets.
",0,0,1
"Widget Captioning: Generating Natural Language Description for Mobile
  User Interface Elements","  Natural language descriptions of user interface (UI) elements such as
alternative text are crucial for accessibility and language-based interaction
in general. Yet these descriptions are constantly missing in mobile UIs. We
propose widget captioning a novel task for automatically generating language
descriptions for UI elements from multimodal input including both the image and
the structural representations of user interfaces. We collected a large-scale
dataset for widget captioning with crowdsourcing. Our dataset contains 162859
language phrases created by human workers for annotating 61285 UI elements
across 21750 unique UI screens. We thoroughly analyze the dataset and train
and evaluate a set of deep model configurations to investigate how each feature
modality as well as the choice of learning strategies impact the quality of
predicted captions. The task formulation and the dataset as well as our
benchmark models contribute a solid basis for this novel multimodal captioning
task that connects language and user interfaces.
",0,1,0
Improving Adversarial Transferability with Gradient Refining,"  Deep neural networks are vulnerable to adversarial examples which are
crafted by adding human-imperceptible perturbations to original images. Most
existing adversarial attack methods achieve nearly 100% attack success rates
under the white-box setting but only achieve relatively low attack success
rates under the black-box setting. To improve the transferability of
adversarial examples for the black-box setting several methods have been
proposed e.g. input diversity translation-invariant attack and
momentum-based attack. In this paper we propose a method named Gradient
Refining which can further improve the adversarial transferability by
correcting useless gradients introduced by input diversity through multiple
transformations. Our method is generally applicable to many gradient-based
attack methods combined with input diversity. Extensive experiments are
conducted on the ImageNet dataset and our method can achieve an average
transfer success rate of 82.07% for three different models under single-model
setting which outperforms the other state-of-the-art methods by a large margin
of 6.0% averagely. And we have applied the proposed method to the competition
CVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and
won the second place in attack success rates among 1558 teams.
",0,0,1
Open Domain Question Answering over Tables via Dense Retrieval,"  Recent advances in open-domain QA have led to strong models based on dense
retrieval but only focused on retrieving textual passages. In this work we
tackle open-domain QA over tables for the first time and show that retrieval
can be improved by a retriever designed to handle tabular context. We present
an effective pre-training procedure for our retriever and improve retrieval
quality with mined hard negatives. As relevant datasets are missing we extract
a subset of Natural Questions (Kwiatkowski et al. 2019) into a Table QA
dataset. We find that our retriever improves retrieval results from 72.0 to
81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match over a
BERT based retriever.
",0,1,0
Handwritten Digit Recognition by Elastic Matching,"  A simple model of MNIST handwritten digit recognition is presented here. The
model is an adaptation of a previous theory of face recognition. It realizes
translation and rotation invariance in a principled way instead of being based
on extensive learning from large masses of sample data. The presented
recognition rates fall short of other publications but due to its
inspectability and conceptual and numerical simplicity our system commends
itself as a basis for further development.
",0,0,1
"On the One-Shot Zero-Error Classical Capacity of Classical-Quantum
  Channels Assisted by Quantum Non-signalling Correlations","  Duan and Winter studied the one-shot zero-error classical capacity of a
quantum channel assisted by quantum non-signalling correlations and formulated
this problem as a semidefinite program depending only on the Kraus operator
space of the channel. For the class of classical-quantum channels they showed
that the asymptotic zero-error classical capacity assisted by quantum
non-signalling correlations minimized over all classical-quantum channels with
a confusability graph $G$ is exactly $log vartheta(G)$ where $vartheta(G)$
is the celebrated Lov'asz theta function. In this paper we show that the
one-shot capacity for a classical-quantum channel induced from a circulant
graph $G$ defined by equal-sized cyclotomic cosets is $log lfloor
vartheta(G) rfloor$ which further implies that its asymptotic capacity is
$log vartheta(G)$. This type of graphs include the cycle graphs of odd
length the Paley graphs of prime vertices and the cubit residue graphs of
prime vertices. Examples of other graphs are also discussed. This endows the
Lov'asz $theta$ function with a more straightforward operational meaning.
",1,0,0
The generalized Lasso with non-linear observations,"  We study the problem of signal estimation from non-linear observations when
the signal belongs to a low-dimensional set buried in a high-dimensional space.
A rough heuristic often used in practice postulates that non-linear
observations may be treated as noisy linear observations and thus the signal
may be estimated using the generalized Lasso. This is appealing because of the
abundance of efficient specialized solvers for this program. Just as noise may
be diminished by projecting onto the lower dimensional space the error from
modeling non-linear observations with linear observations will be greatly
reduced when using the signal structure in the reconstruction. We allow general
signal structure only assuming that the signal belongs to some set K in R^n.
We consider the single-index model of non-linearity. Our theory allows the
non-linearity to be discontinuous not one-to-one and even unknown. We assume a
random Gaussian model for the measurement matrix but allow the rows to have an
unknown covariance matrix. As special cases of our results we recover
near-optimal theory for noisy linear observations and also give the first
theoretical accuracy guarantee for 1-bit compressed sensing with unknown
covariance matrix of the measurement vectors.
",1,0,0
LiDAR-Camera Calibration using 3D-3D Point correspondences,"  With the advent of autonomous vehicles LiDAR and cameras have become an
indispensable combination of sensors. They both provide rich and complementary
data which can be used by various algorithms and machine learning to sense and
make vital inferences about the surroundings. We propose a novel pipeline and
experimental setup to find accurate rigid-body transformation for extrinsically
calibrating a LiDAR and a camera. The pipeling uses 3D-3D point correspondences
in LiDAR and camera frame and gives a closed form solution. We further show the
accuracy of the estimate by fusing point clouds from two stereo cameras which
align perfectly with the rotation and translation estimated by our method
confirming the accuracy of our method's estimates both mathematically and
visually. Taking our idea of extrinsic LiDAR-camera calibration forward we
demonstrate how two cameras with no overlapping field-of-view can also be
calibrated extrinsically using 3D point correspondences. The code has been made
available as open-source software in the form of a ROS package more
information about which can be sought here:
https://github.com/ankitdhall/lidar_camera_calibration .
",0,0,1
MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving,"  While most approaches to semantic reasoning have focused on improving
performance in this paper we argue that computational times are very important
in order to enable real time applications such as autonomous driving. Towards
this goal we present an approach to joint classification detection and
semantic segmentation via a unified architecture where the encoder is shared
amongst the three tasks. Our approach is very simple can be trained end-to-end
and performs extremely well in the challenging KITTI dataset outperforming the
state-of-the-art in the road segmentation task. Our approach is also very
efficient taking less than 100 ms to perform all tasks.
",0,0,1
"Joint Source Channel and Space-time Coding of Progressive Sources in
  MIMO Systems","  The optimization of joint source and channel coding for a sequence of
numerous progressive packets is a challenging problem. Further the problem
becomes more complicated if the space-time coding is also involved with the
optimization in a multiple-input multiple-output (MIMO) system. This is because
the number of ways of jointly assigning channels codes and space-time codes to
progressive packets is much larger than that of solely assigning channel codes
to the packets. We are unaware of any feasible and complete solution for such
optimization of joint source channel and space-time coding of progressive
packets. This paper applies a parametric approach to address that complex joint
optimization problem in a MIMO system. We use the parametric methodology to
derive some useful theoretical results and then exploit those results to
propose an optimization method where the joint assignment of channel codes and
space-time codes to the packets can be optimized in a packet-by-packet manner.
As a result the computational complexity of the optimization is exponentially
reduced compared to the conventional exhaustive search. The numerical results
show that the proposed method significantly improves the peak-signal-to-noise
ratio performance of the rate-based optimal solution in a MIMO system.
",1,0,0
"Design and Evaluation of Product Aesthetics: A Human-Machine Hybrid
  Approach","  Aesthetics are critically important to market acceptance in many product
categories. In the automotive industry in particular an improved aesthetic
design can boost sales by 30% or more. Firms invest heavily in designing and
testing new product aesthetics. A single automotive ""theme clinic"" costs
between $100000 and $1000000 and hundreds are conducted annually. We use
machine learning to augment human judgment when designing and testing new
product aesthetics. The model combines a probabilistic variational autoencoder
(VAE) and adversarial components from generative adversarial networks (GAN)
along with modeling assumptions that address managerial requirements for firm
adoption. We train our model with data from an automotive partner-7000 images
evaluated by targeted consumers and 180000 high-quality unrated images. Our
model predicts well the appeal of new aesthetic designs-38% improvement
relative to a baseline and substantial improvement over both conventional
machine learning models and pretrained deep learning models. New automotive
designs are generated in a controllable manner for the design team to consider
which we also empirically verify are appealing to consumers. These results
combining human and machine inputs for practical managerial usage suggest that
machine learning offers significant opportunity to augment aesthetic design.
",0,0,1
"Multimodal Relational Tensor Network for Sentiment and Emotion
  Classification","  Understanding Affect from video segments has brought researchers from the
language audio and video domains together. Most of the current multimodal
research in this area deals with various techniques to fuse the modalities and
mostly treat the segments of a video independently. Motivated by the work of
(Zadeh et al. 2017) and (Poria et al. 2017) we present our architecture
Relational Tensor Network where we use the inter-modal interactions within a
segment (intra-segment) and also consider the sequence of segments in a video
to model the inter-segment inter-modal interactions. We also generate rich
representations of text and audio modalities by leveraging richer audio and
linguistic context alongwith fusing fine-grained knowledge based polarity
scores from text. We present the results of our model on CMU-MOSEI dataset and
show that our model outperforms many baselines and state of the art methods for
sentiment classification and emotion recognition.
",0,1,0
"Two infinite classes of rotation symmetric bent functions with simple
  representation","  In the literature few $n$-variable rotation symmetric bent functions have
been constructed. In this paper we present two infinite classes of rotation
symmetric bent functions on $mathbbF_2^n$ of the two forms:
  rm (i) $f(x)=sum_i=0^m-1x_ix_i+m + gamma(x_0+x_mcdots
x_m-1+x_2m-1)$
  rm (ii) $f_t(x)= sum_i=0^n-1(x_ix_i+tx_i+m +x_ix_i+t)+
sum_i=0^m-1x_ix_i+m+ gamma(x_0+x_mcdots x_m-1+x_2m-1)$
  noindent where $n=2m$ $gamma(X_0X_1cdots X_m-1)$ is any rotation
symmetric polynomial and $m/gcd(mt)$ is odd. The class (i) of rotation
symmetric bent functions has algebraic degree ranging from 2 to $m$ and the
other class (ii) has algebraic degree ranging from 3 to $m$.
",1,0,0
"T""ubingen-Oslo system: Linear regression works the best at Predicting
  Current and Future Psychological Health from Childhood Essays in the CLPsych
  2018 Shared Task","  This paper describes our efforts in predicting current and future
psychological health from childhood essays within the scope of the CLPsych-2018
Shared Task. We experimented with a number of different models including
recurrent and convolutional networks Poisson regression support vector
regression and L1 and L2 regularized linear regression. We obtained the best
results on the training/development data with L2 regularized linear regression
(ridge regression) which also got the best scores on main metrics in the
official testing for task A (predicting psychological health from essays
written at the age of 11 years) and task B (predicting later psychological
health from essays written at the age of 11).
",0,1,0
Differentiable Network Adaption with Elastic Search Space,"  In this paper we propose a novel network adaption method called
Differentiable Network Adaption (DNA) which can adapt an existing network to a
specific computation budget by adjusting the width and depth in a
differentiable manner. The gradient-based optimization allows DNA to achieve an
automatic optimization of width and depth rather than previous heuristic
methods that heavily rely on human priors. Moreover we propose a new elastic
search space that can flexibly condense or expand during the optimization
process allowing the network optimization of width and depth in a bi-direction
manner. By DNA we successfully achieve network architecture optimization by
condensing and expanding in both width and depth dimensions. Extensive
experiments on ImageNet demonstrate that DNA can adapt the existing network to
meet different targeted computation requirements with better performance than
previous methods. What's more DNA can further improve the performance of
high-accuracy networks obtained by state-of-the-art neural architecture search
methods such as EfficientNet and MobileNet-v3.
",0,0,1
"Incorporating Context and External Knowledge for Pronoun Coreference
  Resolution","  Linking pronominal expressions to the correct references requires in many
cases better analysis of the contextual information and external knowledge. In
this paper we propose a two-layer model for pronoun coreference resolution
that leverages both context and external knowledge where a knowledge attention
mechanism is designed to ensure the model leveraging the appropriate source of
external knowledge based on different context. Experimental results demonstrate
the validity and effectiveness of our model where it outperforms
state-of-the-art models by a large margin.
",0,1,0
Fast Maximum-Likelihood Decoding of the Golden Code,"  The golden code is a full-rate full-diversity space-time code for two
transmit antennas that has a maximal coding gain. Because each codeword conveys
four information symbols from an M-ary quadrature-amplitude modulation
alphabet the complexity of an exhaustive search decoder is proportional to
M^2. In this paper we present a new fast algorithm for maximum-likelihood
decoding of the golden code that has a worst-case complexity of only O(2M^2.5).
We also present an efficient implementation of the fast decoder that exhibits a
low average complexity. Finally in contrast to the overlaid Alamouti codes
which lose their fast decodability property on time-varying channels we show
that the golden code is fast decodable on both quasistatic and rapid
time-varying channels.
",1,0,0
Class Rectification Hard Mining for Imbalanced Deep Learning,"  Recognising detailed facial or clothing attributes in images of people is a
challenging task for computer vision especially when the training data are
both in very large scale and extremely imbalanced among different attribute
classes. To address this problem we formulate a novel scheme for batch
incremental hard sample mining of minority attribute classes from imbalanced
large scale training data. We develop an end-to-end deep learning framework
capable of avoiding the dominant effect of majority classes by discovering
sparsely sampled boundaries of minority classes. This is made possible by
introducing a Class Rectification Loss (CRL) regularising algorithm. We
demonstrate the advantages and scalability of CRL over existing
state-of-the-art attribute recognition and imbalanced data learning models on
two large scale imbalanced benchmark datasets the CelebA facial attribute
dataset and the X-Domain clothing attribute dataset.
",0,0,1
Text Classification with Few Examples using Controlled Generalization,"  Training data for text classification is often limited in practice
especially for applications with many output classes or involving many related
classification problems. This means classifiers must generalize from limited
evidence but the manner and extent of generalization is task dependent.
Current practice primarily relies on pre-trained word embeddings to map words
unseen in training to similar seen ones. Unfortunately this squishes many
components of meaning into highly restricted capacity. Our alternative begins
with sparse pre-trained representations derived from unlabeled parsed corpora;
based on the available training data we select features that offers the
relevant generalizations. This produces task-specific semantic vectors; here
we show that a feed-forward network over these vectors is especially effective
in low-data scenarios compared to existing state-of-the-art methods. By
further pairing this network with a convolutional neural network we keep this
edge in low data scenarios and remain competitive when using full training
sets.
",0,1,0
StainGAN: Stain Style Transfer for Digital Histological Images,"  Digitized Histological diagnosis is in increasing demand. However color
variations due to various factors are imposing obstacles to the diagnosis
process. The problem of stain color variations is a well-defined problem with
many proposed solutions. Most of these solutions are highly dependent on a
reference template slide. We propose a deep-learning solution inspired by
CycleGANs that is trained end-to-end eliminating the need for an expert to
pick a representative reference slide. Our approach showed superior results
quantitatively and qualitatively against the state of the art methods (10%
improvement visually using SSIM). We further validated our method on a clinical
use-case namely Breast Cancer tumor classification showing 12% increase in
AUC. The code will be made publicly available.
",0,0,1
Rapid Adaptation of POS Tagging for Domain Specific Uses,"  Part-of-speech (POS) tagging is a fundamental component for performing
natural language tasks such as parsing information extraction and question
answering. When POS taggers are trained in one domain and applied in
significantly different domains their performance can degrade dramatically. We
present a methodology for rapid adaptation of POS taggers to new domains. Our
technique is unsupervised in that a manually annotated corpus for the new
domain is not necessary. We use suffix information gathered from large amounts
of raw text as well as orthographic information to increase the lexical
coverage. We present an experiment in the Biological domain where our POS
tagger achieves results comparable to POS taggers specifically trained to this
domain.
",0,1,0
Large-margin Learning of Compact Binary Image Encodings,"  The use of high-dimensional features has become a normal practice in many
computer vision applications. The large dimension of these features is a
limiting factor upon the number of data points which may be effectively stored
and processed however. We address this problem by developing a novel approach
to learning a compact binary encoding which exploits both pair-wise proximity
and class-label information on training data set. Exploiting this extra
information allows the development of encodings which although compact
outperform the original high-dimensional features in terms of final
classification or retrieval performance. The method is general in that it is
applicable to both non-parametric and parametric learning methods. This
generality means that the embedded features are suitable for a wide variety of
computer vision tasks such as image classification and content-based image
retrieval. Experimental results demonstrate that the new compact descriptor
achieves an accuracy comparable to and in some cases better than the visual
descriptor in the original space despite being significantly more compact.
Moreover any convex loss function and convex regularization penalty (e.g. $
ell_p $ norm with $ p ge 1 $) can be incorporated into the framework which
provides future flexibility.
",0,0,1
Multi-modal Datasets for Super-resolution,"  Nowdays most datasets used to train and evaluate super-resolution models are
single-modal simulation datasets. However due to the variety of image
degradation types in the real world models trained on single-modal simulation
datasets do not always have good robustness and generalization ability in
different degradation scenarios. Previous work tended to focus only on
true-color images. In contrast we first proposed real-world black-and-white
old photo datasets for super-resolution (OID-RW) which is constructed using
two methods of manually filling pixels and shooting with different cameras. The
dataset contains 82 groups of images including 22 groups of character type and
60 groups of landscape and architecture. At the same time we also propose a
multi-modal degradation dataset (MDD400) to solve the super-resolution
reconstruction in real-life image degradation scenarios. We managed to simulate
the process of generating degraded images by the following four methods:
interpolation algorithm CNN network GAN network and capturing videos with
different bit rates. Our experiments demonstrate that not only the models
trained on our dataset have better generalization capability and robustness
but also the trained images can maintain better edge contours and texture
features.
",0,0,1
Efficient Exact Inference in Planar Ising Models,"  We give polynomial-time algorithms for the exact computation of lowest-energy
(ground) states worst margin violators log partition functions and marginal
edge probabilities in certain binary undirected graphical models. Our approach
provides an interesting alternative to the well-known graph cut paradigm in
that it does not impose any submodularity constraints; instead we require
planarity to establish a correspondence with perfect matchings (dimer
coverings) in an expanded dual graph. We implement a unified framework while
delegating complex but well-understood subproblems (planar embedding
maximum-weight perfect matching) to established algorithms for which efficient
implementations are freely available. Unlike graph cut methods we can perform
penalized maximum-likelihood as well as maximum-margin parameter estimation in
the associated conditional random fields (CRFs) and employ marginal posterior
probabilities as well as maximum a posteriori (MAP) states for prediction.
Maximum-margin CRF parameter estimation on image denoising and segmentation
problems shows our approach to be efficient and effective. A C++ implementation
is available from http://nic.schraudolph.org/isinf/
",0,0,1
A Neural Network for Coordination Boundary Prediction,"  We propose a neural-network based model for coordination boundary prediction.
The network is designed to incorporate two signals: the similarity between
conjuncts and the observation that replacing the whole coordination phrase with
a conjunct tends to produce a coherent sentences. The modeling makes use of
several LSTM networks. The model is trained solely on conjunction annotations
in a Treebank without using external resources. We show improvements on
predicting coordination boundaries on the PTB compared to two state-of-the-art
parsers; as well as improvement over previous coordination boundary prediction
systems on the Genia corpus.
",0,1,0
"Boosting High-Level Vision with Joint Compression Artifacts Reduction
  and Super-Resolution","  Due to the limits of bandwidth and storage space digital images are usually
down-scaled and compressed when transmitted over networks resulting in loss of
details and jarring artifacts that can lower the performance of high-level
visual tasks. In this paper we aim to generate an artifact-free
high-resolution image from a low-resolution one compressed with an arbitrary
quality factor by exploring joint compression artifacts reduction (CAR) and
super-resolution (SR) tasks. First we propose a context-aware joint CAR and SR
neural network (CAJNN) that integrates both local and non-local features to
solve CAR and SR in one-stage. Finally a deep reconstruction network is
adopted to predict high quality and high-resolution images. Evaluation on CAR
and SR benchmark datasets shows that our CAJNN model outperforms previous
methods and also takes 26.2% shorter runtime. Based on this model we explore
addressing two critical challenges in high-level computer vision: optical
character recognition of low-resolution texts and extremely tiny face
detection. We demonstrate that CAJNN can serve as an effective image
preprocessing method and improve the accuracy for real-scene text recognition
(from 85.30% to 85.75%) and the average precision for tiny face detection (from
0.317 to 0.611).
",0,0,1
"Large-scale Continuous Gesture Recognition Using Convolutional Neural
  Networks","  This paper addresses the problem of continuous gesture recognition from
sequences of depth maps using convolutional neutral networks (ConvNets). The
proposed method first segments individual gestures from a depth sequence based
on quantity of movement (QOM). For each segmented gesture an Improved Depth
Motion Map (IDMM) which converts the depth sequence into one image is
constructed and fed to a ConvNet for recognition. The IDMM effectively encodes
both spatial and temporal information and allows the fine-tuning with existing
ConvNet models for classification without introducing millions of parameters to
learn. The proposed method is evaluated on the Large-scale Continuous Gesture
Recognition of the ChaLearn Looking at People (LAP) challenge 2016. It achieved
the performance of 0.2655 (Mean Jaccard Index) and ranked $3^rd$ place in
this challenge.
",0,0,1
Collaborative Decoding of Polynomial Codes for Distributed Computation,"  We show that polynomial codes (and some related codes) used for distributed
matrix multiplication are interleaved Reed-Solomon codes and hence can be
collaboratively decoded. We consider a fault tolerant setup where $t$ worker
nodes return erroneous values. For an additive random Gaussian error model we
show that for all $t < N-K$ errors can be corrected with probability 1.
Further numerical results show that in the presence of additive errors when
$L$ Reed-Solomon codes are collaboratively decoded the numerical stability in
recovering the error locator polynomial improves with increasing $L$.
",1,0,0
"Classification of Medical Images and Illustrations in the Biomedical
  Literature Using Synergic Deep Learning","  The Classification of medical images and illustrations in the literature aims
to label a medical image according to the modality it was produced or label an
illustration according to its production attributes. It is an essential and
challenging research hotspot in the area of automated literature review
retrieval and mining. The significant intra-class variation and inter-class
similarity caused by the diverse imaging modalities and various illustration
types brings a great deal of difficulties to the problem. In this paper we
propose a synergic deep learning (SDL) model to address this issue.
Specifically a dual deep convolutional neural network with a synergic signal
system is designed to mutually learn image representation. The synergic signal
is used to verify whether the input image pair belongs to the same category and
to give the corrective feedback if a synergic error exists. Our SDL model can
be trained 'end to end'. In the test phase the class label of an input can be
predicted by averaging the likelihood probabilities obtained by two
convolutional neural network components. Experimental results on the
ImageCLEF2016 Subfigure Classification Challenge suggest that our proposed SDL
model achieves the state-of-the art performance in this medical image
classification problem and its accuracy is higher than that of the first place
solution on the Challenge leader board so far.
",0,0,1
Generation of lyrics lines conditioned on music audio clips,"  We present a system for generating novel lyrics lines conditioned on music
audio. A bimodal neural network model learns to generate lines conditioned on
any given short audio clip. The model consists of a spectrogram variational
autoencoder (VAE) and a text VAE. Both automatic and human evaluations
demonstrate effectiveness of our model in generating lines that have an
emotional impact matching a given audio clip. The system is intended to serve
as a creativity tool for songwriters.
",0,1,0
"Hyperparameter Optimization and Boosting for Classifying Facial
  Expressions: How good can a ""Null"" Model be?","  One of the goals of the ICML workshop on representation and learning is to
establish benchmark scores for a new data set of labeled facial expressions.
This paper presents the performance of a ""Null"" model consisting of
convolutions with random weights PCA pooling normalization and a linear
readout. Our approach focused on hyperparameter optimization rather than novel
model components. On the Facial Expression Recognition Challenge held by the
Kaggle website our hyperparameter optimization approach achieved a score of
60% accuracy on the test data. This paper also introduces a new ensemble
construction variant that combines hyperparameter optimization with the
construction of ensembles. This algorithm constructed an ensemble of four
models that scored 65.5% accuracy. These scores rank 12th and 5th respectively
among the 56 challenge participants. It is worth noting that our approach was
developed prior to the release of the data set and applied without
modification; our strong competition performance suggests that the TPE
hyperparameter optimization algorithm and domain expertise encoded in our Null
model can generalize to new image classification data sets.
",0,0,1
"Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge
  Integration","  Relation prediction among entities in images is an important step in scene
graph generation (SGG) which further impacts various visual understanding and
reasoning tasks. Existing SGG frameworks however require heavy training yet
are incapable of modeling unseen (i.e.zero-shot) triplets. In this work we
stress that such incapability is due to the lack of commonsense reasoningi.e.
the ability to associate similar entities and infer similar relations based on
general understanding of the world. To fill this gap we propose
CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER) a framework to
integrate commonsense knowledge for SGG especially for zero-shot relation
prediction. Specifically we develop novel graph mining pipelines to model the
neighborhoods and paths around entities in an external commonsense knowledge
graph and integrate them on top of state-of-the-art SGG frameworks. Extensive
quantitative evaluations and qualitative case studies on both original and
manipulated datasets from Visual Genome demonstrate the effectiveness of our
proposed approach.
",0,0,1
Deep interpretable architecture for plant diseases classification,"  Recently many works have been inspired by the success of deep learning in
computer vision for plant diseases classification. Unfortunately these
end-to-end deep classifiers lack transparency which can limit their adoption in
practice. In this paper we propose a new trainable visualization method for
plant diseases classification based on a Convolutional Neural Network (CNN)
architecture composed of two deep classifiers. The first one is named Teacher
and the second one Student. This architecture leverages the multitask learning
to train the Teacher and the Student jointly. Then the communicated
representation between the Teacher and the Student is used as a proxy to
visualize the most important image regions for classification. This new
architecture produces sharper visualization than the existing methods in plant
diseases context. All experiments are achieved on PlantVillage dataset that
contains 54306 plant images.
",0,0,1
"Optimal Relay Probing in Millimeter Wave Cellular Systems with
  Device-to-Device Relaying","  Millimeter-wave (mmWave) cellular systems are power-limited and susceptible
to blockages. As a result mmWave connectivity will be likely to be
intermittent. One promising approach to increasing mmWave connectivity and
range is to use relays. Device-to-device (D2D) communications open the door to
the vast opportunities of D2D and device-to-network relaying for mmWave
cellular systems. In this correspondence we study how to select a good relay
for a given source-destination pair in a two-hop mmWave cellular system where
the mmWave links are subject to random Bernoulli blockages. In such a system
probing more relays could potentially lead to the discovery of a better relay
but at the cost of more overhead. We find that the throughput-optimal relay
probing strategy is a pure threshold policy: the system can stop relay probing
once the achievable spectral efficiency of the currently probed two-hop link
exceeds some threshold. In general the spectral efficiency threshold can be
obtained by solving a fixed point equation. For the special case with on/off
mmWave links we derive a closed-form solution for the threshold. Numerical
results demonstrate that the threshold-based relay probing strategy can yield
remarkable throughput gains.
",1,0,0
"Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical
  Procedures","  From a computer science viewpoint a surgical domain model needs to be a
conceptual one incorporating both behavior and data. It should therefore model
actors devices tools their complex interactions and data flow. To capture
and model these we take advantage of the latest computer vision methodologies
for generating 3D scene graphs from camera views. We then introduce the
Multimodal Semantic Scene Graph (MSSG) which aims at providing a unified
symbolic spatiotemporal and semantic representation of surgical procedures.
This methodology aims at modeling the relationship between different components
in surgical domain including medical staff imaging systems and surgical
devices opening the path towards holistic understanding and modeling of
surgical procedures. We then use MSSG to introduce a dynamically generated
graphical user interface tool for surgical procedure analysis which could be
used for many applications including process optimization OR design and
automatic report generation. We finally demonstrate that the proposed MSSGs
could also be used for synchronizing different complex surgical procedures.
While the system still needs to be integrated into real operating rooms before
getting validated this conference paper aims mainly at providing the community
with the basic principles of this novel concept through a first prototypal
partial realization based on MVOR dataset.
",0,0,1
"Automated Estimation of Total Lung Volume using Chest Radiographs and
  Deep Learning","  Total lung volume is an important quantitative biomarker and is used for the
assessment of restrictive lung diseases. In this study we investigate the
performance of several deep-learning approaches for automated measurement of
total lung volume from chest radiographs. 7621 posteroanterior and lateral view
chest radiographs (CXR) were collected from patients with chest CT available.
Similarly 928 CXR studies were chosen from patients with pulmonary function
test (PFT) results. The reference total lung volume was calculated from lung
segmentation on CT or PFT data respectively. This dataset was used to train
deep-learning architectures to predict total lung volume from chest
radiographs. The experiments were constructed in a step-wise fashion with
increasing complexity to demonstrate the effect of training with CT-derived
labels only and the sources of error. The optimal models were tested on 291 CXR
studies with reference lung volume obtained from PFT. The optimal deep-learning
regression model showed an MAE of 408 ml and a MAPE of 8.1% and Pearson's r =
0.92 using both frontal and lateral chest radiographs as input. CT-derived
labels were useful for pre-training but the optimal performance was obtained by
fine-tuning the network with PFT-derived labels. We demonstrate for the first
time that state-of-the-art deep learning solutions can accurately measure
total lung volume from plain chest radiographs. The proposed model can be used
to obtain total lung volume from routinely acquired chest radiographs at no
additional cost and could be a useful tool to identify trends over time in
patients referred regularly for chest x-rays.
",0,0,1
Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing,"  This paper investigates the problem of learning cross-lingual representations
in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT) a
simple and efficient approach to generate cross-lingual contextualized word
embeddings based on publicly available pre-trained BERT models (Devlin et al.
2018). In this approach a linear transformation is learned from contextual
word alignments to align the contextualized embeddings independently trained in
different languages. We demonstrate the effectiveness of this approach on
zero-shot cross-lingual transfer parsing. Experiments show that our embeddings
substantially outperform the previous state-of-the-art that uses static
embeddings. We further compare our approach with XLM (Lample and Conneau
2019) a recently proposed cross-lingual language model trained with massive
parallel data and achieve highly competitive results.
",0,1,0
ConVEx: Data-Efficient and Few-Shot Slot Labeling,"  We propose ConVEx (Conversational Value Extractor) an efficient pretraining
and fine-tuning neural approach for slot-labeling dialog tasks. Instead of
relying on more general pretraining objectives from prior work (e.g. language
modeling response selection) ConVEx's pretraining objective a novel pairwise
cloze task using Reddit data is well aligned with its intended usage on
sequence labeling tasks. This enables learning domain-specific slot labelers by
simply fine-tuning decoding layers of the pretrained general-purpose sequence
labeling model while the majority of the pretrained model's parameters are
kept frozen. We report state-of-the-art performance of ConVEx across a range of
diverse domains and data sets for dialog slot-labeling with the largest gains
in the most challenging few-shot setups. We believe that ConVEx's reduced
pretraining times (i.e. only 18 hours on 12 GPUs) and cost along with its
efficient fine-tuning and strong performance promise wider portability and
scalability for data-efficient sequence-labeling tasks in general.
",0,1,0
"Lifelong 3D Object Recognition and Grasp Synthesis Using Dual Memory
  Recurrent Self-Organization Networks","  Humans learn to recognize and manipulate new objects in lifelong settings
without forgetting the previously gained knowledge under non-stationary and
sequential conditions. In autonomous systems the agents also need to mitigate
similar behavior to continually learn the new object categories and adapt to
new environments. In most conventional deep neural networks this is not
possible due to the problem of catastrophic forgetting where the newly gained
knowledge overwrites existing representations. Furthermore most
state-of-the-art models excel either in recognizing the objects or in grasp
prediction while both tasks use visual input. The combined architecture to
tackle both tasks is very limited. In this paper we proposed a hybrid model
architecture consists of a dynamically growing dual-memory recurrent neural
network (GDM) and an autoencoder to tackle object recognition and grasping
simultaneously. The autoencoder network is responsible to extract a compact
representation for a given object which serves as input for the GDM learning
and is responsible to predict pixel-wise antipodal grasp configurations. The
GDM part is designed to recognize the object in both instances and categories
levels. We address the problem of catastrophic forgetting using the intrinsic
memory replay where the episodic memory periodically replays the neural
activation trajectories in the absence of external sensory information. To
extensively evaluate the proposed model in a lifelong setting we generate a
synthetic dataset due to lack of sequential 3D objects dataset. Experiment
results demonstrated that the proposed model can learn both object
representation and grasping simultaneously in continual learning scenarios.
",0,0,1
AirLab: Autograd Image Registration Laboratory,"  Medical image registration is an active research topic and forms a basis for
many medical image analysis tasks. Although image registration is a rather
general concept specialized methods are usually required to target a specific
registration problem. The development and implementation of such methods has
been tough so far as the gradient of the objective has to be computed. Also
its evaluation has to be performed preferably on a GPU for larger images and
for more complex transformation models and regularization terms. This hinders
researchers from rapid prototyping and poses hurdles to reproduce research
results. There is a clear need for an environment which hides this complexity
to put the modeling and the experimental exploration of registration methods
into the foreground. With the ""Autograd Image Registration Laboratory""
(AIRLab) we introduce an open laboratory for image registration tasks where
the analytic gradients of the objective function are computed automatically and
the device where the computations are performed on a CPU or a GPU is
transparent. It is meant as a laboratory for researchers and developers
enabling them to rapidly try out new ideas for registering images and to
reproduce registration results which have already been published. AIRLab is
implemented in Python using PyTorch as tensor and optimization library and
SimpleITK for basic image IO. Therefore it profits from recent advances made
by the machine learning community concerning optimization and deep neural
network models. The presented draft of this paper outlines AIRLab with first
code snippets and performance analyses. A more exhaustive introduction will
follow as a final version soon.
",0,0,1
"Sequence-based Person Attribute Recognition with Joint CTC-Attention
  Model","  Attribute recognition has become crucial because of its wide applications in
many computer vision tasks such as person re-identification. Like many object
recognition problems variations in viewpoints illumination and recognition
at far distance all make this task challenging. In this work we propose a
joint CTC-Attention model (JCM) which maps attribute labels into sequences to
learn the semantic relationship among attributes. Besides this network uses
neural network to encode images into sequences and employs connectionist
temporal classification (CTC) loss to train the network with the aim of
improving the encoding performance of the network. At the same time it adopts
the attention model to decode the sequences which can realize aligning the
sequences and better learning the semantic information from attributes.
Extensive experiments on three public datasets i.e. Market-1501 attribute
dataset Duke attribute dataset and PETA dataset demonstrate the effectiveness
of the proposed method.
",0,0,1
A Class of Novel STAP Algorithms Using Sparse Recovery Technique,"  A class of novel STAP algorithms based on sparse recovery technique were
presented. Intrinsic sparsity of distribution of clutter and target energy on
spatial-frequency plane was exploited from the viewpoint of compressed sensing.
The original sample data and distribution of target and clutter energy was
connected by a ill-posed linear algebraic equation and popular $L_1$
optimization method could be utilized to search for its solution with sparse
characteristic. Several new filtering algorithm acting on this solution were
designed to clean clutter component on spatial-frequency plane effectively for
detecting invisible targets buried in clutter. The method above is called
CS-STAP in general. CS-STAP showed their advantage compared with conventional
STAP technique such as SMI in two ways: Firstly the resolution of CS-STAP on
estimation for distribution of clutter and target energy is ultra-high such
that clutter energy might be annihilated almost completely by carefully tuned
filter. Output SCR of CS-STAP algorithms is far superior to the requirement of
detection; Secondly a much smaller size of training sample support compared
with SMI method is requested for CS-STAP method. Even with only one snapshot
(from target range cell) could CS-STAP method be able to reveal the existence
of target clearly. CS-STAP method display its great potential to be used in
heterogeneous situation. Experimental result on dataset from mountaintop
program has provided the evidence for our assertion on CS-STAP.
",1,0,0
"Generalized Intersection Algorithms with Fixpoints for Image
  Decomposition Learning","  In image processing classical methods minimize a suitable functional that
balances between computational feasibility (convexity of the functional is
ideal) and suitable penalties reflecting the desired image decomposition. The
fact that algorithms derived from such minimization problems can be used to
construct (deep) learning architectures has spurred the development of
algorithms that can be trained for a specifically desired image decomposition
e.g. into cartoon and texture. While many such methods are very successful
theoretical guarantees are only scarcely available. To this end in this
contribution we formalize a general class of intersection point problems
encompassing a wide range of (learned) image decomposition models and we give
an existence result for a large subclass of such problems i.e. giving the
existence of a fixpoint of the corresponding algorithm. This class generalizes
classical model-based variational problems such as the TV-l2 -model or the
more general TV-Hilbert model. To illustrate the potential for learned
algorithms novel (non learned) choices within our class show comparable
results in denoising and texture removal.
",0,0,1
"Brain Tumor Segmentation Based on Refined Fully Convolutional Neural
  Networks with A Hierarchical Dice Loss","  As a basic task in computer vision semantic segmentation can provide
fundamental information for object detection and instance segmentation to help
the artificial intelligence better understand real world. Since the proposal of
fully convolutional neural network (FCNN) it has been widely used in semantic
segmentation because of its high accuracy of pixel-wise classification as well
as high precision of localization. In this paper we apply several famous FCNN
to brain tumor segmentation making comparisons and adjusting network
architectures to achieve better performance measured by metrics such as
precision recall mean of intersection of union (mIoU) and dice score
coefficient (DSC). The adjustments to the classic FCNN include adding more
connections between convolutional layers enlarging decoders after up sample
layers and changing the way shallower layers' information is reused. Besides
the structure modification we also propose a new classifier with a
hierarchical dice loss. Inspired by the containing relationship between
classes the loss function converts multiple classification to multiple binary
classification in order to counteract the negative effect caused by imbalance
data set. Massive experiments have been done on the training set and testing
set in order to assess our refined fully convolutional neural networks and new
types of loss function. Competitive figures prove they are more effective than
their predecessors.
",0,0,1
"Channel Attention based Iterative Residual Learning for Depth Map
  Super-Resolution","  Despite the remarkable progresses made in deep-learning based depth map
super-resolution (DSR) how to tackle real-world degradation in low-resolution
(LR) depth maps remains a major challenge. Existing DSR model is generally
trained and tested on synthetic dataset which is very different from what
would get from a real depth sensor. In this paper we argue that DSR models
trained under this setting are restrictive and not effective in dealing with
real-world DSR tasks. We make two contributions in tackling real-world
degradation of different depth sensors. First we propose to classify the
generation of LR depth maps into two types: non-linear downsampling with noise
and interval downsampling for which DSR models are learned correspondingly.
Second we propose a new framework for real-world DSR which consists of four
modules : 1) An iterative residual learning module with deep supervision to
learn effective high-frequency components of depth maps in a coarse-to-fine
manner; 2) A channel attention strategy to enhance channels with abundant
high-frequency components; 3) A multi-stage fusion module to effectively
re-exploit the results in the coarse-to-fine process; and 4) A depth refinement
module to improve the depth map by TGV regularization and input loss. Extensive
experiments on benchmarking datasets demonstrate the superiority of our method
over current state-of-the-art DSR methods.
",0,0,1
Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods,"  Vision-based monocular human pose estimation as one of the most fundamental
and challenging problems in computer vision aims to obtain posture of the
human body from input images or video sequences. The recent developments of
deep learning techniques have been brought significant progress and remarkable
breakthroughs in the field of human pose estimation. This survey extensively
reviews the recent deep learning-based 2D and 3D human pose estimation methods
published since 2014. This paper summarizes the challenges main frameworks
benchmark datasets evaluation metrics performance comparison and discusses
some promising future research directions.
",0,0,1
"ANTNets: Mobile Convolutional Neural Networks for Resource Efficient
  Image Classification","  Deep convolutional neural networks have achieved remarkable success in
computer vision. However deep neural networks require large computing
resources to achieve high performance. Although depthwise separable convolution
can be an efficient module to approximate a standard convolution it often
leads to reduced representational power of networks. In this paper under
budget constraints such as computational cost (MAdds) and the parameter count
we propose a novel basic architectural block ANTBlock. It boosts the
representational power by modeling in a high dimensional space
interdependency of channels between a depthwise convolution layer and a
projection layer in the ANTBlocks. Our experiments show that ANTNet built by a
sequence of ANTBlocks consistently outperforms state-of-the-art low-cost
mobile convolutional neural networks across multiple datasets. On CIFAR100 our
model achieves 75.7% top-1 accuracy which is 1.5% higher than MobileNetV2 with
8.3% fewer parameters and 19.6% less computational cost. On ImageNet our model
achieves 72.8% top-1 accuracy which is 0.8% improvement with 157.7ms (20%
faster) on iPhone 5s over MobileNetV2.
",0,0,1
Simple unity among the fundamental equations of science,"  The Price equation describes the change in populations. Change concerns some
value such as biological fitness information or physical work. The Price
equation reveals universal aspects for the nature of change independently of
the meaning ascribed to values. By understanding those universal aspects we
can see more clearly why fundamental mathematical results in different
disciplines often share a common form. We can also interpret more clearly the
meaning of key results within each discipline. For example the mathematics of
natural selection in biology has a form closely related to information theory
and physical entropy. Does that mean that natural selection is about
information or entropy? Or do natural selection information and entropy arise
as interpretations of a common underlying abstraction? The Price equation
suggests the latter. The Price equation achieves its abstract generality by
partitioning change into two terms. The first term naturally associates with
the direct forces that cause change. The second term naturally associates with
the changing frame of reference. In the Price equation's canonical form total
change remains zero because the conservation of total probability requires that
all probabilities invariantly sum to one. Much of the shared common form for
the mathematics of different disciplines may arise from that seemingly trivial
invariance of total probability which leads to the partitioning of total
change into equal and opposite components of the direct forces and the changing
frame of reference.
",1,0,0
Low-power Secret-key Agreement over OFDM,"  Information-theoretic secret-key agreement is perhaps the most practically
feasible mechanism that provides unconditional security at the physical layer
to date. In this paper we consider the problem of secret-key agreement by
sharing randomness at low power over an orthogonal frequency division
multiplexing (OFDM) link in the presence of an eavesdropper. The low power
assumption greatly simplifies the design of the randomness sharing scheme even
in a fading channel scenario. We assess the performance of the proposed system
in terms of secrecy key rate and show that a practical approach to key sharing
is obtained by using low-density parity check (LDPC) codes for information
reconciliation. Numerical results confirm the merits of the proposed approach
as a feasible and practical solution. Moreover the outage formulation allows
to implement secret-key agreement even when only statistical knowledge of the
eavesdropper channel is available.
",1,0,0
Fast Rotational Sparse Coding,"  We propose an algorithm for rotational sparse coding along with an efficient
implementation using steerability. Sparse coding (also called dictionary
learning) is an important technique in image processing useful in inverse
problems compression and analysis; however the usual formulation fails to
capture an important aspect of the structure of images: images are formed from
building blocks e.g. edges lines or points that appear at different
locations orientations and scales. The sparse coding problem can be
reformulated to explicitly account for these transforms at the cost of
increased computation. In this work we propose an algorithm for a rotational
version of sparse coding that is based on K-SVD with additional rotation
operations. We then propose a method to accelerate these rotations by learning
the dictionary in a steerable basis. Our experiments on patch coding and
texture classification demonstrate that the proposed algorithm is fast enough
for practical use and compares favorably to standard sparse coding.
",0,0,1
"Rapid Fading Due to Human Blockage in Pedestrian Crowds at 5G
  Millimeter-Wave Frequencies","  Rapidly fading channels caused by pedestrians in dense urban environments
will have a significant impact on millimeter-wave (mmWave) communications
systems that employ electrically-steerable and narrow beamwidth antenna arrays.
A peer-to-peer (P2P) measurement campaign was conducted with 7-degree
15-degree and 60-degree half-power beamwidth (HPBW) antenna pairs at 73.5 GHz
and with 1 GHz of RF null-to-null bandwidth in a heavily populated open square
scenario in Brooklyn New York to study blockage events caused by typical
pedestrian traffic. Antenna beamwidths that range approximately an order of
magnitude were selected to gain knowledge of fading events for antennas with
different beamwidths since antenna patterns for mmWave systems will be
electronically-adjustable. Two simple modeling approaches in the literature are
introduced to characterize the blockage events by either a two-state Markov
model or a four-state piecewise linear modeling approach. Transition
probability rates are determined from the measurements and it is shown that
average fade durations with a -5 dB threshold are 299.0 ms for 7-degree HPBW
antennas and 260.2 ms for 60-degree HPBW antennas. The four-state piecewise
linear modeling approach shows that signal strength decay and rise times are
asymmetric for blockage events and that mean signal attenuations (average fade
depths) are inversely proportional to antenna HPBW where 7-degree and
60-degree HPBW antennas resulted in mean signal fades of 15.8 dB and 11.5 dB
respectively. The models presented herein are valuable for extending
statistical channel models at mmWave to accurately simulate real-world
pedestrian blockage events when designing fifth-generation (5G) wireless
systems.
",1,0,0
Pooling Faces: Template based Face Recognition with Pooled Face Images,"  We propose a novel approach to template based face recognition. Our dual goal
is to both increase recognition accuracy and reduce the computational and
storage costs of template matching. To do this we leverage on an approach
which was proven effective in many other domains but to our knowledge never
fully explored for face images: average pooling of face photos. We show how
(and why!) the space of a template's images can be partitioned and then pooled
based on image quality and head pose and the effect this has on accuracy and
template size. We perform extensive tests on the IJB-A and Janus CS2 template
based face identification and verification benchmarks. These show that not only
does our approach outperform published state of the art despite requiring far
fewer cross template comparisons but also surprisingly that image pooling
performs on par with deep feature pooling.
",0,0,1
Gate-Shift Networks for Video Action Recognition,"  Deep 3D CNNs for video action recognition are designed to learn powerful
representations in the joint spatio-temporal feature space. In practice
however because of the large number of parameters and computations involved
they may under-perform in the lack of sufficiently large datasets for training
them at scale. In this paper we introduce spatial gating in spatial-temporal
decomposition of 3D kernels. We implement this concept with Gate-Shift Module
(GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient
spatio-temporal feature extractor. With GSM plugged in a 2D-CNN learns to
adaptively route features through time and combine them at almost no
additional parameters and computational overhead. We perform an extensive
evaluation of the proposed module to study its effectiveness in video action
recognition achieving state-of-the-art results on Something Something-V1 and
Diving48 datasets and obtaining competitive results on EPIC-Kitchens with far
less model complexity.
",0,0,1
Cross-Modal Distillation for RGB-Depth Person Re-Identification,"  Person re-identification is a key challenge for surveillance across multiple
sensors. Prompted by the advent of powerful deep learning models for visual
recognition and inexpensive RGB-D cameras and sensor-rich mobile robotic
platforms e.g. self-driving vehicles we investigate the relatively unexplored
problem of cross-modal re-identification of persons between RGB (color) and
depth images. The considerable divergence in data distributions across
different sensor modalities introduces additional challenges to the typical
difficulties like distinct viewpoints occlusions and pose and illumination
variation. While some work has investigated re-identification across RGB and
infrared we take inspiration from successes in transfer learning from RGB to
depth in object detection tasks. Our main contribution is a novel method for
cross-modal distillation for robust person re-identification which learns a
shared feature representation space of person's appearance in both RGB and
depth images. In addition we propose a cross-modal attention mechanism where
the gating signal from one modality can dynamically activate the most
discriminant CNN filters of the other modality. The proposed distillation
method is compared to conventional and deep learning approaches proposed for
other cross-domain re-identification tasks. Results obtained on the public BIWI
and RobotPKU datasets indicate that the proposed method can significantly
outperform the state-of-the-art approaches by up to 16.1% in mean Average
Precision (mAP) demonstrating the benefit of the distillation paradigm. The
experimental results also indicate that using cross-modal attention allows to
improve recognition accuracy considerably with respect to the proposed
distillation method and relevant state-of-the-art approaches.
",0,0,1
"Semantics Altering Modifications for Evaluating Comprehension in Machine
  Reading","  Advances in NLP have yielded impressive results for the task of machine
reading comprehension (MRC) with approaches having been reported to achieve
performance comparable to that of humans. In this paper we investigate whether
state-of-the-art MRC models are able to correctly process Semantics Altering
Modifications (SAM): linguistically-motivated phenomena that alter the
semantics of a sentence while preserving most of its lexical surface form. We
present a method to automatically generate and align challenge sets featuring
original and altered examples. We further propose a novel evaluation
methodology to correctly assess the capability of MRC systems to process these
examples independent of the data they were optimised on by discounting for
effects introduced by domain shift. In a large-scale empirical study we apply
the methodology in order to evaluate extractive MRC models with regard to their
capability to correctly process SAM-enriched data. We comprehensively cover 12
different state-of-the-art neural architecture configurations and four training
datasets and find that -- despite their well-known remarkable performance --
optimised models consistently struggle to correctly process semantically
altered data.
",0,1,0
"Coding Theorems for a (22)-Threshold Scheme with Detectability of
  Impersonation Attacks","  In this paper we discuss coding theorems on a $(2 2)$--threshold scheme in
the presence of an opponent who impersonates one of the two shareholders in an
asymptotic setup. We consider a situation where $n$ secrets $S^n$ from a
memoryless source is blockwisely encoded to two shares and the two shares are
decoded to $S^n$ with permitting negligible decoding error. We introduce
correlation level of the two shares and characterize the minimum attainable
rates of the shares and a uniform random number for realizing a $(2
2)$--threshold scheme that is secure against the impersonation attack by an
opponent. It is shown that if the correlation level between the two shares
equals to an $ell ge 0$ the minimum attainable rates coincide with
$H(S)+ell$ where $H(S)$ denotes the entropy of the source and the maximum
attainable exponent of the success probability of the impersonation attack
equals to $ell$. We also give a simple construction of an encoder and a
decoder using an ordinary $(22)$--threshold scheme where the two shares are
correlated and attains all the bounds.
",1,0,0
Visual Question Generation for Class Acquisition of Unknown Objects,"  Traditional image recognition methods only consider objects belonging to
already learned classes. However since training a recognition model with every
object class in the world is unfeasible a way of getting information on
unknown objects (i.e. objects whose class has not been learned) is necessary.
A way for an image recognition system to learn new classes could be asking a
human about objects that are unknown. In this paper we propose a method for
generating questions about unknown objects in an image as means to get
information about classes that have not been learned. Our method consists of a
module for proposing objects a module for identifying unknown objects and a
module for generating questions about unknown objects. The experimental results
via human evaluation show that our method can successfully get information
about unknown objects in an image dataset. Our code and dataset are available
at https://github.com/mil-tokyo/vqg-unknown.
",0,0,1
"Attention-Guided Discriminative Region Localization and Label
  Distribution Learning for Bone Age Assessment","  Bone age assessment (BAA) is clinically important as it can be used to
diagnose endocrine and metabolic disorders during child development. Existing
deep learning based methods for classifying bone age use the global image as
input or exploit local information by annotating extra bounding boxes or key
points. However training with the global image underutilizes discriminative
local information while providing extra annotations is expensive and
subjective. In this paper we propose an attention-guided approach to
automatically localize the discriminative regions for BAA without any extra
annotations. Specifically we first train a classification model to learn the
attention maps of the discriminative regions finding the hand region the most
discriminative region (the carpal bones) and the next most discriminative
region (the metacarpal bones). Guided by those attention maps we then crop the
informative local regions from the original image and aggregate different
regions for BAA. Instead of taking BAA as a general regression task which is
suboptimal due to the label ambiguity problem in the age label space we
propose using joint age distribution learning and expectation regression which
makes use of the ordinal relationship among hand images with different
individual ages and leads to more robust age estimation. Extensive experiments
are conducted on the RSNA pediatric bone age data set. Using no training
annotations our method achieves competitive results compared with existing
state-of-the-art semi-automatic deep learning-based methods that require manual
annotation. Code is available at https:
//github.com/chenchao666/Bone-Age-Assessment.
",0,0,1
Augmenting Imitation Experience via Equivariant Representations,"  The robustness of visual navigation policies trained through imitation often
hinges on the augmentation of the training image-action pairs. Traditionally
this has been done by collecting data from multiple cameras by using standard
data augmentations from computer vision such as adding random noise to each
image or by synthesizing training images. In this paper we show that there is
another practical alternative for data augmentation for visual navigation based
on extrapolating viewpoint embeddings and actions nearby the ones observed in
the training data. Our method makes use of the geometry of the visual
navigation problem in 2D and 3D and relies on policies that are functions of
equivariant embeddings as opposed to images. Given an image-action pair from a
training navigation dataset our neural network model predicts the latent
representations of images at nearby viewpoints using the equivariance
property and augments the dataset. We then train a policy on the augmented
dataset. Our simulation results indicate that policies trained in this way
exhibit reduced cross-track error and require fewer interventions compared to
policies trained using standard augmentation methods. We also show similar
results in autonomous visual navigation by a real ground robot along a path of
over 500m.
",0,0,1
"Deep Network for Scatterer Distribution Estimation for Ultrasound Image
  Simulation","  Simulation-based ultrasound training can be an essential educational tool.
Realistic ultrasound image appearance with typical speckle texture can be
modeled as convolution of a point spread function with point scatterers
representing tissue microstructure. Such scatterer distribution however is in
general not known and its estimation for a given tissue type is fundamentally
an ill-posed inverse problem. In this paper we demonstrate a convolutional
neural network approach for probabilistic scatterer estimation from observed
ultrasound data. We herein propose to impose a known statistical distribution
on scatterers and learn the mapping between ultrasound image and distribution
parameter map by training a convolutional neural network on synthetic images.
In comparison with several existing approaches we demonstrate in numerical
simulations and with in-vivo images that the synthesized images from scatterer
representations estimated with our approach closely match the observations with
varying acquisition parameters such as compression and rotation of the imaged
domain.
",0,0,1
"Hardness Results on Finding Leafless Elementary Trapping Sets and
  Elementary Absorbing Sets of LDPC Codes","  Leafless elementary trapping sets (LETSs) are known to be the problematic
structures in the error floor region of low-density parity-check (LDPC) codes
over the additive white Gaussian (AWGN) channel under iterative decoding
algorithms. While problems involving the general category of trapping sets and
the subcategory of elementary trapping sets (ETSs) have been shown to be
NP-hard similar results for LETSs which are a subset of ETSs are not
available. In this paper we prove that for a general LDPC code finding a
LETS of a given size a with minimum number of unsatisfied check nodes b is
NP-hard to approximate with any guaranteed precision. We also prove that
finding the minimum size a of a LETS with a given b is NP-hard to approximate.
Similar results are proved for elementary absorbing sets a popular subcategory
of LETSs.
",1,0,0
"Private Computation of Systematically Encoded Data with Colluding
  Servers","  Private Computation (PC) recently introduced by Sun and Jafar is a
generalization of Private Information Retrieval (PIR) in which a user wishes to
privately compute an arbitrary function of data stored across several servers.
We construct a PC scheme which accounts for server collusion coded data and
non-linear functions. For data replicated over several possibly colluding
servers our scheme computes arbitrary functions of the data with rate equal to
the asymptotic capacity of PIR for this setup. For systematically encoded data
stored over colluding servers we privately compute arbitrary functions of the
columns of the data matrix and calculate the rate explicitly for polynomial
functions. The scheme is a generalization of previously studied star-product
PIR schemes.
",1,0,0
"Training Deep Neural Networks to Detect Repeatable 2D Features Using
  Large Amounts of 3D World Capture Data","  Image space feature detection is the act of selecting points or parts of an
image that are easy to distinguish from the surrounding image region. By
combining a repeatable point detection with a descriptor parts of an image can
be matched with one another which is useful in applications like estimating
pose from camera input or rectifying images. Recently precise indoor tracking
has started to become important for Augmented and Virtual reality as it is
necessary to allow positioning of a headset in 3D space without the need for
external tracking devices. Several modern feature detectors use homographies to
simulate different viewpoints not only to train feature detection and
description but test them as well. The problem is that often views of indoor
spaces contain high depth disparity. This makes the approximation that a
homography applied to an image represents a viewpoint change inaccurate. We
claim that in order to train detectors to work well in indoor environments
they must be robust to this type of geometry and repeatable under true
viewpoint change instead of homographies. Here we focus on the problem of
detecting repeatable feature locations under true viewpoint change. To this
end we generate labeled 2D images from a photo-realistic 3D dataset. These
images are used for training a neural network based feature detector. We
further present an algorithm for automatically generating labels of repeatable
2D features and present a fast easy to use test algorithm for evaluating a
detector in an 3D environment.
",0,0,1
Towards Near-imperceptible Steganographic Text,"  We show that the imperceptibility of several existing linguistic
steganographic systems (Fang et al. 2017; Yang et al. 2018) relies on
implicit assumptions on statistical behaviors of fluent text. We formally
analyze them and empirically evaluate these assumptions. Furthermore based on
these observations we propose an encoding algorithm called patient-Huffman
with improved near-imperceptible guarantees.
",0,1,0
Fairness Comparison of Uplink NOMA and OMA,"  In this paper we compare the resource allocation fairness of uplink
communications between non-orthogonal multiple access (NOMA) schemes and
orthogonal multiple access (OMA) schemes. Through characterizing the
contribution of the individual user data rate to the system sum rate we
analyze the fundamental reasons that NOMA offers a more fair resource
allocation than that of OMA in asymmetric channels. Furthermore a fairness
indicator metric based on Jain's index is proposed to measure the asymmetry of
multiuser channels. More importantly the proposed metric provides a selection
criterion for choosing between NOMA and OMA for fair resource allocation. Based
on this discussion we propose a hybrid NOMA-OMA scheme to further enhance the
users fairness. Simulation results confirm the accuracy of the proposed metric
and demonstrate the fairness enhancement of the proposed hybrid NOMA-OMA scheme
compared to the conventional OMA and NOMA schemes.
",1,0,0
Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,"  Few-shot Knowledge Graph (KG) completion is a focus of current research
where each task aims at querying unseen facts of a relation given its few-shot
reference entity pairs. Recent attempts solve this problem by learning static
representations of entities and references ignoring their dynamic properties
i.e. entities may exhibit diverse roles within task relations and references
may make different contributions to queries. This work proposes an adaptive
attentional network for few-shot KG completion by learning adaptive entity and
reference representations. Specifically entities are modeled by an adaptive
neighbor encoder to discern their task-oriented roles while references are
modeled by an adaptive query-aware aggregator to differentiate their
contributions. Through the attention mechanism both entities and references
can capture their fine-grained semantic meanings and thus render more
expressive representations. This will be more predictive for knowledge
acquisition in the few-shot scenario. Evaluation in link prediction on two
public datasets shows that our approach achieves new state-of-the-art results
with different few-shot sizes.
",0,1,0
Active Learning for Visual Question Answering: An Empirical Study,"  We present an empirical study of active learning for Visual Question
Answering where a deep VQA model selects informative question-image pairs from
a pool and queries an oracle for answers to maximally improve its performance
under a limited query budget. Drawing analogies from human learning we explore
cramming (entropy) curiosity-driven (expected model change) and goal-driven
(expected error reduction) active learning approaches and propose a fast and
effective goal-driven active learning scoring function to pick question-image
pairs for deep VQA models under the Bayesian Neural Network framework. We find
that deep VQA models need large amounts of training data before they can start
asking informative questions. But once they do all three approaches outperform
the random selection baseline and achieve significant query savings. For the
scenario where the model is allowed to ask generic questions about images but
is evaluated only on specific questions (e.g. questions whose answer is either
yes or no) our proposed goal-driven scoring function performs the best.
",0,0,1
"Learning to Regress Bodies from Images using Differentiable Semantic
  Rendering","  Learning to regress 3D human body shape and pose (e.g.~SMPL parameters) from
monocular images typically exploits losses on 2D keypoints silhouettes and/or
part-segmentation when 3D training data is not available. Such losses however
are limited because 2D keypoints do not supervise body shape and segmentations
of people in clothing do not match projected minimally-clothed SMPL shapes. To
exploit richer image information about clothed people we introduce
higher-level semantic information about clothing to penalize clothed and
non-clothed regions of the image differently. To do so we train a body
regressor using a novel Differentiable Semantic Rendering - DSR loss. For
Minimally-Clothed regions we define the DSR-MC loss which encourages a tight
match between a rendered SMPL body and the minimally-clothed regions of the
image. For clothed regions we define the DSR-C loss to encourage the rendered
SMPL body to be inside the clothing mask. To ensure end-to-end differentiable
training we learn a semantic clothing prior for SMPL vertices from thousands
of clothed human scans. We perform extensive qualitative and quantitative
experiments to evaluate the role of clothing semantics on the accuracy of 3D
human pose and shape estimation. We outperform all previous state-of-the-art
methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code
and trained models are available for research at https://dsr.is.tue.mpg.de/.
",0,0,1
"Semi-Dense Visual Odometry for RGB-D Cameras Using Approximate Nearest
  Neighbour Fields","  This paper presents a robust and efficient semi-dense visual odometry
solution for RGB-D cameras. The core of our method is a 2D-3D ICP pipeline
which estimates the pose of the sensor by registering the projection of a 3D
semi-dense map of the reference frame with the 2D semi-dense region extracted
in the current frame. The processing is speeded up by efficiently implemented
approximate nearest neighbour fields under the Euclidean distance criterion
which permits the use of compact Gauss-Newton updates in the optimization. The
registration is formulated as a maximum a posterior problem to deal with
outliers and sensor noises and consequently the equivalent weighted least
squares problem is solved by iteratively reweighted least squares method. A
variety of robust weight functions are tested and the optimum is determined
based on the characteristics of the sensor model. Extensive evaluation on
publicly available RGB-D datasets shows that the proposed method predominantly
outperforms existing state-of-the-art methods.
",0,0,1
Stabilized Medical Image Attacks,"  Convolutional Neural Networks (CNNs) have advanced existing medical systems
for automatic disease diagnosis. However a threat to these systems arises that
adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a
negative influence on human healthcare. There is a need to investigate
potential adversarial attacks to robustify deep medical diagnosis systems. On
the other side there are several modalities of medical images (e.g. CT
fundus and endoscopic image) of which each type is significantly different
from others. It is more challenging to generate adversarial perturbations for
different types of medical images. In this paper we propose an image-based
medical adversarial attack method to consistently produce adversarial
perturbations on medical images. The objective function of our method consists
of a loss deviation term and a loss stabilization term. The loss deviation term
increases the divergence between the CNN prediction of an adversarial example
and its ground truth label. Meanwhile the loss stabilization term ensures
similar CNN predictions of this example and its smoothed input. From the
perspective of the whole iterations for perturbation generation the proposed
loss stabilization term exhaustively searches the perturbation space to smooth
the single spot for local optimum escape. We further analyze the KL-divergence
of the proposed loss function and find that the loss stabilization term makes
the perturbations updated towards a fixed objective spot while deviating from
the ground truth. This stabilization ensures the proposed medical attack
effective for different types of medical images while producing perturbations
in small variance. Experiments on several medical image analysis benchmarks
including the recent COVID-19 dataset show the stability of the proposed
method.
",0,0,1
Entanglement-Assisted Quantum Data Compression,"  Ask how the quantum compression of ensembles of pure states is affected by
the availability of entanglement and in settings where the encoder has access
to side information. We find the optimal asymptotic quantum rate and the
optimal tradeoff (rate region) of quantum and entanglement rates. It turns out
that the amount by which the quantum rate beats the Schumacher limit the
entropy of the source is precisely half the entropy of classical information
that can be extracted from the source and side information states without
disturbing them at all (""reversible extraction of classical information"").
  In the special case that the encoder has no side information or that she has
access to the identity of the states this problem reduces to the known
settings of blind and visible Schumacher compression respectively albeit here
additionally with entanglement assistance. We comment on connections to
previously studied and further rate tradeoffs when also classical information
is considered.
",1,0,0
"Hardware-In-the-Loop Measurements of the Multi-Carrier Compressed
  Sensing Multi-User Detection (MCSM) System","  MCSM is a recently proposed novel system concept to solve the massive access
problem envisioned in future communication systems like 5G and industry 4.0
systems. This work focuses on the practical verification of the theoretical
gains that MCSM provides using a Hardware-In-the-Loop (HIL) measurement setup.
We present results in two different scenarios: (i) a LoS lab setup and (ii) a
non-LoS machine hall. In both scenarios MCSM shows promising performance in
terms of the number of supported users and the achieved reliability.
",1,0,0
Single Shot Multitask Pedestrian Detection and Behavior Prediction,"  Detecting and predicting the behavior of pedestrians is extremely crucial for
self-driving vehicles to plan and interact with them safely. Although there
have been several research works in this area it is important to have fast and
memory efficient models such that it can operate in embedded hardware in these
autonomous machines. In this work we propose a novel architecture using
spatial-temporal multi-tasking to do camera based pedestrian detection and
intention prediction. Our approach significantly reduces the latency by being
able to detect and predict all pedestrians' intention in a single shot manner
while also being able to attain better accuracy by sharing features with
relevant object level information and interactions.
",0,0,1
"Learning Pixel-level Semantic Affinity with Image-level Supervision for
  Weakly Supervised Semantic Segmentation","  The deficiency of segmentation labels is one of the main obstacles to
semantic segmentation in the wild. To alleviate this issue we present a novel
framework that generates segmentation labels of images given their image-level
class labels. In this weakly supervised setting trained models have been known
to segment local discriminative parts rather than the entire object area. Our
solution is to propagate such local responses to nearby areas which belong to
the same semantic entity. To this end we propose a Deep Neural Network (DNN)
called AffinityNet that predicts semantic affinity between a pair of adjacent
image coordinates. The semantic propagation is then realized by random walk
with the affinities predicted by AffinityNet. More importantly the supervision
employed to train AffinityNet is given by the initial discriminative part
segmentation which is incomplete as a segmentation annotation but sufficient
for learning semantic affinities within small image areas. Thus the entire
framework relies only on image-level class labels and does not require any
extra data or annotations. On the PASCAL VOC 2012 dataset a DNN learned with
segmentation labels generated by our method outperforms previous models trained
with the same level of supervision and is even as competitive as those relying
on stronger supervision.
",0,0,1
"A Structurally Regularized Convolutional Neural Network for Image
  Classification using Wavelet-based SubBand Decomposition","  We propose a convolutional neural network (CNN) architecture for image
classification based on subband decomposition of the image using wavelets. The
proposed architecture decomposes the input image spectra into multiple
critically sampled subbands extracts features using a single CNN per subband
and finally performs classification by combining the extracted features using
a fully connected layer. Processing each of the subbands by an individual CNN
thereby limiting the learning scope of each CNN to a single subband imposes a
form of structural regularization. This provides better generalization
capability as seen by the presented results. The proposed architecture achieves
best-in-class performance in terms of total multiply-add-accumulator operations
and nearly best-in-class performance in terms of total parameters required yet
it maintains competitive classification performance. We also show the proposed
architecture is more robust than the regular full-band CNN to noise caused by
weight-and-bias quantization and input quantization.
",0,0,1
GGNN: Graph-based GPU Nearest Neighbor Search,"  Approximate nearest neighbor (ANN) search in high dimensions is an integral
part of several computer vision systems and gains importance in deep learning
with explicit memory representations. Since PQT and FAISS started to leverage
the massive parallelism offered by GPUs GPU-based implementations are a
crucial resource for today's state-of-the-art ANN methods. While most of these
methods allow for faster queries less emphasis is devoted to accelerate the
construction of the underlying index structures. In this paper we propose a
novel search structure based on nearest neighbor graphs and information
propagation on graphs. Our method is designed to take advantage of GPU
architectures to accelerate the hierarchical building of the index structure
and for performing the query. Empirical evaluation shows that GGNN
significantly surpasses the state-of-the-art GPU- and CPU-based systems in
terms of build-time accuracy and search speed.
",0,0,1
Text Flow: A Unified Text Detection System in Natural Scene Images,"  The prevalent scene text detection approach follows four sequential steps
comprising character candidate detection false character candidate removal
text line extraction and text line verification. However errors occur and
accumulate throughout each of these sequential steps which often lead to low
detection performance. To address these issues we propose a unified scene text
detection system namely Text Flow by utilizing the minimum cost (min-cost)
flow network model. With character candidates detected by cascade boosting the
min-cost flow network model integrates the last three sequential steps into a
single process which solves the error accumulation problem at both character
level and text line level effectively. The proposed technique has been tested
on three public datasets i.e ICDAR2011 dataset ICDAR2013 dataset and a
multilingual dataset and it outperforms the state-of-the-art methods on all
three datasets with much higher recall and F-score. The good performance on the
multilingual dataset shows that the proposed technique can be used for the
detection of texts in different languages.
",0,0,1
"Attention-Augmented End-to-End Multi-Task Learning for Emotion
  Prediction from Speech","  Despite the increasing research interest in end-to-end learning systems for
speech emotion recognition conventional systems either suffer from the
overfitting due in part to the limited training data or do not explicitly
consider the different contributions of automatically learnt representations
for a specific task. In this contribution we propose a novel end-to-end
framework which is enhanced by learning other auxiliary tasks and an attention
mechanism. That is we jointly train an end-to-end network with several
different but related emotion prediction tasks i.e. arousal valence and
dominance predictions to extract more robust representations shared among
various tasks than traditional systems with the hope that it is able to relieve
the overfitting problem. Meanwhile an attention layer is implemented on top of
the layers for each task with the aim to capture the contribution distribution
of different segment parts for each individual task. To evaluate the
effectiveness of the proposed system we conducted a set of experiments on the
widely used database IEMOCAP. The empirical results show that the proposed
systems significantly outperform corresponding baseline systems.
",0,1,0
"Complex Block Floating-Point Format with Box Encoding For Wordlength
  Reduction in Communication Systems","  We propose a new complex block floating-point format to reduce implementation
complexity. The new format achieves wordlength reduction by sharing an exponent
across the block of samples and uses box encoding for the shared exponent to
reduce quantization error. Arithmetic operations are performed on blocks of
samples at time which can also reduce implementation complexity. For a case
study of a baseband quadrature amplitude modulation (QAM) transmitter and
receiver we quantify the tradeoffs in signal quality vs. implementation
complexity using the new approach to represent IQ samples. Signal quality is
measured using error vector magnitude (EVM) in the receiver and implementation
complexity is measured in terms of arithmetic complexity as well as memory
allocation and memory input/output rates. The primary contributions of this
paper are (1) a complex block floating-point format with box encoding of the
shared exponent to reduce quantization error (2) arithmetic operations using
the new complex block floating-point format and (3) a QAM transceiver case
study to quantify signal quality vs. implementation complexity tradeoffs using
the new format and arithmetic operations.
",1,0,0
AdaFrame: Adaptive Frame Selection for Fast Video Recognition,"  We present AdaFrame a framework that adaptively selects relevant frames on a
per-input basis for fast video recognition. AdaFrame contains a Long Short-Term
Memory network augmented with a global memory that provides context information
for searching which frames to use over time. Trained with policy gradient
methods AdaFrame generates a prediction determines which frame to observe
next and computes the utility i.e. expected future rewards of seeing more
frames at each time step. At testing time AdaFrame exploits predicted
utilities to achieve adaptive lookahead inference such that the overall
computational costs are reduced without incurring a decrease in accuracy.
Extensive experiments are conducted on two large-scale video benchmarks FCVID
and ActivityNet. AdaFrame matches the performance of using all frames with only
8.21 and 8.65 frames on FCVID and ActivityNet respectively. We further
qualitatively demonstrate learned frame usage can indicate the difficulty of
making classification decisions; easier samples need fewer frames while harder
ones require more both at instance-level within the same class and at
class-level among different categories.
",0,0,1
"Skin Lesion Diagnosis using Ensembles Unscaled Multi-Crop Evaluation
  and Loss Weighting","  In this paper we present the methods of our submission to the ISIC 2018
challenge for skin lesion diagnosis (Task 3). The dataset consists of 10000
images with seven image-level classes to be distinguished by an automated
algorithm. We employ an ensemble of convolutional neural networks for this
task. In particular we fine-tune pretrained state-of-the-art deep learning
models such as Densenet SENet and ResNeXt. We identify heavy class imbalance
as a key problem for this challenge and consider multiple balancing approaches
such as loss weighting and balanced batch sampling. Another important feature
of our pipeline is the use of a vast amount of unscaled crops for evaluation.
Last we consider meta learning approaches for the final predictions. Our team
placed second at the challenge while being the best approach using only
publicly available data.
",0,0,1
Symmetric Two-User Gaussian Interference Channel with Common Messages,"  We consider symmetric two-user Gaussian interference channel with common
messages. We derive an upper bound on the sum capacity and show that the upper
bound is tight in the low interference regime where the optimal transmission
scheme is to send no common messages and each receiver treats interference as
noise. Our result shows that although the availability of common messages
provides a cooperation opportunity for transmitters in the low interference
regime the presence of common messages does not help increase the sum capacity.
",1,0,0
"D2D User Selection For Simultaneous Spectrum Sharing And Energy
  Harvesting","  This paper presents a device-to-device (D2D) user selection protocol wherein
multiple D2D pairs coexist with a cellular network. In the developed framework
certain D2D users harvest energy and share the spectrum of the cellular users
by adopting a hybrid time switching and power splitting protocol. The D2D user
which harvests the maximum energy and achieves the desired target rate for the
cellular communication is selected to serve as a decode-and-forward (DF) relay
for the cellular user. The proposed work analyzes the impact of increase in the
number of D2D users on the performance of cellular user as well as derives an
upper bound on the time duration of energy harvesting within which best
possible rate for cellular user can be obtained. The performance of the
proposed protocol has been quantified by obtaining the closed form expressions
of outage probability.
",1,0,0
"SDC - Stacked Dilated Convolution: A Unified Descriptor Network for
  Dense Matching Tasks","  Dense pixel matching is important for many computer vision tasks such as
disparity and flow estimation. We present a robust unified descriptor network
that considers a large context region with high spatial variance. Our network
has a very large receptive field and avoids striding layers to maintain spatial
resolution. These properties are achieved by creating a novel neural network
layer that consists of multiple parallel stacked dilated convolutions (SDC).
Several of these layers are combined to form our SDC descriptor network. In our
experiments we show that our SDC features outperform state-of-the-art feature
descriptors in terms of accuracy and robustness. In addition we demonstrate
the superior performance of SDC in state-of-the-art stereo matching optical
flow and scene flow algorithms on several famous public benchmarks.
",0,0,1
"New Optimal Binary Sequences with Period $4p$ via Interleaving
  Ding-Helleseth-Lam Sequences","  Binary sequences with optimal autocorrelation play important roles in radar
communication and cryptography. Finding new binary sequences with optimal
autocorrelation has been an interesting research topic in sequence design.
Ding-Helleseth-Lam sequences are such a class of binary sequences of period
$p$ where $p$ is an odd prime with $pequiv 1(bmod~4)$. The objective of this
letter is to present a construction of binary sequences of period $4p$ via
interleaving four suitable Ding-Helleseth-Lam sequences. This construction
generates new binary sequences with optimal autocorrelation which can not be
produced by earlier ones.
",1,0,0
Joint Relay-User Beamforming Design in Full-Duplex Two-Way Relay Channel,"  A full-duplex two-way relay channel with multiple antennas is considered. For
this three-node network the beamforming design needs to suppress
self-interference. While a traditional way is to apply zero-forcing for
self-interference mitigation it may harm the desired signals. In this paper a
design which reserves a fraction of self-interference is proposed by solving a
quality-of-service constrained beamforming design problem. Since the problem is
challenging due to the loop self-interference a convergence-guaranteed
alternating optimization algorithm is proposed to jointly design the relay-user
beamformers. Numerical results show that the proposed scheme outperforms
zero-forcing method and achieves a transmit power close to the ideal case.
",1,0,0
The Weight Enumerator of Three Families of Cyclic Codes,"  Cyclic codes are a subclass of linear codes and have wide applications in
consumer electronics data storage systems and communication systems due to
their efficient encoding and decoding algorithms. Cyclic codes with many zeros
and their dual codes have been a subject of study for many years. However
their weight distributions are known only for a very small number of cases. In
general the calculation of the weight distribution of cyclic codes is heavily
based on the evaluation of some exponential sums over finite fields. Very
recently Li Hu Feng and Ge studied a class of $p$-ary cyclic codes of length
$p^2m-1$ where $p$ is a prime and $m$ is odd. They determined the weight
distribution of this class of cyclic codes by establishing a connection between
the involved exponential sums with the spectrum of Hermitian forms graphs. In
this paper this class of $p$-ary cyclic codes is generalized and the weight
distribution of the generalized cyclic codes is settled for both even $m$ and
odd $m$ alone with the idea of Li Hu Feng and Ge. The weight distributions
of two related families of cyclic codes are also determined.
",1,0,0
"Optimal Repair of MDS Codes in Distributed Storage via Subspace
  Interference Alignment","  It is well known that an (nk) code can be used to store 'k' units of
information in 'n' unit-capacity disks of a distributed data storage system. If
the code used is maximum distance separable (MDS) then the system can tolerate
any (n-k) disk failures since the original information can be recovered from
any k surviving disks. The focus of this paper is the design of a systematic
MDS code with the additional property that a single disk failure can be
repaired with minimum repair bandwidth i.e. with the minimum possible amount
of data to be downloaded for recovery of the failed disk. Previously a lower
bound of (n-1)/(n-k) units has been established by Dimakis et. al on the
repair bandwidth for a single disk failure in an (nk) MDS code . Recently the
existence of asymptotic codes achieving this lower bound for arbitrary (nk)
has been established by drawing connections to interference alignment. While
the existence of asymptotic constructions achieving this lower bound have been
shown finite code constructions achieving this lower bound existed in previous
literature only for the special (high-redundancy) scenario where $k leq
max(n/23)$. The question of existence of finite codes for arbitrary values of
(nk) achieving the lower bound on the repair bandwidth remained open. In this
paper by using permutation coding sub-matrices we provide the first known
finite MDS code which achieves the optimal repair bandwidth of (n-1)/(n-k) for
arbitrary (nk) for recovery of a failed systematic disk. We also generalize
our permutation matrix based constructions by developing a novel framework for
repair-bandwidth-optimal MDS codes based on the idea of subspace interference
alignment - a concept previously introduced by Suh and Tse the context of
wireless cellular networks.
",1,0,0
Learning to Compare: Relation Network for Few-Shot Learning,"  We present a conceptually simple flexible and general framework for
few-shot learning where a classifier must learn to recognise new classes given
only few examples from each. Our method called the Relation Network (RN) is
trained end-to-end from scratch. During meta-learning it learns to learn a
deep distance metric to compare a small number of images within episodes each
of which is designed to simulate the few-shot setting. Once trained a RN is
able to classify images of new classes by computing relation scores between
query images and the few examples of each new class without further updating
the network. Besides providing improved performance on few-shot learning our
framework is easily extended to zero-shot learning. Extensive experiments on
five benchmarks demonstrate that our simple approach provides a unified and
effective approach for both of these two tasks.
",0,0,1
Communicating with sentences: A multi-word naming game model,"  Naming game simulates the process of naming an object by a single word in
which a population of communicating agents can reach global consensus
asymptotically through iteratively pair-wise conversations. We propose an
extension of the single-word model to a multi-word naming game (MWNG)
simulating the case of describing a complex object by a sentence (multiple
words). Words are defined in categories and then organized as sentences by
combining them from different categories. We refer to a formatted combination
of several words as a pattern. In such an MWNG through a pair-wise
conversation it requires the hearer to achieve consensus with the speaker with
respect to both every single word in the sentence as well as the sentence
pattern so as to guarantee the correct meaning of the saying otherwise they
fail reaching consensus in the interaction. We validate the model in three
typical topologies as the underlying communication network and employ both
conventional and man-designed patterns in performing the MWNG.
",0,1,0
"Subtitles to Segmentation: Improving Low-Resource Speech-to-Text
  Translation Pipelines","  In this work we focus on improving ASR output segmentation in the context of
low-resource language speech-to-text translation. ASR output segmentation is
crucial as ASR systems segment the input audio using purely acoustic
information and are not guaranteed to output sentence-like segments. Since most
MT systems expect sentences as input feeding in longer unsegmented passages
can lead to sub-optimal performance. We explore the feasibility of using
datasets of subtitles from TV shows and movies to train better ASR segmentation
models. We further incorporate part-of-speech (POS) tag and dependency label
information (derived from the unsegmented ASR outputs) into our segmentation
model. We show that this noisy syntactic information can improve model
accuracy. We evaluate our models intrinsically on segmentation quality and
extrinsically on downstream MT performance as well as downstream tasks
including cross-lingual information retrieval (CLIR) tasks and human relevance
assessments. Our model shows improved performance on downstream tasks for
Lithuanian and Bulgarian.
",0,1,0
"SE3M: A Model for Software Effort Estimation Using Pre-trained Embedding
  Models","  Estimating effort based on requirement texts presents many challenges
especially in obtaining viable features to infer effort. Aiming to explore a
more effective technique for representing textual requirements to infer effort
estimates by analogy this paper proposes to evaluate the effectiveness of
pre-trained embeddings models. For this two embeddings approach context-less
and contextualized models are used. Generic pre-trained models for both
approaches went through a fine-tuning process. The generated models were used
as input in the applied deep learning architecture with linear output. The
results were very promising realizing that pre-trained incorporation models
can be used to estimate software effort based only on requirements texts. We
highlight the results obtained to apply the pre-trained BERT model with
fine-tuning in a single project repository whose value is the Mean Absolute
Error (MAE) is 4.25 and the standard deviation of only 0.17 which represents a
result very positive when compared to similar works. The main advantages of the
proposed estimation method are reliability the possibility of generalization
speed and low computational cost provided by the fine-tuning process and the
possibility to infer new or existing requirements.
",0,1,0
Exact Channel Synthesis,"  We consider the exact channel synthesis problem. This problem concerns the
determination of the minimum amount of information required to create exact
correlation remotely when there is a certain rate of randomness shared by two
terminals. This problem generalizes an existing approximate version in which
the generated joint distribution is required to be close to a target
distribution under the total variation (TV) distance measure (instead being
exactly equal to the target distribution). We provide single-letter inner and
outer bounds on the admissible region of the shared randomness rate and the
communication rate for the exact channel synthesis problem. These two bounds
coincide for doubly symmetric binary sources. We observe that for such sources
the admissible rate region for exact channel synthesis is strictly included in
that for the TV-approximate version. We also extend the exact and
TV-approximate channel synthesis problems to sources with countably infinite
alphabets and continuous sources; the latter includes Gaussian sources. As
by-products lemmas concerning soft-covering under R'enyi divergence measures
are derived.
",1,0,0
"Decoupled Appearance and Motion Learning for Efficient Anomaly Detection
  in Surveillance Video","  Automating the analysis of surveillance video footage is of great interest
when urban environments or industrial sites are monitored by a large number of
cameras. As anomalies are often context-specific it is hard to predefine
events of interest and collect labelled training data. A purely unsupervised
approach for automated anomaly detection is much more suitable. For every
camera a separate algorithm could then be deployed that learns over time a
baseline model of appearance and motion related features of the objects within
the camera viewport. Anything that deviates from this baseline is flagged as an
anomaly for further analysis downstream. We propose a new neural network
architecture that learns the normal behavior in a purely unsupervised fashion.
In contrast to previous work we use latent code predictions as our anomaly
metric. We show that this outperforms reconstruction-based and frame
prediction-based methods on different benchmark datasets both in terms of
accuracy and robustness against changing lighting and weather conditions. By
decoupling an appearance and a motion model our model can also process 16 to
45 times more frames per second than related approaches which makes our model
suitable for deploying on the camera itself or on other edge devices.
",0,0,1
"Analysis and study on text representation to improve the accuracy of the
  Normalized Compression Distance","  The huge amount of information stored in text form makes methods that deal
with texts really interesting. This thesis focuses on dealing with texts using
compression distances. More specifically the thesis takes a small step towards
understanding both the nature of texts and the nature of compression distances.
Broadly speaking the way in which this is done is exploring the effects that
several distortion techniques have on one of the most successful distances in
the family of compression distances the Normalized Compression Distance -NCD-.
",1,0,0
Responsible AI: Gender bias assessment in emotion recognition,"  Rapid development of artificial intelligence (AI) systems amplify many
concerns in society. These AI algorithms inherit different biases from humans
due to mysterious operational flow and because of that it is becoming adverse
in usage. As a result researchers have started to address the issue by
investigating deeper in the direction towards Responsible and Explainable AI.
Among variety of applications of AI facial expression recognition might not be
the most important one yet is considered as a valuable part of human-AI
interaction. Evolution of facial expression recognition from the feature based
methods to deep learning drastically improve quality of such algorithms. This
research work aims to study a gender bias in deep learning methods for facial
expression recognition by investigating six distinct neural networks training
them and further analysed on the presence of bias according to the three
definition of fairness. The main outcomes show which models are gender biased
which are not and how gender of subject affects its emotion recognition. More
biased neural networks show bigger accuracy gap in emotion recognition between
male and female test sets. Furthermore this trend keeps for true positive and
false positive rates. In addition due to the nature of the research we can
observe which types of emotions are better classified for men and which for
women. Since the topic of biases in facial expression recognition is not well
studied a spectrum of continuation of this research is truly extensive and
may comprise detail analysis of state-of-the-art methods as well as targeting
other biases.
",0,0,1
"A Comparative Study of Human thermal face recognition based on Haar
  wavelet transform (HWT) and Local Binary Pattern (LBP)","  Thermal infra-red (IR) images focus on changes of temperature distribution on
facial muscles and blood vessels. These temperature changes can be regarded as
texture features of images. A comparative study of face recognition methods
working in thermal spectrum is carried out in this paper. In these study two
local-matching methods based on Haar wavelet transform and Local Binary Pattern
(LBP) are analyzed. Wavelet transform is a good tool to analyze multi-scale
multi-direction changes of texture. Local binary patterns (LBP) are a type of
feature used for classification in computer vision. Firstly human thermal IR
face image is preprocessed and cropped the face region only from the entire
image. Secondly two different approaches are used to extract the features from
the cropped face region. In the first approach the training images and the
test images are processed with Haar wavelet transform and the LL band and the
average of LH/HL/HH bands sub-images are created for each face image. Then a
total confidence matrix is formed for each face image by taking a weighted sum
of the corresponding pixel values of the LL band and average band. For LBP
feature extraction each of the face images in training and test datasets is
divided into 161 numbers of sub images each of size 8X8 pixels. For each such
sub images LBP features are extracted which are concatenated in row wise
manner. PCA is performed separately on the individual feature set for
dimensionality reeducation. Finally two different classifiers are used to
classify face images. One such classifier multi-layer feed forward neural
network and another classifier is minimum distance classifier. The Experiments
have been performed on the database created at our own laboratory and Terravic
Facial IR Database.
",0,0,1
Effects of padding on LSTMs and CNNs,"  Long Short-Term Memory (LSTM) Networks and Convolutional Neural Networks
(CNN) have become very common and are used in many fields as they were
effective in solving many problems where the general neural networks were
inefficient. They were applied to various problems mostly related to images and
sequences. Since LSTMs and CNNs take inputs of the same length and dimension
input images and sequences are padded to maximum length while testing and
training. This padding can affect the way the networks function and can make a
great deal when it comes to performance and accuracies. This paper studies this
and suggests the best way to pad an input sequence. This paper uses a simple
sentiment analysis task for this purpose. We use the same dataset on both the
networks with various padding to show the difference. This paper also discusses
some preprocessing techniques done on the data to ensure effective analysis of
the data.
",0,1,0
Train in Germany Test in The USA: Making 3D Object Detectors Generalize,"  In the domain of autonomous driving deep learning has substantially improved
the 3D object detection accuracy for LiDAR and stereo camera data alike. While
deep networks are great at generalization they are also notorious to over-fit
to all kinds of spurious artifacts such as brightness car sizes and models
that may appear consistently throughout the data. In fact most datasets for
autonomous driving are collected within a narrow subset of cities within one
country typically under similar weather conditions. In this paper we consider
the task of adapting 3D object detectors from one dataset to another. We
observe that naively this appears to be a very challenging task resulting in
drastic drops in accuracy levels. We provide extensive experiments to
investigate the true adaptation challenges and arrive at a surprising
conclusion: the primary adaptation hurdle to overcome are differences in car
sizes across geographic areas. A simple correction based on the average car
size yields a strong correction of the adaptation gap. Our proposed method is
simple and easily incorporated into most 3D object detection frameworks. It
provides a first baseline for 3D object detection adaptation across countries
and gives hope that the underlying problem may be more within grasp than one
may have hoped to believe. Our code is available at
https://github.com/cxy1997/3D_adapt_auto_driving.
",0,0,1
"Comparing Attention-based Convolutional and Recurrent Neural Networks:
  Success and Limitations in Machine Reading Comprehension","  We propose a machine reading comprehension model based on the
compare-aggregate framework with two-staged attention that achieves
state-of-the-art results on the MovieQA question answering dataset. To
investigate the limitations of our model as well as the behavioral difference
between convolutional and recurrent neural networks we generate adversarial
examples to confuse the model and compare to human performance. Furthermore we
assess the generalizability of our model by analyzing its differences to human
inference
",0,1,0
Connectivity Learning in Multi-Branch Networks,"  While much of the work in the design of convolutional networks over the last
five years has revolved around the empirical investigation of the importance of
depth filter sizes and number of feature channels recent studies have shown
that branching i.e. splitting the computation along parallel but distinct
threads and then aggregating their outputs represents a new promising
dimension for significant improvements in performance. To combat the complexity
of design choices in multi-branch architectures prior work has adopted simple
strategies such as a fixed branching factor the same input being fed to all
parallel branches and an additive combination of the outputs produced by all
branches at aggregation points.
  In this work we remove these predefined choices and propose an algorithm to
learn the connections between branches in the network. Instead of being chosen
a priori by the human designer the multi-branch connectivity is learned
simultaneously with the weights of the network by optimizing a single loss
function defined with respect to the end task. We demonstrate our approach on
the problem of multi-class image classification using three different datasets
where it yields consistently higher accuracy compared to the state-of-the-art
""ResNeXt"" multi-branch network given the same learning capacity.
",0,0,1
"End-to-end optimization of nonlinear transform codes for perceptual
  quality","  We introduce a general framework for end-to-end optimization of the
rate--distortion performance of nonlinear transform codes assuming scalar
quantization. The framework can be used to optimize any differentiable pair of
analysis and synthesis transforms in combination with any differentiable
perceptual metric. As an example we consider a code built from a linear
transform followed by a form of multi-dimensional local gain control.
Distortion is measured with a state-of-the-art perceptual metric. When
optimized over a large database of images this representation offers
substantial improvements in bitrate and perceptual appearance over fixed (DCT)
codes and over linear transform codes optimized for mean squared error.
",1,0,0
Coding for Combined Block-Symbol Error Correction,"  We design low-complexity error correction coding schemes for channels that
introduce different types of errors and erasures: on the one hand the proposed
schemes can successfully deal with symbol errors and erasures and on the
other hand they can also successfully handle phased burst errors and erasures.
",1,0,0
COPD Classification in CT Images Using a 3D Convolutional Neural Network,"  Chronic obstructive pulmonary disease (COPD) is a lung disease that is not
fully reversible and one of the leading causes of morbidity and mortality in
the world. Early detection and diagnosis of COPD can increase the survival rate
and reduce the risk of COPD progression in patients. Currently the primary
examination tool to diagnose COPD is spirometry. However computed tomography
(CT) is used for detecting symptoms and sub-type classification of COPD. Using
different imaging modalities is a difficult and tedious task even for
physicians and is subjective to inter-and intra-observer variations. Hence
developing meth-ods that can automatically classify COPD versus healthy
patients is of great interest. In this paper we propose a 3D deep learning
approach to classify COPD and emphysema using volume-wise annotations only. We
also demonstrate the impact of transfer learning on the classification of
emphysema using knowledge transfer from a pre-trained COPD classification
model.
",0,0,1
The algebro-geometric study of range maps,"  Localizing a radiant source is a widespread problem to many scientific and
technological research areas. E.g. localization based on range measurements
stays at the core of technologies like radar sonar and wireless sensors
networks. In this manuscript we study in depth the model for source
localization based on range measurements obtained from the source signal from
the point of view of algebraic geometry. In the case of three receivers we
find unexpected connections between this problem and the geometry of Kummer's
and Cayley's surfaces. Our work gives new insights also on the localization
based on range differences.
",1,0,0
"Multiple Instance Curriculum Learning for Weakly Supervised Object
  Detection","  When supervising an object detector with weakly labeled data most existing
approaches are prone to trapping in the discriminative object parts e.g.
finding the face of a cat instead of the full body due to lacking the
supervision on the extent of full objects. To address this challenge we
incorporate object segmentation into the detector training which guides the
model to correctly localize the full objects. We propose the multiple instance
curriculum learning (MICL) method which injects curriculum learning (CL) into
the multiple instance learning (MIL) framework. The MICL method starts by
automatically picking the easy training examples where the extent of the
segmentation masks agree with detection bounding boxes. The training set is
gradually expanded to include harder examples to train strong detectors that
handle complex images. The proposed MICL method with segmentation in the loop
outperforms the state-of-the-art weakly supervised object detectors by a
substantial margin on the PASCAL VOC datasets.
",0,0,1
