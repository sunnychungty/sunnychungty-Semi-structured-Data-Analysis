{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744da80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "import torch.nn as nn\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, Dataset\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0302e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sunny\\\\Desktop\\\\Master\\\\Sem3\\\\5212\\\\A1\\\\Dataset_Assignment1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ada5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random seed\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef81df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up field & loading datasets\n",
    "\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text not in STOP_WORDS]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", TEXT), \n",
    "                   (\"abstract\", None),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "from torchtext.legacy.data import Dataset\n",
    "\n",
    "def split_dataset(dataset, split_index):\n",
    "    fields = dataset.fields\n",
    "    examples = dataset.examples\n",
    "    top_examples = examples[:split_index]\n",
    "    remaining_examples = examples[split_index:]\n",
    "    \n",
    "    top_dataset = Dataset(top_examples, fields)\n",
    "    remaining_dataset = Dataset(remaining_examples, fields)\n",
    "    \n",
    "    return top_dataset, remaining_dataset\n",
    "\n",
    "train_data_1000, valid_data = split_dataset(train_data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed33e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocab\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data_1000, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9e7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterator #1 for small train_data\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "def preprocess_target_label(target_field):\n",
    "    for example in train_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "        \n",
    "    for example in test_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "\n",
    "def generate_label_iterator(dataset, target, validation = True):\n",
    "    preprocess_target_label(target)\n",
    "    \n",
    "    label_attr = f\"label_{target}\"\n",
    "    if validation:\n",
    "        iterators = data.BucketIterator.splits(\n",
    "            (dataset, valid_data, test_data),\n",
    "            batch_size = BATCH_SIZE,\n",
    "            device = device,\n",
    "            sort_key = lambda x: len(getattr(x, label_attr)),\n",
    "            sort_within_batch = False)\n",
    "        return iterators[0], iterators[1], iterators[2]\n",
    "    else:\n",
    "        iterators = data.BucketIterator.splits(\n",
    "            (dataset, test_data),\n",
    "            batch_size = BATCH_SIZE,\n",
    "            device = device,\n",
    "            sort_key = lambda x: len(getattr(x, label_attr)),\n",
    "\n",
    "            sort_within_batch = False)\n",
    "        return iterators[0], iterators[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b805220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f338238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4cf009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization & optimizer\n",
    "def generate_model_and_optimizer(embedding_dim=100, hidden_dim=256, output_dim=1, lr=1e-3):\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "    model = RNN(INPUT_DIM, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a870deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, label_field, target_field):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "        \n",
    "        # Use the specific label field\n",
    "        loss = criterion(predictions, getattr(batch, label_field))\n",
    "        \n",
    "        acc = binary_accuracy(predictions, getattr(batch, label_field))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, label_field, target_field):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "            \n",
    "            # Use the specific label field\n",
    "            loss = criterion(predictions, getattr(batch, label_field))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, getattr(batch, label_field))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce365d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "# models = [model_IT, model_CL, model_CV]\n",
    "# optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "# iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "# for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "#     print(f\"Training model for {label_name}...\")\n",
    "#     best_valid_loss_IT = float(\"inf\")\n",
    "#     best_valid_loss_CL = float(\"inf\")\n",
    "#     best_valid_loss_CV = float(\"inf\")\n",
    "    \n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        \n",
    "#         if label_name == label_names[0]:\n",
    "#             train_loss_IT, train_acc_IT = train(model_IT, train_iterator_IT, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_IT, test_acc_IT = evaluate(model_IT, test_iterator_IT, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_IT < best_valid_loss_IT:\n",
    "#                 best_valid_loss_IT = test_loss_IT\n",
    "#                 torch.save(models[0].state_dict(), \"RNN_model_IT.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'InformationTheory Test Loss: {test_loss_IT:.3f} | Test Acc: {test_acc_IT*100:.2f}%')\n",
    "#             model_IT.eval()\n",
    "#             y_predict = []\n",
    "#             y_test = []\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in test_iterator_IT:\n",
    "#                     predictions = model_IT(batch.title).squeeze(1)\n",
    "#                     rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "#                     y_predict += rounded_preds.tolist()\n",
    "#                     y_test += batch.InformationTheory.tolist()\n",
    "                       \n",
    "#         elif label_name == label_names[1]:\n",
    "#             train_loss_CL, train_acc_CL = train(model_CL, train_iterator_CL, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_CL, test_acc_CL = evaluate(model_CL, test_iterator_CL, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CL < best_valid_loss_CL:\n",
    "#                 best_valid_loss_CL = test_loss_CL\n",
    "#                 torch.save(models[1].state_dict(), \"RNN_model_CL.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputationalLinguistics Test Loss: {test_loss_CL:.3f} | Test Acc: {test_acc_CL*100:.2f}%')\n",
    "\n",
    "           \n",
    "#         elif label_name == label_names[2]:\n",
    "#             train_loss_CV, train_acc_CV = train(model_CV, train_iterator_CV, optimizers[2], criterion, label_name, \"title\")\n",
    "#             test_loss_CV, test_acc_CV = evaluate(model_CV, test_iterator_CV, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CV < best_valid_loss_CV:\n",
    "#                 best_valid_loss_CV = test_loss_CV\n",
    "#                 torch.save(models[2].state_dict(), \"RNN_model_CV.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputerVision Test Loss: {test_loss_CV:.3f} | Test Acc: {test_acc_CV*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1d84a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "# models = [model_IT, model_CL, model_CV]\n",
    "# optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "# iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "# best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "# model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "# target_field = \"title\"\n",
    "\n",
    "# for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "#     print(f\"Training model for {label_name}...\")\n",
    "    \n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         train_loss, train_acc = train(model, train_iterator, optimizer, criterion, label_name, target_field)\n",
    "#         test_loss, test_acc = evaluate(model, test_iterator, criterion, label_name, target_field)\n",
    "        \n",
    "#         end_time = time.time()\n",
    "#         epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "#         if test_loss < best_valid_losses[idx]:\n",
    "#             best_valid_losses[idx] = test_loss\n",
    "#             torch.save(model.state_dict(), model_file_names[idx])\n",
    "\n",
    "#         print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#         print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        \n",
    "#         # For predictions and ground truth collection\n",
    "#         model.eval()\n",
    "#         y_predict = []\n",
    "#         y_test = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_iterator:\n",
    "#                 predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "#                 rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "#                 y_predict += rounded_preds.tolist()\n",
    "#                 y_test += getattr(batch, label_name).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be8dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.709 | Test Acc: 53.10%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.727 | Test Acc: 53.11%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.744 | Test Acc: 53.12%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.757 | Test Acc: 53.13%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.768 | Test Acc: 53.14%\n",
      "Training model for P1_ComputationalLinguistics, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.576 | Test Acc: 80.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.546 | Test Acc: 80.43%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.528 | Test Acc: 80.50%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.517 | Test Acc: 80.62%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.510 | Test Acc: 80.69%\n",
      "Training model for P1_ComputerVision, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.654 | Test Acc: 65.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.662 | Test Acc: 65.36%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.670 | Test Acc: 65.37%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.677 | Test Acc: 65.30%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.682 | Test Acc: 65.07%\n"
     ]
    }
   ],
   "source": [
    "def train_loop(ModelsList, OptimizersList, IteratorsList, N_EPOCHS, label_names, targe_field, modelFileNames, bestLossesList, preprocess, train_size):\n",
    "\n",
    "    for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, ModelsList, OptimizersList, IteratorsList)):\n",
    "        print(f\"Training model for {preprocess}_{label_name}, using {train_size} data of {target_field}...\")\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion, label_name, target_field)\n",
    "            test_loss, test_acc = evaluate(model, test_iterator, criterion, label_name, target_field)\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "            if test_loss < best_valid_losses[idx]:\n",
    "                bestLossesList[idx] = test_loss\n",
    "                torch.save(model.state_dict(), modelFileNames[idx])\n",
    "\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "            # For predictions and ground truth collection\n",
    "            model.eval()\n",
    "            y_predict = []\n",
    "            y_test = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_iterator:\n",
    "                    predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "                    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                    y_predict += rounded_preds.tolist()\n",
    "                    y_test += getattr(batch, label_name).tolist()\n",
    "#     return y_predict, y_test\n",
    "\n",
    "N_EPOCHS = 5\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "target_field = \"title\"\n",
    "\n",
    "# train_loop for \"title\", using top 1000 records of train data and preprocessing method 1\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"1000\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, iterator, label_field):\n",
    "    y_predict = []\n",
    "    y_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.title).squeeze(1)\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "            y_predict += rounded_preds.tolist()\n",
    "            y_test += getattr(batch, label_field).tolist()\n",
    "\n",
    "    y_predict = np.asarray(y_predict)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    recall = recall_score(y_test, y_predict, average='macro')\n",
    "    precision = precision_score(y_test, y_predict, average='macro')\n",
    "    f1score = f1_score(y_test, y_predict, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    matthews = matthews_corrcoef(y_test, y_predict)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"{label_field}:\")\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Macro Precision:', precision)\n",
    "    print('Macro Recall:', recall)\n",
    "    print('Macro F1 score:', f1score)\n",
    "    print('MCC:', matthews)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluate models\n",
    "evaluate_model(model_IT, test_iterator_IT, \"InformationTheory\")\n",
    "evaluate_model(model_CL, test_iterator_CL, \"ComputationalLinguistics\")\n",
    "evaluate_model(model_CV, test_iterator_CV, \"ComputerVision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e9de735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P2_InformationTheory, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.697 | Test Acc: 53.11%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.710 | Test Acc: 53.06%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.725 | Test Acc: 53.06%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.740 | Test Acc: 53.05%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "InformationTheory Test Loss: 0.751 | Test Acc: 53.02%\n",
      "Training model for P2_ComputationalLinguistics, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.591 | Test Acc: 80.95%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.555 | Test Acc: 81.00%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.534 | Test Acc: 81.01%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.521 | Test Acc: 81.00%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputationalLinguistics Test Loss: 0.512 | Test Acc: 81.01%\n",
      "Training model for P2_ComputerVision, using 1000 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.672 | Test Acc: 65.37%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.677 | Test Acc: 65.24%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.682 | Test Acc: 64.99%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.686 | Test Acc: 64.67%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "ComputerVision Test Loss: 0.689 | Test Acc: 63.89%\n"
     ]
    }
   ],
   "source": [
    "TEXT_P2 = Field(sequential=True, tokenize=\"spacy\", lower=True)\n",
    "LABEL_P2 = LabelField(dtype=torch.float)\n",
    "\n",
    "# Field - P2\n",
    "train_datafield_P2 = [(\"title\", TEXT_P2),\n",
    "                      (\"abstract\", None),\n",
    "                      (\"InformationTheory\", LABEL_P2),\n",
    "                      (\"ComputationalLinguistics\", LABEL_P2),\n",
    "                      (\"ComputerVision\", LABEL_P2)\n",
    "                      ]\n",
    "\n",
    "train_data_P2, test_data_P2 = TabularDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.csv\", test=\"test.csv\", format=\"csv\",\n",
    "    skip_header=True, fields=train_datafield_P2)\n",
    "\n",
    "train_data_1000_P2, valid_data_P2 = split_dataset(train_data_P2, 1000)\n",
    "\n",
    "# Building vocab - P2\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT_P2.build_vocab(train_data_1000_P2, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL_P2.build_vocab(train_data_1000_P2)\n",
    "\n",
    "# Create iterator #2 for P2 train_data\n",
    "\n",
    "train_iterator_IT_P2, validation_IT_P2, test_iterator_IT_P2 = generate_label_iterator(train_data_1000_P2, \"InformationTheory\", validation=True)\n",
    "train_iterator_CL_P2, validation_CL_P2, test_iterator_CL_P2 = generate_label_iterator(train_data_1000_P2, \"ComputationalLinguistics\", validation=True)\n",
    "train_iterator_CV_P2, validation_CV_P2, test_iterator_CV_P2 = generate_label_iterator(train_data_1000_P2, \"ComputerVision\", validation=True)\n",
    "\n",
    "model_IT_P2, optimizer_IT_P2 = generate_model_and_optimizer()\n",
    "model_CL_P2, optimizer_CL_P2 = generate_model_and_optimizer()\n",
    "model_CV_P2, optimizer_CV_P2 = generate_model_and_optimizer()\n",
    "\n",
    "# Training Loop - P2\n",
    "\n",
    "models_P2 = [model_IT_P2, model_CL_P2, model_CV_P2]\n",
    "optimizers_P2 = [optimizer_IT_P2, optimizer_CL_P2, optimizer_CV_P2]\n",
    "iterators_P2 = [(train_iterator_IT_P2, test_iterator_IT_P2), (train_iterator_CL_P2, test_iterator_CL_P2), (train_iterator_CV_P2, test_iterator_CV_P2)]\n",
    "\n",
    "best_valid_losses_P2 = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names_P2 = [\"RNN_model_IT_P2.pt\", \"RNN_model_CL_P2.pt\", \"RNN_model_CV_P2.pt\"]\n",
    "\n",
    "train_loop(models_P2, optimizers_P2, iterators_P2, N_EPOCHS, label_names, target_field, model_file_names_P2, best_valid_losses_P2, \"P2\", \"1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate models\n",
    "evaluate_model(model_IT_P2, test_iterator_IT_P2, \"InformationTheory\")\n",
    "evaluate_model(model_CL_P2, test_iterator_CL_P2, \"ComputationalLinguistics\")\n",
    "evaluate_model(model_CV_P2, test_iterator_CV_P2, \"ComputerVision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f28070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "557606b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=custom_tokenizer, lower=False)\n",
    "# TEXT = Field(sequential = True, tokenize = \"spacy\", lower = True)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", TEXT), \n",
    "                   (\"abstract\", None),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, test_iterator_IT = generate_label_iterator(train_data, \"InformationTheory\", validation = False)\n",
    "train_iterator_CL, test_iterator_CL = generate_label_iterator(train_data, \"ComputationalLinguistics\", validation = False)\n",
    "train_iterator_CV, test_iterator_CV = generate_label_iterator(train_data, \"ComputerVision\", validation = False)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec8fa74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 25s\n",
      "InformationTheory Test Loss: 0.806 | Test Acc: 53.06%\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "InformationTheory Test Loss: 0.814 | Test Acc: 53.04%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "InformationTheory Test Loss: 0.817 | Test Acc: 53.06%\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "InformationTheory Test Loss: 0.814 | Test Acc: 53.08%\n",
      "Epoch: 05 | Epoch Time: 0m 24s\n",
      "InformationTheory Test Loss: 0.817 | Test Acc: 53.08%\n",
      "Training model for P1_ComputationalLinguistics, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.07%\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.15%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.23%\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.25%\n",
      "Epoch: 05 | Epoch Time: 0m 23s\n",
      "ComputationalLinguistics Test Loss: 0.493 | Test Acc: 81.27%\n",
      "Training model for P1_ComputerVision, using ALL data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "ComputerVision Test Loss: 0.705 | Test Acc: 34.84%\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "ComputerVision Test Loss: 0.710 | Test Acc: 34.70%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "ComputerVision Test Loss: 0.711 | Test Acc: 34.69%\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "ComputerVision Test Loss: 0.707 | Test Acc: 34.73%\n",
      "Epoch: 05 | Epoch Time: 0m 24s\n",
      "ComputerVision Test Loss: 0.705 | Test Acc: 34.75%\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "\n",
    "train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "181ebccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for InformationTheory...\n",
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "InformationTheory Test Loss: 0.803 | Test Acc: 53.09%\n",
      "Epoch: 02 | Epoch Time: 0m 26s\n",
      "InformationTheory Test Loss: 0.812 | Test Acc: 53.08%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "InformationTheory Test Loss: 0.817 | Test Acc: 53.07%\n",
      "Epoch: 04 | Epoch Time: 0m 24s\n",
      "InformationTheory Test Loss: 0.812 | Test Acc: 53.05%\n",
      "Epoch: 05 | Epoch Time: 0m 23s\n",
      "InformationTheory Test Loss: 0.815 | Test Acc: 53.06%\n",
      "Training model for ComputationalLinguistics...\n",
      "Epoch: 01 | Epoch Time: 0m 24s\n",
      "ComputationalLinguistics Test Loss: 0.493 | Test Acc: 80.97%\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.14%\n",
      "Epoch: 03 | Epoch Time: 0m 26s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.22%\n",
      "Epoch: 04 | Epoch Time: 0m 26s\n",
      "ComputationalLinguistics Test Loss: 0.492 | Test Acc: 81.26%\n",
      "Epoch: 05 | Epoch Time: 0m 25s\n",
      "ComputationalLinguistics Test Loss: 0.493 | Test Acc: 81.29%\n",
      "Training model for ComputerVision...\n",
      "Epoch: 01 | Epoch Time: 0m 25s\n",
      "ComputerVision Test Loss: 0.706 | Test Acc: 34.73%\n",
      "Epoch: 02 | Epoch Time: 0m 24s\n",
      "ComputerVision Test Loss: 0.710 | Test Acc: 34.71%\n",
      "Epoch: 03 | Epoch Time: 0m 26s\n",
      "ComputerVision Test Loss: 0.711 | Test Acc: 34.66%\n",
      "Epoch: 04 | Epoch Time: 0m 25s\n",
      "ComputerVision Test Loss: 0.707 | Test Acc: 34.69%\n",
      "Epoch: 05 | Epoch Time: 0m 24s\n",
      "ComputerVision Test Loss: 0.705 | Test Acc: 34.70%\n"
     ]
    }
   ],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "# models = [model_IT, model_CL, model_CV]\n",
    "# optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "# iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "# for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "#     print(f\"Training model for {label_name}...\")\n",
    "#     best_valid_loss_IT = float(\"inf\")\n",
    "#     best_valid_loss_CL = float(\"inf\")\n",
    "#     best_valid_loss_CV = float(\"inf\")\n",
    "    \n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        \n",
    "#         if label_name == label_names[0]:\n",
    "#             train_loss_IT, train_acc_IT = train(model_IT, train_iterator_IT, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_IT, test_acc_IT = evaluate(model_IT, test_iterator_IT, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_IT < best_valid_loss_IT:\n",
    "#                 best_valid_loss_IT = test_loss_IT\n",
    "#                 torch.save(models[0].state_dict(), \"RNN_model_IT.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'InformationTheory Test Loss: {test_loss_IT:.3f} | Test Acc: {test_acc_IT*100:.2f}%')\n",
    "#             model_IT.eval()\n",
    "#             y_predict = []\n",
    "#             y_test = []\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in test_iterator_IT:\n",
    "#                     predictions = model_IT(batch.title).squeeze(1)\n",
    "#                     rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "#                     y_predict += rounded_preds.tolist()\n",
    "#                     y_test += batch.InformationTheory.tolist()\n",
    "                       \n",
    "#         elif label_name == label_names[1]:\n",
    "#             train_loss_CL, train_acc_CL = train(model_CL, train_iterator_CL, optimizers[idx], criterion, label_name, \"title\")\n",
    "#             test_loss_CL, test_acc_CL = evaluate(model_CL, test_iterator_CL, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CL < best_valid_loss_CL:\n",
    "#                 best_valid_loss_CL = test_loss_CL\n",
    "#                 torch.save(models[1].state_dict(), \"RNN_model_CL.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputationalLinguistics Test Loss: {test_loss_CL:.3f} | Test Acc: {test_acc_CL*100:.2f}%')\n",
    "\n",
    "           \n",
    "#         elif label_name == label_names[2]:\n",
    "#             train_loss_CV, train_acc_CV = train(model_CV, train_iterator_CV, optimizers[2], criterion, label_name, \"title\")\n",
    "#             test_loss_CV, test_acc_CV = evaluate(model_CV, test_iterator_CV, criterion, label_name, \"title\")\n",
    "#             end_time = time.time()\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#             if test_loss_CV < best_valid_loss_CV:\n",
    "#                 best_valid_loss_CV = test_loss_CV\n",
    "#                 torch.save(models[2].state_dict(), \"RNN_model_CV.pt\")\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'ComputerVision Test Loss: {test_loss_CV:.3f} | Test Acc: {test_acc_CV*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd86a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_P2 = Field(sequential=True, tokenize=\"spacy\", lower=True)\n",
    "LABEL_P2 = LabelField(dtype=torch.float)\n",
    "\n",
    "# Field - P2\n",
    "train_datafield_P2 = [(\"title\", TEXT_P2),\n",
    "                      (\"abstract\", None),\n",
    "                      (\"InformationTheory\", LABEL_P2),\n",
    "                      (\"ComputationalLinguistics\", LABEL_P2),\n",
    "                      (\"ComputerVision\", LABEL_P2)\n",
    "                      ]\n",
    "\n",
    "train_data_P2, test_data_P2 = TabularDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.csv\", test=\"test.csv\", format=\"csv\",\n",
    "    skip_header=True, fields=train_datafield_P2)\n",
    "\n",
    "\n",
    "# Building vocab - P2\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT_P2.build_vocab(train_data_P2, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL_P2.build_vocab(train_data_P2)\n",
    "\n",
    "# Create iterator #2 for P2 train_data\n",
    "\n",
    "train_iterator_IT_P2, test_iterator_IT_P2 = generate_label_iterator(train_data_1000_P2, \"InformationTheory\", validation=False)\n",
    "train_iterator_CL_P2, test_iterator_CL_P2 = generate_label_iterator(train_data_1000_P2, \"ComputationalLinguistics\", validation=False)\n",
    "train_iterator_CV_P2, test_iterator_CV_P2 = generate_label_iterator(train_data_1000_P2, \"ComputerVision\", validation=False)\n",
    "\n",
    "model_IT_P2, optimizer_IT_P2 = generate_model_and_optimizer()\n",
    "model_CL_P2, optimizer_CL_P2 = generate_model_and_optimizer()\n",
    "model_CV_P2, optimizer_CV_P2 = generate_model_and_optimizer()\n",
    "\n",
    "# Training Loop - P2\n",
    "\n",
    "models_P2 = [model_IT_P2, model_CL_P2, model_CV_P2]\n",
    "optimizers_P2 = [optimizer_IT_P2, optimizer_CL_P2, optimizer_CV_P2]\n",
    "iterators_P2 = [(train_iterator_IT_P2, test_iterator_IT_P2), (train_iterator_CL_P2, test_iterator_CL_P2), (train_iterator_CV_P2, test_iterator_CV_P2)]\n",
    "\n",
    "best_valid_losses_P2 = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names_P2 = [\"RNN_model_IT_P2.pt\", \"RNN_model_CL_P2.pt\", \"RNN_model_CV_P2.pt\"]\n",
    "\n",
    "for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models_P2, optimizers_P2, iterators_P2)):\n",
    "    print(f\"Training model for P2_{label_name}...\")\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion, label_name, target_field)\n",
    "        test_loss, test_acc = evaluate(model, test_iterator, criterion, label_name, target_field)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if test_loss < best_valid_losses_P2[idx]:\n",
    "            best_valid_losses_P2[idx] = test_loss\n",
    "            torch.save(model.state_dict(), model_file_names_P2[idx])\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "            # For predictions and ground truth collection\n",
    "        model.eval()\n",
    "        y_predict_P2 = []\n",
    "        y_test_P2 = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_iterator:\n",
    "                predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "                rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                y_predict_P2 += rounded_preds.tolist()\n",
    "                y_test_P2 += getattr(batch, label_name).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834217e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5700825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c921e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8508b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d328da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2011764",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, tokenize = \"spacy\", lower = True)\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    " \n",
    "train_datafield = [(\"title\", None), \n",
    "                   (\"abstract\", TEXT),\n",
    "                   (\"InformationTheory\", LABEL), \n",
    "                   (\"ComputationalLinguistics\", LABEL),\n",
    "                   (\"ComputerVision\", LABEL)\n",
    "                   ]\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path = \"./\",\n",
    "    train = \"train.csv\", test = \"test.csv\", format = \"csv\",\n",
    "    skip_header = True, fields = train_datafield)\n",
    "\n",
    "train_data_1000, valid_data = split_dataset(train_data, 1000)\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "TEXT.build_vocab(train_data_1000, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data_1000)\n",
    "\n",
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data_1000, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data_1000, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data_1000, \"ComputerVision\", validation = True)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "N_EPOCHS = 5\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "    print(f\"Training model for {label_name}...\")\n",
    "    best_valid_loss_IT = float(\"inf\")\n",
    "    best_valid_loss_CL = float(\"inf\")\n",
    "    best_valid_loss_CV = float(\"inf\")\n",
    "    \n",
    "    target_field = \"abstract\"\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        if label_name == label_names[0]:\n",
    "            train_loss_IT, train_acc_IT = train(model_IT, train_iterator_IT, optimizers[idx], criterion, label_name, target_field)\n",
    "            test_loss_IT, test_acc_IT = evaluate(model_IT, test_iterator_IT, criterion, label_name, \"abstract\")\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if test_loss_IT < best_valid_loss_IT:\n",
    "                best_valid_loss_IT = test_loss_IT\n",
    "                torch.save(models[0].state_dict(), \"RNN_model_IT.pt\")\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'InformationTheory Test Loss: {test_loss_IT:.3f} | Test Acc: {test_acc_IT*100:.2f}%')\n",
    "            model_IT.eval()\n",
    "            y_predict = []\n",
    "            y_test = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_iterator_IT:\n",
    "                    predictions = model_IT(batch.abstract).squeeze(1)\n",
    "                    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                    y_predict += rounded_preds.tolist()\n",
    "                    y_test += batch.InformationTheory.tolist()\n",
    "                       \n",
    "        elif label_name == label_names[1]:\n",
    "            train_loss_CL, train_acc_CL = train(model_CL, train_iterator_CL, optimizers[idx], criterion, label_name, \"abstract\")\n",
    "            test_loss_CL, test_acc_CL = evaluate(model_CL, test_iterator_CL, criterion, label_name, \"abstract\")\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if test_loss_CL < best_valid_loss_CL:\n",
    "                best_valid_loss_CL = test_loss_CL\n",
    "                torch.save(models[1].state_dict(), \"RNN_model_CL.pt\")\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'ComputationalLinguistics Test Loss: {test_loss_CL:.3f} | Test Acc: {test_acc_CL*100:.2f}%')\n",
    "\n",
    "           \n",
    "        elif label_name == label_names[2]:\n",
    "            train_loss_CV, train_acc_CV = train(model_CV, train_iterator_CV, optimizers[2], criterion, label_name, \"abstract\")\n",
    "            test_loss_CV, test_acc_CV = evaluate(model_CV, test_iterator_CV, criterion, label_name, \"abstract\")\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if test_loss_CV < best_valid_loss_CV:\n",
    "                best_valid_loss_CV = test_loss_CV\n",
    "                torch.save(models[2].state_dict(), \"RNN_model_CV.pt\")\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'ComputerVision Test Loss: {test_loss_CV:.3f} | Test Acc: {test_acc_CV*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "N_EPOCHS = 5\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n",
    "model_file_names = [\"RNN_model_IT.pt\", \"RNN_model_CL.pt\", \"RNN_model_CV.pt\"]\n",
    "\n",
    "for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "    print(f\"Training model for {label_name}...\")\n",
    "    target_field = \"abstract\"\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion, label_name, target_field)\n",
    "        test_loss, test_acc = evaluate(model, test_iterator, criterion, label_name, target_field)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if test_loss < best_valid_losses[idx]:\n",
    "            best_valid_losses[idx] = test_loss\n",
    "            torch.save(model.state_dict(), model_file_names[idx])\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4cf651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generate_label_iterator functions\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, \"InformationTheory\", validation = True)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, \"ComputationalLinguistics\", validation = True)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, \"ComputerVision\", validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14081be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_IT, optimizer_IT = generate_model_and_optimizer()\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer()\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "N_EPOCHS = 5\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, test_iterator_IT), (train_iterator_CL, test_iterator_CL), (train_iterator_CV, test_iterator_CV)]\n",
    "\n",
    "\n",
    "for idx, (label_name, model, optimizer, (train_iterator, test_iterator)) in enumerate(zip(label_names, models, optimizers, iterators)):\n",
    "    print(f\"Training model for {label_name}...\")\n",
    "    best_valid_loss_IT = float(\"inf\")\n",
    "    best_valid_loss_CL = float(\"inf\")\n",
    "    best_valid_loss_CV = float(\"inf\")\n",
    "    \n",
    "    target_field = \"abstract\"\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        if label_name == label_names[0]:\n",
    "            train_loss_IT, train_acc_IT = train(model_IT, train_iterator_IT, optimizers[idx], criterion, label_name, target_field)\n",
    "            test_loss_IT, test_acc_IT = evaluate(model_IT, test_iterator_IT, criterion, label_name, \"abstract\")\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if test_loss_IT < best_valid_loss_IT:\n",
    "                best_valid_loss_IT = test_loss_IT\n",
    "                torch.save(models[0].state_dict(), \"RNN_model_IT.pt\")\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'InformationTheory Test Loss: {test_loss_IT:.3f} | Test Acc: {test_acc_IT*100:.2f}%')\n",
    "            model_IT.eval()\n",
    "            y_predict = []\n",
    "            y_test = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_iterator_IT:\n",
    "                    predictions = model_IT(batch.abstract).squeeze(1)\n",
    "                    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                    y_predict += rounded_preds.tolist()\n",
    "                    y_test += batch.InformationTheory.tolist()\n",
    "                       \n",
    "        elif label_name == label_names[1]:\n",
    "            train_loss_CL, train_acc_CL = train(model_CL, train_iterator_CL, optimizers[idx], criterion, label_name, \"abstract\")\n",
    "            test_loss_CL, test_acc_CL = evaluate(model_CL, test_iterator_CL, criterion, label_name, \"abstract\")\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if test_loss_CL < best_valid_loss_CL:\n",
    "                best_valid_loss_CL = test_loss_CL\n",
    "                torch.save(models[1].state_dict(), \"RNN_model_CL.pt\")\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'ComputationalLinguistics Test Loss: {test_loss_CL:.3f} | Test Acc: {test_acc_CL*100:.2f}%')\n",
    "\n",
    "           \n",
    "        elif label_name == label_names[2]:\n",
    "            train_loss_CV, train_acc_CV = train(model_CV, train_iterator_CV, optimizers[2], criterion, label_name, \"abstract\")\n",
    "            test_loss_CV, test_acc_CV = evaluate(model_CV, test_iterator_CV, criterion, label_name, \"abstract\")\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if test_loss_CV < best_valid_loss_CV:\n",
    "                best_valid_loss_CV = test_loss_CV\n",
    "                torch.save(models[2].state_dict(), \"RNN_model_CV.pt\")\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'ComputerVision Test Loss: {test_loss_CV:.3f} | Test Acc: {test_acc_CV*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04f593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6edfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
