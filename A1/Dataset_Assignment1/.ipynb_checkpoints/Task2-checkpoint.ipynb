{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbd39ad",
   "metadata": {},
   "source": [
    "<h2>Importing Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10608b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyldavis==2.1.2\n",
    "!pip3 install scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdcf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk, spacy, string\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed383b",
   "metadata": {},
   "source": [
    "<h2>P1 1000</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9630b",
   "metadata": {},
   "source": [
    "<h3>Loading data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1000 = pd.read_csv(\"train_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "df = df1000.copy()\n",
    "docs = df['abstract'].tolist()\n",
    "print(len(docs))\n",
    "print(docs[0][0:500])\n",
    "raw_docs = docs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac99e3",
   "metadata": {},
   "source": [
    "<h3>Pre-processing: Tokenization, filtering, and lemmatization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61425800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b755393",
   "metadata": {},
   "source": [
    "<h3>Adding Birgrams and Trigrams</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe901b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ef1da",
   "metadata": {},
   "source": [
    "<h3>Creating corpus</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(corpus) == 0 or len(dictionary) == 0:\n",
    "    print(\"Error: Corpus or dictionary is empty.\")\n",
    "else:\n",
    "    print(f\"Corpus size: {len(corpus)}, Dictionary size: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2cfc09",
   "metadata": {},
   "source": [
    "<h3>Getting best NUM_TOPICS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "# Import the necessary libraries\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "NUM_TOPICS = 30\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Initialize variables\n",
    "best_avg_topic_coherence = -100\n",
    "best_num_topics = 0\n",
    "avg_topic_coherence_values = []\n",
    "\n",
    "# Define the range of values for NUM_TOPICS you want to explore\n",
    "num_topics_range = range(10, 16, 2)  # In this example, we'll try values from 10 to 30, with a step of 2\n",
    "\n",
    "# Loop through each value in the range\n",
    "for num_topics in num_topics_range:\n",
    "    print(f\"Now processing LDA model with {num_topics} number of topics\")\n",
    "    temp = dictionary[0]\n",
    "    id2word = dictionary.id2token\n",
    "    # Train the LDA model with the current value of NUM_TOPICS\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "\n",
    "    # Get top_topics and calculate average topic coherence\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "\n",
    "    # Store the average topic coherence value for the current number of topics\n",
    "    avg_topic_coherence_values.append(avg_topic_coherence)\n",
    "\n",
    "    # Update the best values if the current average topic coherence is higher than the previous best\n",
    "    if avg_topic_coherence > best_avg_topic_coherence:\n",
    "        best_avg_topic_coherence = avg_topic_coherence\n",
    "        best_num_topics = num_topics\n",
    "        best_model = model\n",
    "\n",
    "# Print the best value of NUM_TOPICS and the corresponding average topic coherence value\n",
    "print(f\"Best number of topics: {best_num_topics}, average topic coherence: {best_avg_topic_coherence}\")\n",
    "\n",
    "# Plot the average topic coherence values for different numbers of topics\n",
    "plt.plot(num_topics_range, avg_topic_coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Average Topic Coherence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d453b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c8c74a",
   "metadata": {},
   "source": [
    "<h3>Training LDA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train LDA model.\n",
    "# from gensim.models import LdaModel\n",
    "\n",
    "# # Set training parameters.\n",
    "# NUM_TOPICS = 12\n",
    "# chunksize = 2000\n",
    "# passes = 20\n",
    "# iterations = 400\n",
    "# eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# temp = dictionary[0]\n",
    "# id2word = dictionary.id2token\n",
    "\n",
    "# model = LdaModel(\n",
    "#     corpus=corpus,\n",
    "#     id2word=id2word,\n",
    "#     chunksize=chunksize,\n",
    "#     alpha='auto',\n",
    "#     eta='auto',\n",
    "#     iterations=iterations,\n",
    "#     num_topics=NUM_TOPICS,\n",
    "#     passes=passes,\n",
    "#     eval_every=eval_every\n",
    "# )\n",
    "# outputfile = f'model{NUM_TOPICS}.gensim'\n",
    "# print(\"Saving model in \" + outputfile)\n",
    "# # print(\"\")\n",
    "# # model.save(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbe9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2569f4d",
   "metadata": {},
   "source": [
    "<h3>Calculating and printing average topic coherence</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b491aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "model.num_topics\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / NUM_TOPICS\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c940b41",
   "metadata": {},
   "source": [
    "<h3> Printint out topics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38bfbe47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"image\" + 0.021*\"network\" + 0.014*\"feature\" + 0.012*\"our\" + 0.011*\"training\" + 0.011*\"model\" + 0.010*\"neural\" + 0.010*\"which\" + 0.009*\"learning\" + 0.009*\"from\" + 0.009*\"task\" + 0.008*\"method\" + 0.008*\"classification\" + 0.007*\"using\" + 0.007*\"can\" + 0.007*\"data\" + 0.007*\"propose\" + 0.006*\"trained\" + 0.006*\"result\" + 0.006*\"convolutional\"'),\n",
       " (1,\n",
       "  '0.034*\"code\" + 0.018*\"channel\" + 0.014*\"bound\" + 0.011*\"be\" + 0.011*\"rate\" + 0.010*\"optimal\" + 0.010*\"information\" + 0.009*\"matrix\" + 0.009*\"which\" + 0.009*\"capacity\" + 0.008*\"number\" + 0.008*\"result\" + 0.008*\"over\" + 0.008*\"where\" + 0.008*\"error\" + 0.008*\"number_of\" + 0.008*\"can\" + 0.008*\"paper\" + 0.008*\"scheme\" + 0.007*\"show\"'),\n",
       " (2,\n",
       "  '0.018*\"method\" + 0.015*\"our\" + 0.015*\"model\" + 0.013*\"object\" + 0.012*\"3d\" + 0.010*\"pose\" + 0.009*\"network\" + 0.009*\"state\" + 0.009*\"dataset\" + 0.009*\"from\" + 0.008*\"state_of\" + 0.008*\"art\" + 0.008*\"frame\" + 0.008*\"video\" + 0.008*\"based\" + 0.007*\"face\" + 0.007*\"learning\" + 0.007*\"which\" + 0.007*\"feature\" + 0.007*\"training\"'),\n",
       " (3,\n",
       "  '0.024*\"model\" + 0.019*\"learning\" + 0.018*\"data\" + 0.017*\"method\" + 0.016*\"our\" + 0.016*\"video\" + 0.011*\"from\" + 0.010*\"training\" + 0.010*\"performance\" + 0.008*\"new\" + 0.008*\"task\" + 0.008*\"can\" + 0.006*\"in_this\" + 0.006*\"which\" + 0.006*\"datasets\" + 0.006*\"based\" + 0.006*\"it\" + 0.006*\"propose\" + 0.005*\"be\" + 0.005*\"class\"'),\n",
       " (4,\n",
       "  '0.021*\"model\" + 0.016*\"system\" + 0.010*\"our\" + 0.009*\"show\" + 0.009*\"information\" + 0.009*\"it\" + 0.009*\"based\" + 0.008*\"propose\" + 0.008*\"object\" + 0.008*\"approach\" + 0.008*\"question\" + 0.007*\"knowledge\" + 0.007*\"each\" + 0.007*\"attention\" + 0.007*\"which\" + 0.007*\"not\" + 0.006*\"can\" + 0.006*\"at\" + 0.006*\"entity\" + 0.006*\"visual\"'),\n",
       " (5,\n",
       "  '0.023*\"network\" + 0.019*\"model\" + 0.018*\"method\" + 0.016*\"deep\" + 0.013*\"neural\" + 0.012*\"our\" + 0.011*\"learning\" + 0.010*\"can\" + 0.009*\"state_of\" + 0.009*\"art\" + 0.009*\"state\" + 0.009*\"which\" + 0.007*\"it\" + 0.007*\"based\" + 0.007*\"propose\" + 0.006*\"point\" + 0.006*\"representation\" + 0.006*\"image\" + 0.006*\"deep_learning\" + 0.006*\"paper\"'),\n",
       " (6,\n",
       "  '0.042*\"image\" + 0.013*\"approach\" + 0.013*\"method\" + 0.011*\"based\" + 0.010*\"be\" + 0.010*\"from\" + 0.010*\"resolution\" + 0.010*\"problem\" + 0.009*\"it\" + 0.009*\"algorithm\" + 0.009*\"quality\" + 0.009*\"which\" + 0.009*\"high\" + 0.008*\"our\" + 0.007*\"such\" + 0.007*\"can\" + 0.006*\"using\" + 0.006*\"proposed\" + 0.006*\"reconstruction\" + 0.006*\"based_on\"'),\n",
       " (7,\n",
       "  '0.016*\"from\" + 0.013*\"model\" + 0.012*\"detection\" + 0.011*\"based\" + 0.010*\"method\" + 0.010*\"be\" + 0.009*\"ha\" + 0.009*\"result\" + 0.009*\"research\" + 0.008*\"used\" + 0.008*\"system\" + 0.008*\"different\" + 0.008*\"it\" + 0.008*\"data\" + 0.008*\"can\" + 0.008*\"in_this\" + 0.008*\"feature\" + 0.007*\"were\" + 0.007*\"which\" + 0.007*\"been\"'),\n",
       " (8,\n",
       "  '0.018*\"algorithm\" + 0.017*\"proposed\" + 0.014*\"channel\" + 0.014*\"network\" + 0.013*\"user\" + 0.013*\"time\" + 0.012*\"it\" + 0.011*\"data\" + 0.011*\"problem\" + 0.011*\"performance\" + 0.011*\"can\" + 0.010*\"scheme\" + 0.010*\"rate\" + 0.009*\"in_this\" + 0.009*\"communication\" + 0.009*\"signal\" + 0.009*\"based\" + 0.008*\"complexity\" + 0.008*\"at\" + 0.008*\"power\"'),\n",
       " (9,\n",
       "  '0.034*\"model\" + 0.032*\"language\" + 0.025*\"text\" + 0.018*\"word\" + 0.017*\"task\" + 0.013*\"sentence\" + 0.011*\"based\" + 0.010*\"system\" + 0.010*\"natural\" + 0.010*\"our\" + 0.009*\"it\" + 0.009*\"speech\" + 0.009*\"translation\" + 0.009*\"natural_language\" + 0.008*\"from\" + 0.008*\"approach\" + 0.008*\"corpus\" + 0.008*\"or\" + 0.007*\"english\" + 0.007*\"these\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics( num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065832e5",
   "metadata": {},
   "source": [
    "<h3>Visualizing topics using pyLDAvis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis==2.1.2\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b132907",
   "metadata": {},
   "source": [
    "<h3>Creating a dataframe with dominant topics for each document</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a080e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(ldamodel=model, corpus=corpus, texts=raw_docs):\n",
    "   # Init output\n",
    "    document_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                document_topics_df = document_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    document_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    document_topics_df = pd.concat([document_topics_df, contents], axis=1)\n",
    "\n",
    "    document_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Original_Text']\n",
    "\n",
    "    return document_topics_df\n",
    "doc_topic_df = get_document_topics(ldamodel=model, corpus=corpus, texts=raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "doc_topics_sorted_df = pd.DataFrame()\n",
    "\n",
    "doc_topic_df_grpd = doc_topic_df.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in doc_topic_df_grpd:\n",
    "    doc_topics_sorted_df = pd.concat([doc_topics_sorted_df, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "doc_topics_sorted_df.reset_index(drop=True, inplace=True)\n",
    "doc_topics_sorted_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "doc_topics_sorted_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82271c",
   "metadata": {},
   "source": [
    "<h3>Find the most representative document for each topic</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f790d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_doc(doc_topic_df=doc_topic_df, k=5):\n",
    "\n",
    "    doc_topics_sorted_df = pd.DataFrame()\n",
    "\n",
    "    doc_topic_df_grpd = doc_topic_df.groupby('Dominant_Topic')\n",
    "\n",
    "    for i, grp in doc_topic_df_grpd:\n",
    "        doc_topics_sorted_df = pd.concat([doc_topics_sorted_df, \n",
    "                                          grp.sort_values(['Perc_Contribution'], ascending=[0]).head(k)], \n",
    "                                          axis=0)\n",
    "\n",
    "    doc_topics_sorted_df.reset_index(drop=True, inplace=True)\n",
    "    doc_topics_sorted_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "    return doc_topics_sorted_df\n",
    "\n",
    "top_k_df = find_top_k_doc()\n",
    "top_k_df\n",
    "\n",
    "from pprint import pprint\n",
    "T = 2\n",
    "for index, row in top_k_df[top_k_df['Topic_Num']==T].iterrows():\n",
    "\n",
    "    print(\"Doc Contribution is %d\" , row[\"Topic_Perc_Contrib\"])\n",
    "    pprint(row[\"Text\"] + \"\\n-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfcd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7409a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = {\n",
    "    'num_topics': [10, 12, 14, 16],\n",
    "    'alpha': ['auto', 0.1, 0.01],\n",
    "    'eta': ['auto', 0.1, 0.01],\n",
    "    'passes': [10, 20, 30],\n",
    "    'iterations': [200, 400, 600]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "best_params = {}\n",
    "best_coherence = -100\n",
    "\n",
    "# for params in grid:\n",
    "#     model = LdaModel(\n",
    "#         corpus=corpus,\n",
    "#         id2word=id2word,\n",
    "#         chunksize=chunksize,\n",
    "#         alpha=params['alpha'],\n",
    "#         eta=params['eta'],\n",
    "#         iterations=params['iterations'],\n",
    "#         num_topics=params['num_topics'],\n",
    "#         passes=params['passes'],\n",
    "#         eval_every=eval_every\n",
    "#     )\n",
    "    \n",
    "#     top_topics = model.top_topics(corpus)\n",
    "#     avg_topic_coherence = sum([t[1] for t in top_topics]) / params['num_topics']\n",
    "    \n",
    "#     if avg_topic_coherence > best_coherence:\n",
    "#         best_coherence = avg_topic_coherence\n",
    "#         best_params = params\n",
    "\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fc483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "def train_lda_model(params, corpus, id2word, chunksize, eval_every):\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha=params['alpha'],\n",
    "        eta=params['eta'],\n",
    "        iterations=params['iterations'],\n",
    "        num_topics=params['num_topics'],\n",
    "        passes=params['passes'],\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    top_topics = model.top_topics(corpus)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / params['num_topics']\n",
    "    \n",
    "    return (params, avg_topic_coherence)\n",
    "\n",
    "# Modify the train_lda_model function call in the loop\n",
    "with Pool() as pool:\n",
    "    results = pool.map(\n",
    "        partial(train_lda_model, corpus=corpus, id2word=id2word, chunksize=chunksize, eval_every=eval_every),\n",
    "        grid\n",
    "    )\n",
    "\n",
    "best_params, best_coherence = max(results, key=lambda x: x[1])\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "601f1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = {\n",
    "    'num_topics': [10, 12, 14, 16],\n",
    "    'alpha': ['auto', 0.1, 0.01],\n",
    "    'eta': ['auto', 0.1, 0.01],\n",
    "    'passes': [10],\n",
    "    'iterations': [200]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "def train_lda_model(params, corpus, id2word, chunksize, eval_every):\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha=params['alpha'],\n",
    "        eta=params['eta'],\n",
    "        iterations=params['iterations'],\n",
    "        num_topics=params['num_topics'],\n",
    "        passes=params['passes'],\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    print(params)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / params['num_topics']\n",
    "    \n",
    "    return (params, avg_topic_coherence, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 400, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.1, 'iterations': 600, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 400, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 'auto', 'eta': 0.01, 'iterations': 600, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 400, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.1, 'iterations': 600, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 400, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.1, 'eta': 0.01, 'iterations': 600, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 12, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 12, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 200, 'num_topics': 16, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 400, 'num_topics': 14, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 400, 'num_topics': 10, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 10, 'passes': 30}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 16, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 16, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 400, 'num_topics': 12, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 200, 'num_topics': 14, 'passes': 20}\n",
      "{'alpha': 0.01, 'eta': 0.1, 'iterations': 400, 'num_topics': 14, 'passes': 10}\n",
      "{'alpha': 0.01, 'eta': 'auto', 'iterations': 600, 'num_topics': 14, 'passes': 30}\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(train_lda_model, params, corpus, id2word, chunksize, eval_every) for params in grid]\n",
    "\n",
    "results = [future.result() for future in futures]\n",
    "best_params, best_coherence, best_model = max(results, key=lambda x: x[1])\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
