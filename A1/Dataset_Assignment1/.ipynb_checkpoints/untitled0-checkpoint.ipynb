{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150859b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\anaconda3\\envs\\5212A1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Sunny\\anaconda3\\envs\\5212A1\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.legacy import data\n",
    "import torch.nn as nn\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, Dataset\n",
    "import copy\n",
    "\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7523955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1e477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1766f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5106cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE):\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "    LABEL = data.LabelField(dtype=torch.float, use_vocab=False, preprocessing=int)\n",
    "    if target_field == \"title\":\n",
    "        train_datafield = [\n",
    "            (target_field, TEXT),\n",
    "            (\"abstract\", None), \n",
    "            (\"InformationTheory\", LABEL),\n",
    "            (\"ComputationalLinguistics\", LABEL),\n",
    "            (\"ComputerVision\", LABEL)\n",
    "        ]\n",
    "    else:\n",
    "        train_datafield = [\n",
    "            (\"title\", None),\n",
    "            (target_field, TEXT), \n",
    "            (\"InformationTheory\", LABEL),\n",
    "            (\"ComputationalLinguistics\", LABEL),\n",
    "            (\"ComputerVision\", LABEL)\n",
    "        ]\n",
    "        \n",
    "    train_data, test_data = TabularDataset.splits(\n",
    "        path=path,\n",
    "        train=train_file, test=test_file, format=\"csv\",\n",
    "        skip_header=True, fields=train_datafield\n",
    "    )\n",
    "    train_data, valid_data = train_data.split(split_ratio=0.9, random_state=random.getstate())\n",
    "\n",
    "    MAX_VOCAB_SIZE = MAX_VOCAB_SIZE\n",
    "\n",
    "    TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "    \n",
    "    return TEXT, LABEL, train_data, valid_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea70128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_target_label(train_data, target_field):\n",
    "    for example in train_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "        \n",
    "    for example in test_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "\n",
    "    for example in valid_data:\n",
    "        setattr(example, f\"label_{target_field}\", getattr(example, target_field))\n",
    "\n",
    "def generate_label_iterator(train_data, valid_data, test_data, label, target_field):\n",
    "#     preprocess_target_label(train_data, target_field)\n",
    "    \n",
    "    label_attr = f\"label_{label}\"\n",
    "    BATCH_SIZE = 64\n",
    "    iterators = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device,\n",
    "        sort_key = lambda x: len(getattr(x, target_field)),\n",
    "        sort_within_batch = False)\n",
    "    return iterators[0], iterators[1], iterators[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9cc69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pos_weight = None):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "        if self.pos_weight is not None:\n",
    "            self.criterion = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "        else:\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b839c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weight(train_data):\n",
    "    pos_weights = []\n",
    "    for field in [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]:\n",
    "        negative_count, positive_count = 0, 0\n",
    "        for example in train_data.examples:\n",
    "            if getattr(example, field) == 0:\n",
    "                negative_count += 1\n",
    "            else:\n",
    "                positive_count += 1\n",
    "        \n",
    "        pos_weights.append(torch.tensor([negative_count / positive_count]).to(device))\n",
    "    return pos_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9472a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_and_optimizer(pos_weight, embedding_dim=100, hidden_dim=256, output_dim=1, lr=1e-2):\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "    model = RNN(INPUT_DIM, embedding_dim, hidden_dim, output_dim, pos_weight)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5556ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, label_field, target_field):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "        \n",
    "        # Use the specific label field\n",
    "        loss = criterion(predictions, getattr(batch, label_field))\n",
    "        \n",
    "        acc = binary_accuracy(predictions, getattr(batch, label_field))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, label_field, target_field):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "            \n",
    "            # Use the specific label field\n",
    "            loss = criterion(predictions, getattr(batch, label_field))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, getattr(batch, label_field))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ac6e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_df = pd.DataFrame()\n",
    "\n",
    "def train_loop(ModelsList, OptimizersList, IteratorsList, N_EPOCHS, label_names, target_field, modelFileNames, bestLossesList, preprocess, train_size, patience=5, rnn_metric_results=[]):\n",
    "    result = []\n",
    "    best_model_states = []\n",
    "    best_epochs = [0, 0, 0]\n",
    "    for idx, (label_name, model, optimizer, iterators_) in enumerate(zip(label_names, ModelsList, OptimizersList, IteratorsList)):\n",
    "        print(f\"Training model for {preprocess}_{label_name}, using {train_size} data of {target_field}...\")\n",
    "\n",
    "        bad_epochs = 0\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_iterator = iterators_[0]\n",
    "            test_iterator = iterators_[2] if len(iterators_) > 2 else iterators_[1]\n",
    "            valid_iterator = iterators_[1] if len(iterators_) > 2 else None\n",
    "\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, model.criterion, label_name, target_field)\n",
    "\n",
    "            if valid_iterator:\n",
    "                valid_loss, valid_acc = evaluate(model, valid_iterator, model.criterion, label_name, target_field)\n",
    "\n",
    "            test_loss, test_acc = evaluate(model, test_iterator, model.criterion, label_name, target_field)\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "            if test_loss < bestLossesList[idx]:\n",
    "                bestLossesList[idx] = test_loss\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "#                 best_model_state = model.state_dict()\n",
    "                bad_epochs = 0\n",
    "                best_epochs[idx] = epoch + 1\n",
    "        \n",
    "                print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "                print(f'{label_name} Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "                print(f'{label_name} Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "                print(f'{label_name} Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "        \n",
    "            if bad_epochs > patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for {label_name} model.\")\n",
    "#                 model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "            # For predictions and ground truth collection\n",
    "            model.eval()\n",
    "            y_predict, y_test = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_iterator:\n",
    "                    predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "                    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                    y_predict += rounded_preds.tolist()\n",
    "                    y_test += getattr(batch, label_name).tolist()\n",
    "        print(f\"Best performing epoch for {label_name} model: {best_epochs[idx]}\")\n",
    "        best_model_states.append(best_model_state)\n",
    "        \n",
    "        model.load_state_dict(best_model_state)\n",
    "        # Call 'evaluate_model' and store its return value\n",
    "        rnn_eval_metrics = evaluate_model(model, test_iterator, label_name, target_field)\n",
    "        \n",
    "        # Append the evaluation metrics to the 'rnn_metric_results' list\n",
    "        rnn_metric_results.append({\n",
    "            'label_name': label_name,\n",
    "            'model_name': model.__class__.__name__,\n",
    "            'target_field': target_field,\n",
    "            'train_size': train_size,\n",
    "            'tokenizer': preprocess,\n",
    "            **rnn_eval_metrics\n",
    "        })\n",
    "#         result.append((y_predict, y_test))\n",
    "        \n",
    "    return rnn_metric_results, best_model_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfcbcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, iterator, label_field, target_field):\n",
    "    \n",
    "\n",
    "    y_probs = []\n",
    "    y_predict = []\n",
    "    y_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(getattr(batch, target_field)).squeeze(1)\n",
    "            probs = torch.sigmoid(predictions)\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "            \n",
    "            y_predict += rounded_preds.tolist()\n",
    "            y_test += getattr(batch, label_field).tolist()\n",
    "            y_probs += probs.tolist()\n",
    "    \n",
    "    y_predict = np.asarray(y_predict)\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_probs = np.asarray(y_probs)\n",
    "    # Compute metrics\n",
    "    recall_macro = recall_score(y_test, y_predict, average='macro')\n",
    "    precision_macro = precision_score(y_test, y_predict, average='macro')\n",
    "    recall = recall_score(y_test, y_predict, average=None)\n",
    "    precision = precision_score(y_test, y_predict, average=None)\n",
    "    f1score = f1_score(y_test, y_predict, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    matthews = matthews_corrcoef(y_test, y_predict)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "    return {\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'Macro Recall': recall_macro,\n",
    "        'Macro Precision': precision_macro,\n",
    "        'Macro F1 score': f1score,\n",
    "        'MCC': matthews,\n",
    "        'Recall_curve': recall_curve,  # Add this line to store the recall curve values\n",
    "        'Precision_curve': precision_curve  # Add this line to store the precision curve values\n",
    "    }\n",
    "#     # Print metrics\n",
    "#     print(f\"{label_field}:\")\n",
    "#     print(confusion_matrix(y_test, y_predict))\n",
    "#     print('Accuracy:', accuracy)\n",
    "#     print('Macro Precision:', precision)\n",
    "#     print('Macro Recall:', recall)\n",
    "#     print('Macro F1 score:', f1score)\n",
    "#     print('MCC:', matthews)\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3991c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c981b971",
   "metadata": {},
   "source": [
    "<h2>P1 Title 1000</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e9b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):   \n",
    "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text.lower() not in STOP_WORDS]\n",
    "\n",
    "path = './'\n",
    "train_file = \"train_1000.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"title\"\n",
    "tokenizer = custom_tokenizer\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, custom_tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_T_P1_1000\", \"RNN_model_CL_T_P1_1000.pt\", \"RNN_model_CV_T_P1_1000.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ac49b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', 607), ('\\n  ', 454), (':', 210), ('learning', 133), ('based', 77), ('neural', 70), ('networks', 69), ('deep', 65), ('image', 60), ('network', 56), ('detection', 55), ('segmentation', 44), ('codes', 40), ('multi', 38), ('recognition', 38), ('classification', 33), ('data', 33), ('model', 31), ('analysis', 31), ('language', 31), ('images', 30), ('estimation', 29), ('convolutional', 28), ('models', 27), ('systems', 27), ('video', 26), ('text', 26), ('supervised', 25), ('robust', 25), ('time', 24), ('channel', 24), ('semantic', 23), ('information', 23), ('generation', 22), ('end', 22), ('face', 22), ('training', 22), ('visual', 21), ('attention', 21), ('joint', 21), ('3d', 21), ('(', 21), ('channels', 20), ('resolution', 20), ('object', 20), ('efficient', 20), ('speech', 20), ('graph', 19), ('system', 19), ('shot', 19), ('adversarial', 18), ('non', 18), ('low', 18), ('aware', 17), ('distributed', 17), ('modeling', 17), ('feature', 17), ('self', 17), ('real', 17), ('unsupervised', 16), ('domain', 16), ('scale', 16), ('prediction', 16), ('$', 16), ('rate', 16), ('translation', 15), ('automatic', 15), ('coding', 15), ('evaluation', 15), ('person', 15), ('mimo', 15), ('machine', 14), ('transfer', 14), ('answering', 14), ('human', 14), ('task', 14), (')', 14), ('large', 14), ('improving', 14), ('motion', 14), ('communication', 13), ('question', 13), ('tracking', 13), ('super', 13), ('high', 13), ('sequence', 13), ('scene', 12), ('wireless', 12), ('?', 12), ('multiple', 11), ('resource', 11), ('capacity', 11), ('bounds', 11), ('approach', 11), ('reconstruction', 11), ('identification', 11), ('feedback', 11), ('temporal', 11), ('representation', 11), ('semi', 11), ('natural', 11), ('method', 11), ('sparse', 11), ('performance', 11), ('dataset', 11), ('study', 11), ('interference', 11), ('videos', 10), ('word', 10), ('fine', 10), ('style', 10), ('single', 10), ('multimodal', 10), ('features', 10), ('pose', 10), ('adaptation', 10), ('optimization', 10), ('framework', 10), ('new', 10), ('frame', 10), ('optimal', 10), ('rank', 10), ('error', 10), ('distillation', 10), ('selection', 10), ('retrieval', 10), ('cross', 10), ('knowledge', 10), ('point', 10), ('spatial', 9), ('understanding', 9), ('online', 9), ('dense', 9), ('net', 9), ('state', 9), ('context', 9), ('fading', 9), ('fully', 9), ('level', 9), ('energy', 9), ('hybrid', 9), ('guided', 9), ('class', 9), ('dynamic', 9), ('probabilistic', 9), ('design', 9), ('dual', 9), ('gaussian', 9), ('local', 9), ('zero', 9), ('dialogue', 9), ('instance', 8), ('convolution', 8), ('binary', 8), ('synthesis', 8), ('processing', 8), ('space', 8), ('sentiment', 8), ('vision', 8), ('applications', 8), ('relation', 8), ('transmission', 8), ('sequences', 8), ('linear', 8), ('reasoning', 8), ('loss', 8), ('beamforming', 8), ('novel', 8), ('sensing', 8), ('technique', 8), ('user', 8), ('fast', 8), ('adaptive', 8), ('extraction', 8), ('transformer', 8), ('distribution', 8), ('control', 8), ('english', 7), ('coded', 7), ('tensor', 7), ('ct', 7), ('sets', 7), ('open', 7), ('iterative', 7), ('hierarchical', 7), ('view', 7), ('decomposition', 7), ('optical', 7), ('clustering', 7), ('encoder', 7), ('action', 7), ('dimensional', 7), ('algorithm', 7), ('structured', 7), ('massive', 7), ('residual', 7), ('localization', 7), ('complexity', 7), ('computation', 7), ('depth', 7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(900, 100, 18066, 2514)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(TEXT.vocab.freqs.most_common(200))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0beeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb9b5d",
   "metadata": {},
   "source": [
    "<h2>P2 Title 1000</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bbbce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer_P2(text):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(tok.text) for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "path = './'\n",
    "train_file = \"train_1000.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"title\"\n",
    "tokenizer = custom_tokenizer_P2\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_T_P2_1000\", \"RNN_model_CL_T_P2_1000.pt\", \"RNN_model_CV_T_P2_1000.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e0af864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', 607), ('\\n  ', 454), ('for', 354), ('of', 241), (':', 210), ('and', 206), ('a', 154), ('in', 151), ('learn', 136), ('with', 133), ('the', 126), ('network', 125), ('on', 95), ('imag', 93), ('base', 78), ('model', 76), ('use', 76), ('to', 73), ('neural', 70), ('code', 68), ('deep', 65), ('detect', 62), ('segment', 47), ('system', 46), ('channel', 45), ('gener', 44), ('from', 39), ('multi', 38), ('recognit', 38), ('video', 36), ('convolut', 36), ('via', 35), ('estim', 34), ('classif', 33), ('data', 33), ('languag', 32), ('by', 32), ('analysi', 31), ('object', 30), ('robust', 28), ('distribut', 28), ('supervis', 27), ('text', 27), ('featur', 27), ('train', 27), ('predict', 27), ('attent', 25), ('time', 25), ('optim', 25), ('face', 24), ('semant', 24), ('visual', 23), ('end', 23), ('inform', 23), ('graph', 22), ('adapt', 22), ('improv', 22), ('transform', 22), ('joint', 21), ('over', 21), ('an', 21), ('3d', 21), ('effici', 21), ('(', 21), ('sequenc', 21), ('local', 21), ('commun', 20), ('resolut', 20), ('adversari', 20), ('comput', 20), ('speech', 20), ('shot', 19), ('rate', 19), ('scale', 18), ('task', 18), ('evalu', 18), ('non', 18), ('low', 18), ('represent', 18), ('translat', 17), ('awar', 17), ('question', 17), ('domain', 17), ('person', 17), ('self', 17), ('real', 17), ('answer', 16), ('unsupervis', 16), ('automat', 16), ('relat', 16), ('$', 16), ('toward', 16), ('transfer', 15), ('structur', 15), ('method', 15), ('mimo', 15), ('scene', 14), ('machin', 14), ('word', 14), ('track', 14), ('bound', 14), ('human', 14), ('approach', 14), (')', 14), ('larg', 14), ('rank', 14), ('motion', 14), ('dataset', 14), ('studi', 14), ('multipl', 13), ('state', 13), ('super', 13), ('new', 13), ('encod', 13), ('high', 13), ('techniqu', 13), ('one', 13), ('wireless', 12), ('embed', 12), ('reconstruct', 12), ('sens', 12), ('?', 12), ('map', 12), ('through', 12), ('decod', 12), ('effect', 11), ('dens', 11), ('resourc', 11), ('capac', 11), ('style', 11), ('control', 11), ('process', 11), ('complex', 11), ('level', 11), ('applic', 11), ('identif', 11), ('augment', 11), ('feedback', 11), ('tempor', 11), ('algorithm', 11), ('semi', 11), ('natur', 11), ('class', 11), ('spars', 11), ('perform', 11), ('compress', 11), ('select', 11), ('knowledg', 11), ('enhanc', 11), ('interfer', 11), ('spatial', 10), ('understand', 10), ('fine', 10), ('singl', 10), ('multimod', 10), ('pose', 10), ('cnn', 10), ('interpret', 10), ('framework', 10), ('frame', 10), ('weight', 10), ('re', 10), ('cluster', 10), ('label', 10), ('function', 10), ('error', 10), ('distil', 10), ('devic', 10), ('retriev', 10), ('cross', 10), ('point', 10), ('dialogu', 10), ('onlin', 9), ('net', 9), ('correl', 9), ('context', 9), ('fade', 9), ('set', 9), ('fulli', 9), ('energi', 9), ('hybrid', 9), ('guid', 9), ('dynam', 9), ('probabilist', 9), ('random', 9), ('user', 9), ('design', 9), ('extract', 9), ('dual', 9), ('gaussian', 9), ('zero', 9), ('instanc', 8), ('sentenc', 8), ('binari', 8), ('field', 8), ('synthesi', 8), ('space', 8), ('sentiment', 8), ('regular', 8), ('vision', 8)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(900, 100, 18066, 2083)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(200))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01622f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for P1_InformationTheory, using 900 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.032 | Train Acc: 59.90%\n",
      "InformationTheory Valid Loss: 1.038 | Valid Acc: 30.73%\n",
      "InformationTheory Test Loss: 1.315 | Test Acc: 50.51%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.043 | Train Acc: 55.73%\n",
      "InformationTheory Valid Loss: 1.036 | Valid Acc: 31.51%\n",
      "InformationTheory Test Loss: 1.312 | Test Acc: 51.06%\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.059 | Train Acc: 59.06%\n",
      "InformationTheory Valid Loss: 1.036 | Valid Acc: 30.90%\n",
      "InformationTheory Test Loss: 1.309 | Test Acc: 52.62%\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.054 | Train Acc: 35.31%\n",
      "InformationTheory Valid Loss: 1.037 | Valid Acc: 69.88%\n",
      "InformationTheory Test Loss: 1.308 | Test Acc: 52.61%\n",
      "Epoch: 20 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.051 | Train Acc: 49.27%\n",
      "InformationTheory Valid Loss: 1.038 | Valid Acc: 69.88%\n",
      "InformationTheory Test Loss: 1.304 | Test Acc: 53.52%\n",
      "Epoch: 21 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.050 | Train Acc: 66.88%\n",
      "InformationTheory Valid Loss: 1.039 | Valid Acc: 29.34%\n",
      "InformationTheory Test Loss: 1.289 | Test Acc: 53.56%\n",
      "Epoch: 24 | Epoch Time: 0m 0s\n",
      "InformationTheory Train Loss: 1.082 | Train Acc: 41.77%\n",
      "InformationTheory Valid Loss: 1.038 | Valid Acc: 67.53%\n",
      "InformationTheory Test Loss: 1.289 | Test Acc: 52.86%\n",
      "Best performing epoch for InformationTheory model: 24\n",
      "Training model for P1_ComputationalLinguistics, using 900 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.066 | Train Acc: 45.10%\n",
      "ComputationalLinguistics Valid Loss: 1.062 | Valid Acc: 69.53%\n",
      "ComputationalLinguistics Test Loss: 0.982 | Test Acc: 55.33%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.036 | Train Acc: 55.83%\n",
      "ComputationalLinguistics Valid Loss: 1.076 | Valid Acc: 72.66%\n",
      "ComputationalLinguistics Test Loss: 0.980 | Test Acc: 57.65%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.045 | Train Acc: 44.58%\n",
      "ComputationalLinguistics Valid Loss: 1.062 | Valid Acc: 69.70%\n",
      "ComputationalLinguistics Test Loss: 0.978 | Test Acc: 56.71%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.054 | Train Acc: 40.52%\n",
      "ComputationalLinguistics Valid Loss: 1.063 | Valid Acc: 68.14%\n",
      "ComputationalLinguistics Test Loss: 0.975 | Test Acc: 58.57%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.060 | Train Acc: 38.33%\n",
      "ComputationalLinguistics Valid Loss: 1.068 | Valid Acc: 69.70%\n",
      "ComputationalLinguistics Test Loss: 0.975 | Test Acc: 58.93%\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.036 | Train Acc: 41.67%\n",
      "ComputationalLinguistics Valid Loss: 1.064 | Valid Acc: 70.49%\n",
      "ComputationalLinguistics Test Loss: 0.973 | Test Acc: 60.13%\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.059 | Train Acc: 43.75%\n",
      "ComputationalLinguistics Valid Loss: 1.060 | Valid Acc: 64.06%\n",
      "ComputationalLinguistics Test Loss: 0.973 | Test Acc: 59.49%\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.058 | Train Acc: 47.81%\n",
      "ComputationalLinguistics Valid Loss: 1.061 | Valid Acc: 27.34%\n",
      "ComputationalLinguistics Test Loss: 0.971 | Test Acc: 59.74%\n",
      "Epoch: 16 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.082 | Train Acc: 41.98%\n",
      "ComputationalLinguistics Valid Loss: 1.058 | Valid Acc: 27.34%\n",
      "ComputationalLinguistics Test Loss: 0.971 | Test Acc: 59.41%\n",
      "Epoch: 17 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.064 | Train Acc: 56.15%\n",
      "ComputationalLinguistics Valid Loss: 1.064 | Valid Acc: 72.83%\n",
      "ComputationalLinguistics Test Loss: 0.971 | Test Acc: 59.96%\n",
      "Epoch: 19 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.057 | Train Acc: 45.73%\n",
      "ComputationalLinguistics Valid Loss: 1.061 | Valid Acc: 68.92%\n",
      "ComputationalLinguistics Test Loss: 0.971 | Test Acc: 58.89%\n",
      "Epoch: 20 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.056 | Train Acc: 51.35%\n",
      "ComputationalLinguistics Valid Loss: 1.060 | Valid Acc: 25.95%\n",
      "ComputationalLinguistics Test Loss: 0.970 | Test Acc: 58.98%\n",
      "Epoch: 21 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.037 | Train Acc: 50.62%\n",
      "ComputationalLinguistics Valid Loss: 1.064 | Valid Acc: 71.27%\n",
      "ComputationalLinguistics Test Loss: 0.968 | Test Acc: 61.57%\n",
      "Epoch: 25 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.056 | Train Acc: 45.42%\n",
      "ComputationalLinguistics Valid Loss: 1.061 | Valid Acc: 71.27%\n",
      "ComputationalLinguistics Test Loss: 0.967 | Test Acc: 61.75%\n",
      "Epoch: 26 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.116 | Train Acc: 32.50%\n",
      "ComputationalLinguistics Valid Loss: 1.059 | Valid Acc: 27.52%\n",
      "ComputationalLinguistics Test Loss: 0.967 | Test Acc: 59.76%\n",
      "Epoch: 27 | Epoch Time: 0m 0s\n",
      "ComputationalLinguistics Train Loss: 1.034 | Train Acc: 50.10%\n",
      "ComputationalLinguistics Valid Loss: 1.065 | Valid Acc: 73.61%\n",
      "ComputationalLinguistics Test Loss: 0.965 | Test Acc: 62.64%\n",
      "Best performing epoch for ComputationalLinguistics model: 27\n",
      "Training model for P1_ComputerVision, using 900 data of title...\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "ComputerVision Train Loss: 0.686 | Train Acc: 50.73%\n",
      "ComputerVision Valid Loss: 0.687 | Valid Acc: 51.39%\n",
      "ComputerVision Test Loss: 0.705 | Test Acc: 44.46%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "ComputerVision Train Loss: 0.686 | Train Acc: 50.00%\n",
      "ComputerVision Valid Loss: 0.693 | Valid Acc: 51.39%\n",
      "ComputerVision Test Loss: 0.702 | Test Acc: 44.92%\n",
      "Early stopping at epoch 16 for ComputerVision model.\n",
      "Best performing epoch for ComputerVision model: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\AppData\\Local\\Temp\\ipykernel_20188\\1963127758.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ce203",
   "metadata": {},
   "source": [
    "<h2> P1 Title ALL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"title\"\n",
    "tokenizer = custom_tokenizer\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_T_P1_ALL\", \"RNN_model_CL_T_P1_ALL.pt\", \"RNN_model_CV_T_P1_ALL.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f34aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "505eacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_df.to_csv(\"RNN_Checking.csv\", index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e7ce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>target_field</th>\n",
       "      <th>train_size</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro F1 score</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>InformationTheory</td>\n",
       "      <td>RNN</td>\n",
       "      <td>title</td>\n",
       "      <td>900</td>\n",
       "      <td>P1</td>\n",
       "      <td>[0.5751538541775321, 0.44616110390376224]</td>\n",
       "      <td>[0.5400587659157688, 0.48154276985743383]</td>\n",
       "      <td>0.510657</td>\n",
       "      <td>0.510801</td>\n",
       "      <td>0.510116</td>\n",
       "      <td>0.021458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ComputationalLinguistics</td>\n",
       "      <td>RNN</td>\n",
       "      <td>title</td>\n",
       "      <td>900</td>\n",
       "      <td>P1</td>\n",
       "      <td>[0.7620084365219758, 0.252375296912114]</td>\n",
       "      <td>[0.8164455459979589, 0.19549218031278748]</td>\n",
       "      <td>0.507192</td>\n",
       "      <td>0.505969</td>\n",
       "      <td>0.504305</td>\n",
       "      <td>0.013104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ComputerVision</td>\n",
       "      <td>RNN</td>\n",
       "      <td>title</td>\n",
       "      <td>900</td>\n",
       "      <td>P1</td>\n",
       "      <td>[0.5254494808812358, 0.5237176394918798]</td>\n",
       "      <td>[0.6775878959399151, 0.36682058790404326]</td>\n",
       "      <td>0.524584</td>\n",
       "      <td>0.522204</td>\n",
       "      <td>0.511673</td>\n",
       "      <td>0.046727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InformationTheory</td>\n",
       "      <td>RNN</td>\n",
       "      <td>title</td>\n",
       "      <td>900</td>\n",
       "      <td>P1</td>\n",
       "      <td>[0.4614582246792532, 0.6056138695600897]</td>\n",
       "      <td>[0.5695159629248198, 0.4986405127209167]</td>\n",
       "      <td>0.533536</td>\n",
       "      <td>0.534078</td>\n",
       "      <td>0.528385</td>\n",
       "      <td>0.067612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ComputationalLinguistics</td>\n",
       "      <td>RNN</td>\n",
       "      <td>title</td>\n",
       "      <td>900</td>\n",
       "      <td>P1</td>\n",
       "      <td>[0.6822696965573547, 0.37885985748218526]</td>\n",
       "      <td>[0.8273927392739274, 0.21459804910864447]</td>\n",
       "      <td>0.530565</td>\n",
       "      <td>0.520995</td>\n",
       "      <td>0.510926</td>\n",
       "      <td>0.050664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ComputerVision</td>\n",
       "      <td>RNN</td>\n",
       "      <td>title</td>\n",
       "      <td>900</td>\n",
       "      <td>P1</td>\n",
       "      <td>[0.4571621507554655, 0.43174143753014954]</td>\n",
       "      <td>[0.6051396648044692, 0.2945370776656428]</td>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.449838</td>\n",
       "      <td>0.435512</td>\n",
       "      <td>-0.105573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label_name model_name  ... Macro F1 score       MCC\n",
       "0         InformationTheory        RNN  ...       0.510116  0.021458\n",
       "1  ComputationalLinguistics        RNN  ...       0.504305  0.013104\n",
       "2            ComputerVision        RNN  ...       0.511673  0.046727\n",
       "3         InformationTheory        RNN  ...       0.528385  0.067612\n",
       "4  ComputationalLinguistics        RNN  ...       0.510926  0.050664\n",
       "5            ComputerVision        RNN  ...       0.435512 -0.105573\n",
       "\n",
       "[6 rows x 11 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5abe327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        target_field = 'T' if row['target_field'] == 'title' else 'A'\n",
    "        size = str(1000) if row['train_size'] == 900 else 'All'\n",
    "        tokenizer = 'P1' if row['tokenizer'] == 'P1' else 'P2'\n",
    "        label = f\"{tokenizer}_{size}_{target_field}_{row['label_name']}\"\n",
    "        plt.plot(row['Recall'], row['Precision'], label=f\"{label} (AUC: {auc(row['Recall'], row['Precision']):.2f})\")\n",
    "    \n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "plot_precision_recall_curve(rnn_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3d05b",
   "metadata": {},
   "source": [
    "<h2> P2 Title ALL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0975e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"title\"\n",
    "tokenizer = custom_tokenizer_P2\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_T_P2_ALL\", \"RNN_model_CL_T_P2_ALL.pt\", \"RNN_model_CV_T_P2_ALL.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b06698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20129cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9947af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eae85577",
   "metadata": {},
   "source": [
    "<h2> P1 Abstract 1000</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee80b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_file = \"train_1000.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"abstract\"\n",
    "tokenizer = custom_tokenizer\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_A_P1_1000\", \"RNN_model_CL_A_P1_1000.pt\", \"RNN_model_CV_A_P1_1000.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac886b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbe19c35",
   "metadata": {},
   "source": [
    "<h2> P2 Abstract 1000</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_file = \"train_1000.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"abstract\"\n",
    "tokenizer = custom_tokenizer_P2\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_A_P2_1000\", \"RNN_model_CL_A_P2_1000.pt\", \"RNN_model_CV_A_P2_1000.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0116188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc19235",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e10f7",
   "metadata": {},
   "source": [
    "<h2> P1 Abstract ALL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81286fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"abstract\"\n",
    "tokenizer = custom_tokenizer\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_A_P2_1000\", \"RNN_model_CL_A_P2_1000.pt\", \"RNN_model_CV_A_P2_1000.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7684a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35afa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_df.to_csv(\"RNN_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333150d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc21c4b4",
   "metadata": {},
   "source": [
    "<h2> P2 Abstract ALL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "target_field = \"abstract\"\n",
    "tokenizer = custom_tokenizer_P2\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT, LABEL, train_data, valid_data, test_data = load_data(path, train_file, test_file, target_field, tokenizer, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_iterator_IT, validation_IT, test_iterator_IT = generate_label_iterator(train_data, valid_data, test_data, \"InformationTheory\", target_field)\n",
    "train_iterator_CL, validation_CL, test_iterator_CL = generate_label_iterator(train_data, valid_data, test_data, \"ComputationalLinguistics\", target_field)\n",
    "train_iterator_CV, validation_CV, test_iterator_CV = generate_label_iterator(train_data, valid_data, test_data, \"ComputerVision\", target_field)\n",
    "\n",
    "pos_weights = get_pos_weight(train_data)\n",
    "\n",
    "model_IT, optimizer_IT = generate_model_and_optimizer(pos_weight=pos_weights[0])\n",
    "model_CL, optimizer_CL = generate_model_and_optimizer(pos_weight=pos_weights[1])\n",
    "model_CV, optimizer_CV = generate_model_and_optimizer(pos_weight=pos_weights[2])\n",
    "\n",
    "N_EPOCHS = 30\n",
    "label_names = [\"InformationTheory\", \"ComputationalLinguistics\", \"ComputerVision\"]\n",
    "models = [model_IT, model_CL, model_CV]\n",
    "optimizers = [optimizer_IT, optimizer_CL, optimizer_CV]\n",
    "iterators = [(train_iterator_IT, validation_IT, test_iterator_IT), (train_iterator_CL, validation_CL, test_iterator_CL), (train_iterator_CV, validation_CV, test_iterator_CV)]\n",
    "model_file_names = [\"RNN_model_IT_A_P2_ALL\", \"RNN_model_CL_A_P2_ALL.pt\", \"RNN_model_CV_A_P2_ALL.pt\"]\n",
    "best_valid_losses = [float(\"inf\"), float(\"inf\"), float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))\n",
    "len(train_data), len(valid_data), len(test_data), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results, best_model_states = train_loop(models, optimizers, iterators, N_EPOCHS, label_names, target_field, model_file_names, best_valid_losses, \"P1\", len(train_data), 10, rnn_metric_results=[])\n",
    "rnn_results_df = rnn_results_df.append(pd.DataFrame(rnn_results), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04300972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348f732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4119e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac37804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a67cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84303b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965eb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282df39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034bb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965680e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
